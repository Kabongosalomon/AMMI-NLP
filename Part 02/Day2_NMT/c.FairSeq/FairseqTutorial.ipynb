{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIRSEQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://fairseq.readthedocs.io/en/latest/getting_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks.\" It provides reference implementations of various sequence-to-sequence models making our life much more easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --upgrade fairseq\n",
    "#!pip install sacremoses subword_nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading some data and required scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! bash data/prepare-wmt14en2fr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see how to evaluate a pretrained model in fairseq. We'll download a pretrained model along with it's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! curl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf -"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This model uses a Byte Pair Encoding (BPE) vocabulary, so we’ll have to apply the encoding to the source text before it can be translated. This can be done with the apply_bpe.py script using the wmt14.en-fr.fconv-cuda/bpecodes file. @@ is used as a continuation marker and the original text can be easily recovered with e.g. sed s/@@ //g or by passing the --remove-bpe flag to fairseq-generate. Prior to BPE, input text needs to be tokenized using tokenizer.perl from mosesdecoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written a script to do it, but as a fun example, let's do it in Jupyter Notebook for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Why is it rare to discover new marine mammal species ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is it rare to discover new marine mam@@ mal species ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "data/subword-nmt/apply_bpe.py:345: ResourceWarning: unclosed file <_io.TextIOWrapper name='wmt14.en-fr.fconv-py/bpecodes' mode='r' encoding='UTF-8'>\n",
      "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$sentence\"\n",
    "SCRIPTS=data/mosesdecoder/scripts\n",
    "TOKENIZER=$SCRIPTS/tokenizer/tokenizer.perl\n",
    "CLEAN=$SCRIPTS/training/clean-corpus-n.perl\n",
    "NORM_PUNC=$SCRIPTS/tokenizer/normalize-punctuation.perl\n",
    "REM_NON_PRINT_CHAR=$SCRIPTS/tokenizer/remove-non-printing-char.perl\n",
    "BPEROOT=data/subword-nmt\n",
    "BPE_TOKENS=40000\n",
    "src=en\n",
    "tgt=fr\n",
    "echo $1 | \\\n",
    "            perl $NORM_PUNC $src | \\\n",
    "            perl $REM_NON_PRINT_CHAR | \\\n",
    "            perl $TOKENIZER -threads 8 -a -l $src > temp_tokenized.out         \n",
    "prep=wmt14.en-fr.fconv-py\n",
    "BPE_CODE=$prep/bpecodes\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < temp_tokenized.out > final_result.out\n",
    "rm temp_tokenized.out\n",
    "cat final_result.out\n",
    "rm final_result.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the very cool interactive feature of fairseq. Open shell, cd to this directory and type the copy the following command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MODEL_DIR=wmt14.en-fr.fconv-py\n",
    "fairseq-interactive \\\n",
    "    --path $MODEL_DIR/model.pt $MODEL_DIR \\\n",
    "    --beam 1 --source-lang en --target-lang fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(beam=1, bpe=None, buffer_size=1, cpu=False, criterion='cross_entropy', data='wmt14.en-fr.fconv-py', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', input='-', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=1, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14.en-fr.fconv-py/model.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='fr', task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 43771 types\n",
      "| [fr] dictionary: 43807 types\n",
      "| loading model(s) from wmt14.en-fr.fconv-py/model.pt\n",
      "| Type the input sentence and press return:\n",
      "S-0\tWhy is it rare to discover new marine mam@@ mal species ?\n",
      "H-0\t-0.1525058150291443\tPourquoi est @-@ il rare de découvrir de nouvelles espèces de mammifères marins ?\n",
      "P-0\t-0.2221 -0.3122 -0.1289 -0.2673 -0.1711 -0.1930 -0.1101 -0.1660 -0.1003 -0.0740 -0.1101 -0.0814 -0.1238 -0.0985 -0.1288\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_DIR=wmt14.en-fr.fconv-py\n",
    "echo \"Why is it rare to discover new marine mam@@ mal species ?\" | fairseq-interactive \\\n",
    "    --path $MODEL_DIR/model.pt $MODEL_DIR \\\n",
    "    --beam 1 --source-lang en --target-lang fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generation script produces three types of outputs: a line prefixed with O is a copy of the original source sentence; H is the hypothesis along with an average log-likelihood; and P is the positional score per token position, including the end-of-sentence marker which is omitted from the text. Let's do this in bash again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is it rare to discover new marine mammal species ?\n"
     ]
    }
   ],
   "source": [
    "!  echo \"Why is it rare to discover new marine mam@@ mal species ?\" | sed -r 's/(@@ )|(@@ ?$)//g' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Good! Now let's train a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairseq contains example pre-processing scripts for several translation datasets: IWSLT 2014 (German-English), WMT 2014 (English-French) and WMT 2014 (English-German). We will work with a part of WMT 2014 like we did in the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pre-process and binarize the IWSLT dataset run <code>bash prepare-wmt14en2fr.sh</code> like we did for the previous section. This will download the data, tokenize it, perform byte pair encoding and do a test train split on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Binaize the data, we do the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "TEXT=data/wmt14_en_fr\n",
    "fairseq-preprocess --source-lang en --target-lang fr \\\n",
    "  --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
    "  --destdir data-bin/wmt14_en_fr --thresholdtgt 5 --thresholdsrc 5 \\\n",
    "  --workers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ofcourse, we cannot see what is inside the binary line, but let's check what is in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict.en.txt\t   test.en-fr.fr.bin   train.en-fr.fr.bin  valid.en-fr.fr.bin\n",
      "dict.fr.txt\t   test.en-fr.fr.idx   train.en-fr.fr.idx  valid.en-fr.fr.idx\n",
      "test.en-fr.en.bin  train.en-fr.en.bin  valid.en-fr.en.bin\n",
      "test.en-fr.en.idx  train.en-fr.en.idx  valid.en-fr.en.idx\n"
     ]
    }
   ],
   "source": [
    "! ls data-bin/wmt14_en_fr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 233039\n",
      "the 221252\n",
      ". 164535\n",
      "of 120912\n",
      "to 110944\n"
     ]
    }
   ],
   "source": [
    "! head -5 data-bin/wmt14_en_fr/dict.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de 241097\n",
      ", 209932\n",
      ". 163838\n",
      "la 142626\n",
      "les 109031\n"
     ]
    }
   ],
   "source": [
    "! head -5 data-bin/wmt14_en_fr/dict.fr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairseq provides a lot of predefined architectures to choose from. For English-French, we will choose an architecure known to work well for the problem. In the next section, we will see how to define custom models in Fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p fairseq_models/checkpoints/fconv_wmt_en_fr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! fairseq-train data-bin/wmt14_en_fr \\\n",
    "  --lr 0.5 --clip-norm 0.1 --dropout 0.1 --max-tokens 3000 \\\n",
    "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "  --lr-scheduler fixed --force-anneal 50 \\\n",
    "  --arch fconv_wmt_en_fr --save-dir fairseq_models/checkpoints/fconv_wmt_en_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and Checking BLEU for our model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p fairseq_models/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fairseq-generate data-bin/wmt14_en_fr  \\\n",
    "  --path fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt \\\n",
    "  --beam 1 --batch-size 128 --remove-bpe --sacrebleu --force >> fairseq_models/logs/our_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(beam=1, bpe=None, cpu=False, criterion='cross_entropy', data='data-bin/wmt14_en_fr', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 21744 types\n",
      "| [fr] dictionary: 24256 types\n",
      "| loaded 3003 examples from: data-bin/wmt14_en_fr/test.en-fr.en\n",
      "| loaded 3003 examples from: data-bin/wmt14_en_fr/test.en-fr.fr\n",
      "| data-bin/wmt14_en_fr test en-fr 3003 examples\n",
      "| loading model(s) from fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt\n",
      "Namespace(beam=1, bpe=None, cpu=False, criterion='cross_entropy', data='data-bin/wmt14_en_fr', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 21744 types\n",
      "| [fr] dictionary: 24256 types\n"
     ]
    }
   ],
   "source": [
    "! head -10 fairseq_models/logs/our_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Translated 3003 sentences (97117 tokens) in 54.2s (55.45 sentences/s, 1793.20 tokens/s)\n",
      "| Generate test with beam=1: BLEU = 26.30 54.1/32.2/21.2/13.9 (BP = 0.982 ratio = 0.982 hyp_len = 101496 ref_len = 103343)\n"
     ]
    }
   ],
   "source": [
    "! tail -2 fairseq_models/logs/our_model.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating and Checking BLEU for the large Pretrained Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! curl https://dl.fbaipublicfiles.com/fairseq/data/wmt14.v2.en-fr.newstest2014.tar.bz2 | tar xvjf - -C data-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fairseq-generate data-bin/wmt14.en-fr.newstest2014  \\\n",
    "  --path wmt14.en-fr.fconv-py/model.pt \\\n",
    "  --beam 1 --batch-size 128 --remove-bpe --sacrebleu >> fairseq_models/logs/pretrained_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(beam=1, bpe=None, cpu=False, criterion='cross_entropy', data='data-bin/wmt14.en-fr.newstest2014', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14.en-fr.fconv-py/model.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 43771 types\n",
      "| [fr] dictionary: 43807 types\n",
      "| loaded 3003 examples from: data-bin/wmt14.en-fr.newstest2014/test.en-fr.en\n",
      "| loaded 3003 examples from: data-bin/wmt14.en-fr.newstest2014/test.en-fr.fr\n",
      "| data-bin/wmt14.en-fr.newstest2014 test en-fr 3003 examples\n",
      "| loading model(s) from wmt14.en-fr.fconv-py/model.pt\n",
      "S-647\tAir Raid Against Military Installations in Syria\n",
      "T-647\tRaid aérien contre des installations militaires en Syrie\n",
      "H-647\t-0.25782185792922974\tRaid aérien contre les installations militaires en Syrie\n"
     ]
    }
   ],
   "source": [
    "! head -10 fairseq_models/logs/pretrained_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Translated 3003 sentences (95125 tokens) in 52.2s (57.52 sentences/s, 1822.08 tokens/s)\n",
      "| Generate test with beam=1: BLEU = 43.12 69.0/50.5/39.0/30.4 (BP = 0.956 ratio = 0.957 hyp_len = 95480 ref_len = 99747)\n"
     ]
    }
   ],
   "source": [
    "! tail -2 fairseq_models/logs/pretrained_model.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing A Custom Model in FAIRSEQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extend fairseq by adding a new FairseqModel that encodes a source sentence with an LSTM and then passes the final hidden state to a second LSTM that decodes the target sentence (without attention)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we’ll define a simple LSTM Encoder and Decoder. All Encoders should implement the FairseqEncoder interface and Decoders should implement the FairseqDecoder interface. These interfaces themselves extend torch.nn.Module, so FairseqEncoders and FairseqDecoders can be written and used in the same ways as ordinary PyTorch Modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Encoder will embed the tokens in the source sentence, feed them to a torch.nn.LSTM and return the final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Decoder will predict the next word, conditioned on the Encoder’s final hidden state and an embedded representation of the previous target word – which is sometimes called input feeding or teacher forcing. More specifically, we’ll use a torch.nn.LSTM to produce a sequence of hidden states that we’ll project to the size of the output vocabulary to predict each target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve defined our Encoder and Decoder we must register our model with fairseq using the register_model() function decorator. Once the model is registered we’ll be able to use it with the existing Command-line Tools.\n",
    "\n",
    "All registered models must implement the BaseFairseqModel interface. For sequence-to-sequence models (i.e., any model with a single Encoder and Decoder), we can instead implement the FairseqModel interface.\n",
    "\n",
    "Create a small wrapper class in the same file and register it in fairseq with the name 'simple_lstm':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let’s define a named architecture with the configuration for our model. This is done with the register_model_architecture() function decorator. Thereafter this named architecture can be used with the --arch command-line argument, e.g., --arch tutorial_simple_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/fairseq/models\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "import os\n",
    "\n",
    "fairseq_file = os.path.dirname(fairseq.__file__)\n",
    "fairseq_path = os.path.join(fairseq_file, 'models')\n",
    "print(fairseq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$fairseq_path\"\n",
    "cp fairseq_models/custom_models/simple_lstm.py $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.py\n",
      "simple_lstm.py\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$fairseq_path\"\n",
    "ls $1 | grep lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just to show you how to train a custom model so we'll only train it for 3 epochs.\n",
    "Note that WMT dataset is large so you should train it for a long time. As we only trained for 3 epochs, the BLEU may be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p fairseq_models/checkpoints/tutorial_simple_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "fairseq-train data-bin/wmt14_en_fr \\\n",
    "  --arch tutorial_simple_lstm \\\n",
    "  --encoder-dropout 0.2 --decoder-dropout 0.2 \\\n",
    "  --optimizer adam --lr 0.005 --lr-shrink 0.5 \\\n",
    "  --max-tokens 12000 \\\n",
    "  --max-epoch 3 --save-dir fairseq_models/checkpoints/tutorial_simple_lstm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "\n",
    "fairseq-generate data-bin/wmt14_en_fr \\\n",
    "  --path fairseq_models/checkpoints/tutorial_simple_lstm/checkpoint_best.pt \\\n",
    "  --beam 5 --batch-size 128 \\\n",
    "  --remove-bpe --sacrebleu >> fairseq_models/logs/custom_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(beam=5, bpe=None, cpu=False, criterion='cross_entropy', data='data-bin/wmt14_en_fr', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='fairseq_models/checkpoints/tutorial_simple_lstm/checkpoint_best.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 21744 types\n",
      "| [fr] dictionary: 24256 types\n",
      "| loaded 3003 examples from: data-bin/wmt14_en_fr/test.en-fr.en\n",
      "| loaded 3003 examples from: data-bin/wmt14_en_fr/test.en-fr.fr\n",
      "| data-bin/wmt14_en_fr test en-fr 3003 examples\n",
      "| loading model(s) from fairseq_models/checkpoints/tutorial_simple_lstm/checkpoint_best.pt\n",
      "SimpleLSTMModel(\n",
      "  (encoder): SimpleLSTMEncoder(\n",
      "    (embed_tokens): Embedding(21744, 256, padding_idx=1)\n"
     ]
    }
   ],
   "source": [
    "!head -10 fairseq_models/logs/custom_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Translated 3003 sentences (100167 tokens) in 29.1s (103.16 sentences/s, 3441.03 tokens/s)\n",
      "| Generate test with beam=5: BLEU = 4.37 17.9/5.9/3.3/1.1 (BP = 1.000 ratio = 1.191 hyp_len = 123060 ref_len = 103343)\n"
     ]
    }
   ],
   "source": [
    "!tail -2 fairseq_models/logs/custom_model.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
