{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTkoaG9Jzkey"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdhZGkwZzke2"
   },
   "source": [
    "# ConvAI dataset\n",
    "\n",
    "![convai2-img](http://convai.io/personachat-example.png 'example')\n",
    "\n",
    "\n",
    "\n",
    "#### How would you solve this problem based on what you have learned till now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHYcyMp6XeoO"
   },
   "source": [
    "## Raw Data\n",
    "\n",
    "This is how raw input/target sample from training data of ConvAI dataset looks like:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"text\": \"your persona: i had a gig at local theater last night.\\nyour persona: i work as a stand up comedian.\\nyour persona: i come from a small town.\\nyour persona: my favorite drink is cuba libre.\\nyour persona: i did a few small roles in tv series.\\nwe all live in a yellow submarine , a yellow submarine . morning !\\nhi ! that is a great line for my next stand up .\\nlol . i am shy , anything to break the ice , and i am a beatles fan .\\ni can tell . i am not , you can see me in some tv shows\\nreally ? what shows ? i like tv , it makes me forget i do not like my family\\nwow , i wish i had a big family . i grew up in a very small town .\\ni did too . i do not get along with mine . they have no class .\\njust drink some cola with rum and you'll forget about them !\\nput the lime in the coconut as well . . .\\nnah , plain cuba libre , that's what we drank yesterday at the theater .\\ni prefer mojitos . watermelon or cucumber .\",  \n",
    "    \"labels\": [\"those are really yummy too , but not my favorite .\"], \n",
    "    \"reward\": 0, \n",
    "    \"episode_done\": true, \n",
    "    \"id\": \"convai2:self:no_cands\"\n",
    "}\n",
    "```\n",
    "\n",
    "*  \"text\" is the input we need = [your persona + dialogue so far]\n",
    "\n",
    "* \"labels\" is the output we need = [sentence that model should response]\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Here tokenization is done using a regular expression as in ParlAI framework (where the dataset is coming from!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RevAPsmCzke3"
   },
   "outputs": [],
   "source": [
    "RETOK = re.compile(r'\\w+|[^\\w\\s]|\\n', re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 940,
     "status": "ok",
     "timestamp": 1584066480343,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "W_JMpWkEzke5",
    "outputId": "f960d122-8d30-4cfd-edbd-7f5e28dec0a0"
   },
   "outputs": [],
   "source": [
    "# example of parsed text\n",
    "\n",
    "RETOK.findall('your persona: i had a gig at local theater last night.\\nyour persona: i work as a stand up comedian.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Eh_8POhzke8"
   },
   "source": [
    "# ConvAI dictionary\n",
    "\n",
    "The dataset is coming with a precomputed dictionary, it looks like this:\n",
    "\n",
    "For each word there is a corresponding count. Counts for special symbols are artificially presented / not real.\n",
    "\n",
    "```\n",
    "__null__\t1000000003\n",
    "__start__\t1000000002\n",
    "__end__\t1000000001\n",
    "__unk__\t1000000000\n",
    ".\t276863\n",
    "i\t270789\n",
    "you\t93655\n",
    "your\t91941\n",
    "a\t89140\n",
    "?\t85346\n",
    "persona\t80372\n",
    "\\n\t80365\n",
    ":\t80365\n",
    ",\t79513\n",
    "to\t79240\n",
    "my\t73999\n",
    "'\t68126\n",
    "do\t55199\n",
    "is\t53581\n",
    "the\t49955\n",
    "```\n",
    "\n",
    "`ChatDictionary` class implements the loading of that file with helpful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UboLeurzke9"
   },
   "outputs": [],
   "source": [
    "class ChatDictionary(object):\n",
    "    \"\"\"\n",
    "    Simple dict loader\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_file_path):\n",
    "        self.word2ind = {}  # word:index\n",
    "        self.ind2word = {}  # index:word\n",
    "        self.counts = {}  # word:count\n",
    "\n",
    "        dict_raw = open(dict_file_path, 'r').readlines()\n",
    "        \n",
    "        for i, w in enumerate(dict_raw):\n",
    "            _word, _count = w.strip().split('\\t')\n",
    "            if _word == '\\\\n':\n",
    "                _word = '\\n'\n",
    "            self.word2ind[_word] = i\n",
    "            self.ind2word[i] = _word\n",
    "            self.counts[_word] = _count\n",
    "            \n",
    "    def t2v(self, tokenized_text):\n",
    "        return [self.word2ind[w] if w in self.counts else self.word2ind['__unk__'] for w in tokenized_text]\n",
    "\n",
    "    def v2t(self, list_ids):\n",
    "        return ' '.join([self.ind2word[i] for i in list_ids])\n",
    "    \n",
    "    def pred2text(self, tensor):\n",
    "        result = []\n",
    "        for i in range(tensor.size(0)):\n",
    "            if tensor[i].item() == '__end__'  or tensor[i].item() == '__null__':  # null is pad\n",
    "                break\n",
    "            else:\n",
    "                result.append(self.ind2word[tensor[i].item()])\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8c6mwy5zke_"
   },
   "source": [
    "# Dataset class\n",
    "\n",
    "The `ChatDataset` should be familiar to all of you, nothing fancy there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGpA2vYIzkfA"
   },
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Json dataset wrapper\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_file_path, dictionary, dt='train'):\n",
    "        super().__init__()\n",
    "        \n",
    "        json_text = open(dataset_file_path, 'r').readlines()\n",
    "        self.samples = []\n",
    "        \n",
    "        for sample in tqdm(json_text):\n",
    "            sample = sample.rstrip()\n",
    "            sample = json.loads(sample)\n",
    "            _inp_toked = RETOK.findall(sample['text'])\n",
    "            _inp_toked_id = dictionary.t2v(_inp_toked)\n",
    "\n",
    "            sample['text_vec'] = torch.tensor(_inp_toked_id, dtype=torch.long)\n",
    "            \n",
    "            # train and valid have different key names for target\n",
    "            if dt == 'train':\n",
    "                _tar_toked = RETOK.findall(sample['labels'][0]) + ['__end__']\n",
    "            elif dt == 'valid':\n",
    "                _tar_toked = RETOK.findall(sample['eval_labels'][0]) + ['__end__']\n",
    "                \n",
    "            _tar_toked_id = dictionary.t2v(_tar_toked)\n",
    "            \n",
    "            sample['target_vec'] = torch.tensor(_tar_toked_id, dtype=torch.long)\n",
    "            \n",
    "            self.samples.append(sample)\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        return self.samples[i]['text_vec'], self.samples[i]['target_vec']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nuMz60avzkfC"
   },
   "source": [
    "# Padding and batching\n",
    "\n",
    "`pad_tensor` function implements padding of a given tensor using the specified PAD token.\n",
    "\n",
    "`batchify` uses both previous function to make a minibatch which is ready to be packed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtILdxv1zkfD"
   },
   "outputs": [],
   "source": [
    "def pad_tensor(tensors, sort=True, pad_token=0):\n",
    "    rows = len(tensors)\n",
    "    lengths = [len(i) for i in tensors]\n",
    "    max_t = max(lengths)\n",
    "        \n",
    "    output = tensors[0].new(rows, max_t)\n",
    "    output.fill_(pad_token)  # 0 is a pad token here\n",
    "    \n",
    "    for i, (tensor, length) in enumerate(zip(tensors, lengths)):\n",
    "        output[i,:length] = tensor\n",
    "\n",
    "    return output, lengths\n",
    "\n",
    "\n",
    "\n",
    "def batchify(batch):\n",
    "    inputs = [i[0] for i in batch]\n",
    "    labels = [i[1] for i in batch]\n",
    "    \n",
    "    input_vecs, input_lens = pad_tensor(inputs)\n",
    "    label_vecs, label_lens = pad_tensor(labels)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"text_vecs\": input_vecs,\n",
    "        \"text_lens\": input_lens,\n",
    "        \"target_vecs\": label_vecs,\n",
    "        \"target_lens\": label_lens,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13104,
     "status": "ok",
     "timestamp": 1584066492537,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "p8PmLljdzkfF",
    "outputId": "ee9696b8-2471-458e-8eb4-7a6c3d0a01a9"
   },
   "outputs": [],
   "source": [
    "# loading datasets and dictionary\n",
    "\n",
    "# downloading pretrained models and data\n",
    "\n",
    "### DOWNLOADING THE FILES\n",
    "import os\n",
    "\n",
    "### persona chat dataset\n",
    "if not os.path.exists('./dict'):\n",
    "    !wget \"https://nyu.box.com/shared/static/sj9f87tofpicll89xbc154pmbztu5q4h\" -O './dict'\n",
    "if not os.path.exists('./train.jsonl'):\n",
    "    !wget \"https://nyu.box.com/shared/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\" -O './train.jsonl'\n",
    "if not os.path.exists('./valid.jsonl'):\n",
    "    !wget \"https://nyu.box.com/shared/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\" -O './valid.jsonl'\n",
    "\n",
    "if not os.path.exists('./chat_model_best_22.pt'):\n",
    "    !wget \"https://nyu.box.com/shared/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\" -O './chat_model_best_22.pt'\n",
    "\n",
    "chat_dict = ChatDictionary('./dict')\n",
    "train_dataset = ChatDataset('./train.jsonl', chat_dict)\n",
    "valid_dataset = ChatDataset('./valid.jsonl', chat_dict, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the input to our model looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13084,
     "status": "ok",
     "timestamp": 1584066492539,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "k-1axU0H8XlI",
    "outputId": "ee22d008-e43b-4f8c-989a-7b2d0908d27e"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owl9sDsrzkfH"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, collate_fn=batchify, batch_size=256)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMazqS5YzkfK"
   },
   "source": [
    "# Seq2seq model with attention\n",
    "\n",
    "![Imgur](https://ikulikov.name/files/seq2seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qjSc8aPzkfK"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Encodes the input context.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx=0, dropout=0, shared_lt=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        if shared_lt is None:\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.embed_size, pad_idx)\n",
    "        else:\n",
    "          # share embedding with decoder\n",
    "            self.embedding = shared_lt\n",
    "            \n",
    "        self.gru = nn.GRU(\n",
    "            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, text_vec, text_lens, hidden=None):\n",
    "        embedded = self.embedding(text_vec)\n",
    "        # assign 1 if not equal to pad_idx, otherwisw 0\n",
    "        attention_mask = text_vec.ne(self.pad_idx)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        return output, hidden, attention_mask\n",
    "\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"Generates a sequence of tokens in response to context.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size, 0)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        self.attention = AttentionLayer(self.hidden_size, self.embed_size)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        self.longest_label = 100\n",
    "\n",
    "    def decode_forced(self, ys, encoder_states, xs_lens):\n",
    "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
    "        \n",
    "        batch_size = ys.size(0)\n",
    "        target_length = ys.size(1)\n",
    "        longest_label = max(target_length, self.longest_label)\n",
    "        \n",
    "        starts = torch.Tensor([1]).long().to(self.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
    "        \n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        y_in = ys.narrow(1, 0, ys.size(1) - 1)\n",
    "        decoder_input = torch.cat([starts, y_in], 1)\n",
    "        decoder_output, decoder_hidden, attn_w_log = self.forward(decoder_input, encoder_hidden, encoder_states)\n",
    "        _, preds = decoder_output.max(dim=2)\n",
    "        \n",
    "        return decoder_output, preds, attn_w_log\n",
    "\n",
    "    def forward(self, text_vec, decoder_hidden, encoder_states):\n",
    "        emb = self.embedding(text_vec)\n",
    "        emb = self.dropout(emb)\n",
    "        seqlen = text_vec.size(1)\n",
    "        encoder_output, _, attention_mask = encoder_states\n",
    "        \n",
    "        output = []\n",
    "        attn_w_log = []\n",
    "\n",
    "        for i in range(seqlen):\n",
    "            decoder_output, decoder_hidden = self.gru(emb[:,i,:].unsqueeze(1), decoder_hidden)\n",
    "            \n",
    "            # compute attention at each time step\n",
    "            decoder_output_attended, attn_weights = self.attention(decoder_output, decoder_hidden, encoder_output, attention_mask)\n",
    "            output.append(decoder_output_attended)\n",
    "            attn_w_log.append(attn_weights)\n",
    "            \n",
    "        output = torch.cat(output, dim=1).to(text_vec.device)\n",
    "        scores = self.out(output)\n",
    "        \n",
    "        return scores, decoder_hidden, attn_w_log\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, embedding_size):\n",
    "        super().__init__()\n",
    "        input_dim = hidden_size\n",
    "\n",
    "        self.linear_out = nn.Linear(hidden_size+input_dim, input_dim, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, decoder_output, decoder_hidden, encoder_output, attention_mask):\n",
    "\n",
    "        batch_size, seq_length, hidden_size = encoder_output.size()\n",
    "\n",
    "        encoder_output_t = encoder_output.transpose(1,2)\n",
    "        \n",
    "        attention_scores = torch.bmm(decoder_output, encoder_output_t).squeeze(1)\n",
    "\n",
    "        attention_scores.masked_fill_((~attention_mask), -10e5)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "\n",
    "        mix = torch.bmm(attention_weights.unsqueeze(1), encoder_output)\n",
    "\n",
    "        combined = torch.cat((decoder_output.squeeze(1), mix.squeeze(1)), dim=1)\n",
    "\n",
    "        output = self.linear_out(combined).unsqueeze(1)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "    \n",
    "class seq2seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic seq2seq model with attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, opts):\n",
    "\n",
    "        super().__init__()\n",
    "        self.opts = opts\n",
    "        \n",
    "        self.decoder = DecoderRNN(\n",
    "                                    vocab_size=self.opts['vocab_size'],\n",
    "                                    embed_size=self.opts['embedding_size'],\n",
    "                                    hidden_size=self.opts['hidden_size'],\n",
    "                                    num_layers=self.opts['num_layers_dec'],\n",
    "                                    dropout=self.opts['dropout'],\n",
    "                                )\n",
    "        \n",
    "        self.encoder = EncoderRNN(\n",
    "                                    vocab_size=self.opts['vocab_size'],\n",
    "                                    embed_size=self.opts['embedding_size'],\n",
    "                                    hidden_size=self.opts['hidden_size'],\n",
    "                                    num_layers=self.opts['num_layers_enc'],\n",
    "                                    dropout=self.opts['dropout'],\n",
    "                                    shared_lt=self.decoder.embedding\n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        \n",
    "    def eval(self):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tynCTwJUzkfN"
   },
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "load_pretrained = True\n",
    "    \n",
    "if load_pretrained is True:\n",
    "    if current_device == 'cuda':\n",
    "        model_pt = torch.load('./chat_model_best_22.pt')\n",
    "    else:\n",
    "        model_pt = torch.load('./chat_model_best_22.pt', map_location=torch.device('cpu'))\n",
    "    opts = model_pt['opts']\n",
    "    \n",
    "    model = seq2seq(opts)\n",
    "    model.load_state_dict(model_pt['state_dict'])\n",
    "    model.to(current_device)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    opts = {}\n",
    "\n",
    "    opts['vocab_size'] = len(chat_dict)\n",
    "    opts['hidden_size'] = 512\n",
    "    opts['embedding_size'] = 256\n",
    "    opts['num_layers_enc'] = 2\n",
    "    opts['num_layers_dec'] = 2\n",
    "    opts['dropout'] = 0.3\n",
    "    opts['encoder_shared_lt'] = True\n",
    "\n",
    "    model = seq2seq(opts)\n",
    "    model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSCRkORjzkfP"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14473,
     "status": "error",
     "timestamp": 1584066493971,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "V7A9Yrv0zkfR",
    "outputId": "10960239-8507-4059-ef77-33889b3d7e61"
   },
   "outputs": [],
   "source": [
    "plot_cache = []\n",
    "\n",
    "best_val_loss = 100\n",
    "\n",
    "if not load_pretrained:\n",
    "    for epoch in range(100):\n",
    "\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "        sum_tokens = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text_vecs = batch['text_vecs'].to('cuda')\n",
    "            target_vecs = batch['target_vecs'].to('cuda')\n",
    "\n",
    "            encoded = model.encoder(text_vecs, batch['text_lens'])\n",
    "\n",
    "            decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n",
    "\n",
    "            scores = decoder_output.view(-1, decoder_output.size(-1))\n",
    "\n",
    "            loss = criterion(scores, target_vecs.view(-1))\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            num_tokens = target_vecs.ne(0).long().sum().item()\n",
    "            loss /= num_tokens\n",
    "\n",
    "            sum_tokens += num_tokens\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                avg_train_loss = sum_loss/sum_tokens\n",
    "                print(\"iter {} train loss = {}\".format(i, sum_loss/sum_tokens))\n",
    "\n",
    "        val_loss = 0\n",
    "        val_tokens = 0\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "\n",
    "            text_vecs = batch['text_vecs'].to('cuda')\n",
    "            target_vecs = batch['target_vecs'].to('cuda')\n",
    "\n",
    "            encoded = model.encoder(text_vecs, batch['text_lens'])\n",
    "\n",
    "            decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n",
    "\n",
    "            scores = decoder_output.view(-1, decoder_output.size(-1))\n",
    "\n",
    "            loss = criterion(scores, target_vecs.view(-1))\n",
    "\n",
    "            num_tokens = target_vecs.ne(0).long().sum().item()\n",
    "\n",
    "            val_tokens += num_tokens\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss/val_tokens\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(\"Epoch {} valid loss = {}\".format(epoch, avg_val_loss))\n",
    "\n",
    "        plot_cache.append( (avg_train_loss, avg_val_loss) )\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "            torch.save({\n",
    "            'state_dict': model.state_dict(),\n",
    "            'opts': opts,\n",
    "            'plot_cache': plot_cache,\n",
    "                }, f'./chat_model_best_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H06IUKipzkfU"
   },
   "outputs": [],
   "source": [
    "if load_pretrained is True:\n",
    "    plot_cache = model_pt['plot_cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1584066520693,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "8yU7rhB3zkfW",
    "outputId": "beacd0be-96b0-4a84-f929-7aa6eff2d9e0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "epochs = numpy.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [i[0] for i in plot_cache], label='Train loss')\n",
    "plt.plot(epochs, [i[1] for i in plot_cache], label='Valid loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Loss curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1584066520889,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "rHBkh9jFzkfZ",
    "outputId": "653dc30c-240c-43fc-a870-55acf0ed5a1c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "epochs = numpy.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in plot_cache], label='Train ppl')\n",
    "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in plot_cache], label='Valid ppl')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PPL curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqnniuRCzkfm"
   },
   "outputs": [],
   "source": [
    "# saving the model, be careful to nor overwrite a good model here\n",
    "if False:\n",
    "    torch.save({\n",
    "        'state_dict': model.state_dict(),\n",
    "        'opts': opts,\n",
    "        'plot_cache': plot_cache,\n",
    "    }, './chat_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAg5sK8UtD9s"
   },
   "source": [
    "## Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCa2Nhjszkfc"
   },
   "outputs": [],
   "source": [
    "def greedy_search(model, batch, batch_size, max_len=100):\n",
    "    model.eval()\n",
    "        \n",
    "    text_vecs = batch['text_vecs'].to(current_device)\n",
    "\n",
    "    encoded = model.encoder(text_vecs, batch['text_lens'])\n",
    "    \n",
    "    encoder_output, encoder_hidden, attention_mask = encoded\n",
    "        \n",
    "    # 1 is __start__\n",
    "    starts = torch.Tensor([1]).long().to(model.decoder.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # greedy decoding here        \n",
    "    preds = [starts]\n",
    "    scores = []\n",
    "\n",
    "    # track if each sample in the mini batch is finished\n",
    "    # if all finished, stop predicting\n",
    "    finish_mask = torch.Tensor([0]*batch_size).byte().to(model.decoder.embedding.weight.device)\n",
    "    xs = starts\n",
    "    _attn_w_log = []\n",
    "\n",
    "    for ts in range(max_len):\n",
    "        decoder_output, decoder_hidden, attn_w_log = model.decoder(xs, decoder_hidden, encoded)  # decoder_output: [batch, time, vocab]\n",
    "        \n",
    "        _scores, _preds = torch.log_softmax(decoder_output, dim=-1).max(dim=-1)\n",
    "        \n",
    "        preds.append(_preds)\n",
    "        _attn_w_log.append(attn_w_log)\n",
    "        scores.append(_scores.view(-1)*(finish_mask == 0).float())\n",
    "\n",
    "        finish_mask += (_preds == 2).byte().view(-1)\n",
    "        \n",
    "        if not (torch.any(~finish_mask.bool())):\n",
    "            break\n",
    "        \n",
    "        xs = _preds\n",
    "    \n",
    "    preds = torch.cat(preds, dim=-1)\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bt-R4axzzkfe"
   },
   "outputs": [],
   "source": [
    "# artificial example (try removing the question mark, result will be different)\n",
    "#inputs = RETOK.findall(\"hello , where are you from?\")\n",
    "inputs = RETOK.findall(\"your persona: i live in texas.\\n hello , where are you ? ?\")\n",
    "\n",
    "test_batch = {\n",
    "    'text_vecs': torch.tensor([chat_dict.t2v(inputs)], dtype=torch.long, device=model.decoder.embedding.weight.device),\n",
    "    'text_lens': torch.tensor([len(inputs)], dtype=torch.long)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1091,
     "status": "ok",
     "timestamp": 1584066520893,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "fi_ZrKq4zkfg",
    "outputId": "c3b9192b-3434-4f3c-b46a-891733cd7abc"
   },
   "outputs": [],
   "source": [
    "output = greedy_search(model, test_batch, 1)\n",
    "\n",
    "chat_dict.v2t(output[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFPNPNEatYuT"
   },
   "source": [
    "### Nucleus Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmccoyUof4aS"
   },
   "source": [
    "\n",
    "Reference to the original paper: https://openreview.net/pdf?id=rygGQyrFvH Please read section 3.1 for details. Here we give the needed excerpt:\n",
    "\n",
    "$$\\begin{aligned} P^{\\prime}\\left(x | x_{1: i-1}\\right) &=\\left\\{\\begin{array}{ll}P\\left(x | x_{1: i-1}\\right) / p^{\\prime} & \\text { if } x \\in V^{\\left(p_{\\text {nucleus }}\\right)} \\\\ 0 & \\text { otherwise }\\end{array}\\right.\\\\ p^{\\prime} &=\\sum_{x \\in V^{\\left(p_{\\text {nucleus }}\\right)}} P\\left(x | x_{1: i-1}\\right) \\end{aligned}$$\n",
    "\n",
    "where $V(p_{nucleus}) ⊂ V$ is a top-p vocabulary which is defined as a smallest subset such that:\n",
    "$$\\sum_{x \\in V^{\\left(p_{\\text {nucleus }}\\right)}} P\\left(x | x_{1: i-1}\\right) \\geq p_{\\text {nucleus }}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-YzYzektYuU"
   },
   "outputs": [],
   "source": [
    "# modified from Top-K and Nucleus Sampling from ParlAI\n",
    "class Nucleus(object):\n",
    "  def __init__(self,p):\n",
    "    self.p=p\n",
    "    \n",
    "  def select_paths(self, logprobs):\n",
    "        probs = torch.softmax(logprobs, dim=-1)\n",
    "        sprobs, sinds = probs.sort(dim=-1, descending=True)\n",
    "        org=sprobs.clone()\n",
    "        mask = (sprobs.cumsum(dim=-1) - sprobs[:, :1]) >= self.p\n",
    "        sprobs[mask] = 0\n",
    "        sprobs.div_(sprobs.sum(dim=-1).unsqueeze(1))\n",
    "        choice = torch.multinomial(sprobs[0][0], 1)\n",
    "        tok_id = sinds[0][0][choice]\n",
    "        #  back to log\n",
    "        score = org[0][0][choice].log().detach().data.cpu().numpy()[0]\n",
    "        return (tok_id, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-bRDwekN_hy"
   },
   "outputs": [],
   "source": [
    "def sampling_with_nucleus(model, batch, batch_size, p, previous_hypo=None, verbose=True, sample=100):\n",
    "    model.eval()\n",
    "        \n",
    "    text_vecs = batch['text_vecs'].to(current_device)\n",
    "    encoded = model.encoder(text_vecs, batch['text_lens']) \n",
    "    encoder_output, encoder_hidden, attention_mask = encoded\n",
    "        \n",
    "    all_logpro=[]\n",
    "    unique_tokens=set()\n",
    "\n",
    "    for times in range(sample):\n",
    "      starts = torch.Tensor([1]).long().to(model.decoder.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
    "      decoder_hidden = encoder_hidden\n",
    "      preds = []\n",
    "      scores = []\n",
    "      xs = starts\n",
    "      _attn_w_log = []\n",
    "      for ts in range(50):\n",
    "          score, decoder_hidden, attn_w_log = model.decoder(xs, decoder_hidden, encoded)  # decoder_output: [batch, time, vocab]\n",
    "          N = Nucleus(p)\n",
    "          tok_ids,sc = N.select_paths(score)\n",
    "          t_tok_ids = tok_ids.data.cpu().numpy()[0]\n",
    "          preds.append(t_tok_ids)\n",
    "          unique_tokens.add(t_tok_ids)\n",
    "          _attn_w_log.append(attn_w_log)\n",
    "          scores.append(sc)\n",
    "          eos_token = chat_dict.word2ind['__end__']\n",
    "          if tok_ids==eos_token:\n",
    "            break\n",
    "          xs = torch.Tensor([t_tok_ids]).long().to(model.decoder.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
    "      all_logpro.append(sum(scores))\n",
    "      # printing some sample results\n",
    "      pred_sentence = chat_dict.v2t(preds)\n",
    "      if verbose:\n",
    "        print(pred_sentence)\n",
    "    avg_logpro = sum(all_logpro)/len(all_logpro)\n",
    "    unique_num = len(unique_tokens)\n",
    "    return preds, avg_logpro, unique_num, pred_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBckP2IHtYua"
   },
   "source": [
    "### Some samples from our sampling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gRamdDOtYud"
   },
   "outputs": [],
   "source": [
    "valid_loader_single = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=1)\n",
    "valid_sample = next(iter(valid_loader_single))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1192,
     "status": "ok",
     "timestamp": 1584066521051,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "vT7XKzk5y6WF",
    "outputId": "23e97ec9-5f16-4926-9768-5c805fd3d839"
   },
   "outputs": [],
   "source": [
    "print(\"Input:\\n\", chat_dict.v2t(valid_sample['text_vecs'][0].tolist()))\n",
    "print(\"Target output:\\n\", chat_dict.v2t(valid_sample['target_vecs'][0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 946,
     "status": "ok",
     "timestamp": 1584066537591,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "mbtVutJktYug",
    "outputId": "dd65ca77-ea35-41f3-8e42-686bfba1ba1c"
   },
   "outputs": [],
   "source": [
    "p_values=[0.1,0.5,0.9]\n",
    "n_samples = 5\n",
    "\n",
    "p_logprob=[]\n",
    "p_u_num=[]\n",
    "for p in p_values:\n",
    "  print(\"When p={}, generate {} example decoding sentence:\".format(p, n_samples))\n",
    "  output,res,u_num, _ = sampling_with_nucleus(model, valid_sample, 1, p, sample=n_samples)\n",
    "  p_logprob.append(round(res,2))\n",
    "  p_u_num.append(u_num)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDeZ1OtEzkfp"
   },
   "source": [
    "# Beam search\n",
    "\n",
    "We have learnt what is a beam search. But how to implement it? There are plenty of possible design choices along the way. Here we consider a so called `Beam` class which handles intermediate information.\n",
    "\n",
    "![beamtree with hyptail](https://ikulikov.name/files/beamtree-example.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0-a9bg0zkfq"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from operator import attrgetter\n",
    "\n",
    "class _HypothesisTail(object):\n",
    "    \"\"\"Hold some bookkeeping about a hypothesis.\"\"\"\n",
    "\n",
    "    # use slots because we don't want dynamic attributes here\n",
    "    __slots__ = ['timestep', 'hypid', 'score', 'tokenid']\n",
    "\n",
    "    def __init__(self, timestep, hypid, score, tokenid):\n",
    "        self.timestep = timestep\n",
    "        self.hypid = hypid\n",
    "        self.score = score\n",
    "        self.tokenid = tokenid\n",
    "\n",
    "class Beam(object):\n",
    "    \"\"\"\n",
    "    This class serves to keep info about partial hypothesis and perform the beam step\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        beam_size,\n",
    "        padding_token=0,\n",
    "        bos_token=1,\n",
    "        eos_token=2,\n",
    "        min_length=3,\n",
    "        min_n_best=3,\n",
    "        device='cpu',\n",
    "        # for iterbeam below\n",
    "        similarity_metric='hamming',\n",
    "        similarity_threshold=0,\n",
    "    ):\n",
    "        \n",
    "        self.beam_size = beam_size\n",
    "        self.min_length = min_length\n",
    "        self.eos = eos_token\n",
    "        self.bos = bos_token\n",
    "        self.pad = padding_token\n",
    "        self.device = device\n",
    "        # recent score for each hypo in the beam\n",
    "        self.scores = None\n",
    "        # self.scores values per each time step\n",
    "        self.all_scores = [torch.Tensor([0.0] * beam_size).to(self.device)]\n",
    "        # backtracking id to hypothesis at previous time step\n",
    "        self.bookkeep = []\n",
    "        # output tokens at each time step\n",
    "        self.outputs = [\n",
    "            torch.Tensor(self.beam_size).long().fill_(self.bos).to(self.device)\n",
    "        ]\n",
    "        # keeps tuples (score, time_step, hyp_id)\n",
    "        self.finished = []\n",
    "        self.eos_top = False\n",
    "        self.eos_top_ts = None\n",
    "        self.n_best_counter = 0\n",
    "        self.min_n_best = min_n_best\n",
    "        self.partial_hyps = [[self.bos] for i in range(beam_size)]\n",
    "\n",
    "        # iterbeam related below\n",
    "        self.history_hyps = []\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.banned_tokens = set()\n",
    "        \n",
    "    def get_output_from_current_step(self):\n",
    "        \"\"\"Get the output at the current step.\"\"\"\n",
    "        return self.outputs[-1]\n",
    "\n",
    "    def get_backtrack_from_current_step(self):\n",
    "        \"\"\"Get the backtrack at the current step.\"\"\"\n",
    "        return self.bookkeep[-1]\n",
    "    \n",
    "    ##################### ITER-BEAM BLOCKING PART START #####################\n",
    "    \n",
    "    def hamming_distance(self, t1, t2):\n",
    "        dist = 0\n",
    "        for tok1, tok2 in zip(t1,t2):\n",
    "            if tok1 != tok2:\n",
    "                dist += 1\n",
    "        return dist\n",
    "    \n",
    "    def edit_distance(self, t1, t2):\n",
    "        import editdistance\n",
    "        dist = editdistance.eval(t1, t2)\n",
    "        return dist\n",
    "                \n",
    "    def similarity_check(self, active_hyp, previous_hyps, metric='hamming', threshold=0):\n",
    "        banned_tokens = []\n",
    "        active_len = len(active_hyp)\n",
    "        for observed_hyp, _banned_tokens in previous_hyps.items():\n",
    "            if len(observed_hyp) != active_len:\n",
    "                continue\n",
    "            if metric == 'hamming':\n",
    "                dist = self.hamming_distance(observed_hyp, active_hyp)\n",
    "            if metric == 'edit':\n",
    "                dist = self.edit_distance(observed_hyp, active_hyp)\n",
    "            if dist <= threshold:\n",
    "                banned_tokens.extend(_banned_tokens)\n",
    "                    \n",
    "        return list(set(banned_tokens))\n",
    "    \n",
    "    ##################### ITER-BEAM BLOCKING PART END ########################\n",
    "    \n",
    "    def select_paths(self, logprobs, prior_scores, previous_hyps):\n",
    "        \"\"\"Select the next vocabulary item in these beams.\"\"\"\n",
    "        # beam search actually looks over all hypotheses together so we flatten\n",
    "        beam_scores = logprobs + prior_scores.unsqueeze(1).expand_as(logprobs)\n",
    "        \n",
    "        # iterbeam blocking part\n",
    "        current_length = len(self.all_scores)\n",
    "        if len(previous_hyps) > 0 and current_length > 0:\n",
    "            for hyp_id in range(beam_scores.size(0)):\n",
    "                active_hyp = tuple(self.partial_hyps[hyp_id])\n",
    "                banned_tokens = self.similarity_check(active_hyp, previous_hyps, metric=self.similarity_metric, threshold=self.similarity_threshold)\n",
    "                if len(banned_tokens) > 0:\n",
    "                    beam_scores[:, banned_tokens] = -10e5\n",
    "            \n",
    "        flat_beam_scores = beam_scores.view(-1)\n",
    "        best_scores, best_idxs = torch.topk(flat_beam_scores, self.beam_size, dim=-1)\n",
    "        voc_size = logprobs.size(-1)\n",
    "\n",
    "        # get the backtracking hypothesis id as a multiple of full voc_sizes\n",
    "        hyp_ids = best_idxs / voc_size\n",
    "        # get the actual word id from residual of the same division\n",
    "        tok_ids = best_idxs % voc_size\n",
    "        \n",
    "        return (hyp_ids, tok_ids, best_scores)\n",
    "    \n",
    "    def advance(self, logprobs, previous_hyps):\n",
    "        \"\"\"Advance the beam one step.\"\"\"\n",
    "        current_length = len(self.all_scores) - 1\n",
    "        if current_length < self.min_length:\n",
    "            # penalize all eos probs to make it decode longer\n",
    "            for hyp_id in range(logprobs.size(0)):\n",
    "                logprobs[hyp_id][self.eos] = -10e5\n",
    "\n",
    "        if self.scores is None:\n",
    "            logprobs = logprobs[0:1]  # we use only the first hyp now, since they are all same\n",
    "            self.scores = torch.zeros(1).type_as(logprobs).to(logprobs.device)\n",
    "            \n",
    "        hyp_ids, tok_ids, self.scores = self.select_paths(logprobs, self.scores, previous_hyps)\n",
    "        \n",
    "        # clone scores here to avoid referencing penalized EOS in the future!\n",
    "        self.all_scores.append(self.scores.clone())\n",
    "\n",
    "        self.outputs.append(tok_ids)\n",
    "        self.bookkeep.append(hyp_ids)\n",
    "        self.partial_hyps = [\n",
    "            self.partial_hyps[hyp_ids[i]] + [tok_ids[i].item()]\n",
    "            for i in range(self.beam_size)\n",
    "        ]\n",
    "        self.history_hyps.extend(self.partial_hyps)\n",
    "\n",
    "        #  check new hypos for eos label, if we have some, add to finished\n",
    "        for hypid in range(self.beam_size):\n",
    "            if self.outputs[-1][hypid] == self.eos:\n",
    "                self.scores[hypid] = -10e5\n",
    "                #  this is finished hypo, adding to finished\n",
    "                eostail = _HypothesisTail(\n",
    "                    timestep=len(self.outputs) - 1,\n",
    "                    hypid=hypid,\n",
    "                    score=self.all_scores[-1][hypid],\n",
    "                    tokenid=self.eos,\n",
    "                )\n",
    "                self.finished.append(eostail)\n",
    "                self.n_best_counter += 1\n",
    "\n",
    "        if self.outputs[-1][0] == self.eos:\n",
    "            self.eos_top = True\n",
    "            if self.eos_top_ts is None:\n",
    "                self.eos_top_ts = len(self.outputs) - 1\n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"Return whether beam search is complete.\"\"\"\n",
    "        return self.eos_top and self.n_best_counter >= self.min_n_best\n",
    "\n",
    "    def get_top_hyp(self):\n",
    "        \"\"\"\n",
    "        Get single best hypothesis.\n",
    "        :return: hypothesis sequence and the final score\n",
    "        \"\"\"\n",
    "        return self._get_rescored_finished(n_best=1)[0]\n",
    "\n",
    "    def _get_hyp_from_finished(self, hypothesis_tail):\n",
    "        \"\"\"\n",
    "        Extract hypothesis ending with EOS at timestep with hyp_id.\n",
    "        :param timestep:\n",
    "            timestep with range up to len(self.outputs) - 1\n",
    "        :param hyp_id:\n",
    "            id with range up to beam_size - 1\n",
    "        :return:\n",
    "            hypothesis sequence\n",
    "        \"\"\"\n",
    "        hyp_idx = []\n",
    "        endback = hypothesis_tail.hypid\n",
    "        for i in range(hypothesis_tail.timestep, -1, -1):\n",
    "            hyp_idx.append(\n",
    "                _HypothesisTail(\n",
    "                    timestep=i,\n",
    "                    hypid=endback,\n",
    "                    score=self.all_scores[i][endback],\n",
    "                    tokenid=self.outputs[i][endback],\n",
    "                )\n",
    "            )\n",
    "            endback = self.bookkeep[i - 1][endback]\n",
    "\n",
    "        return hyp_idx\n",
    "\n",
    "    def _get_pretty_hypothesis(self, list_of_hypotails):\n",
    "        \"\"\"Return hypothesis as a tensor of token ids.\"\"\"\n",
    "        return torch.stack([ht.tokenid for ht in reversed(list_of_hypotails)])\n",
    "\n",
    "    def _get_rescored_finished(self, n_best=None, add_length_penalty=False):\n",
    "        \"\"\"\n",
    "        Return finished hypotheses according to adjusted scores.\n",
    "        Score adjustment is done according to the Google NMT paper, which\n",
    "        penalizes long utterances.\n",
    "        :param n_best:\n",
    "            number of finalized hypotheses to return\n",
    "        :return:\n",
    "            list of (tokens, score) pairs, in sorted order, where:\n",
    "              - tokens is a tensor of token ids\n",
    "              - score is the adjusted log probability of the entire utterance\n",
    "        \"\"\"\n",
    "        # if we never actually finished, force one\n",
    "        if not self.finished:\n",
    "            self.finished.append(\n",
    "                _HypothesisTail(\n",
    "                    timestep=len(self.outputs) - 1,\n",
    "                    hypid=0,\n",
    "                    score=self.all_scores[-1][0],\n",
    "                    tokenid=self.eos,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        rescored_finished = []\n",
    "        for finished_item in self.finished:\n",
    "            if add_length_penalty:\n",
    "                current_length = finished_item.timestep + 1\n",
    "                # these weights are from Google NMT paper\n",
    "                length_penalty = math.pow((1 + current_length) / 6, 0.65)\n",
    "            else:\n",
    "                length_penalty = 1\n",
    "            rescored_finished.append(\n",
    "                _HypothesisTail(\n",
    "                    timestep=finished_item.timestep,\n",
    "                    hypid=finished_item.hypid,\n",
    "                    score=finished_item.score / length_penalty,\n",
    "                    tokenid=finished_item.tokenid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Note: beam size is almost always pretty small, so sorting is cheap enough\n",
    "        srted = sorted(rescored_finished, key=attrgetter('score'), reverse=True)\n",
    "\n",
    "        if n_best is not None:\n",
    "            srted = srted[:n_best]\n",
    "\n",
    "        return [\n",
    "            (self._get_pretty_hypothesis(self._get_hyp_from_finished(hyp)), hyp.score)\n",
    "            for hyp in srted\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EDSKb1uzkfs"
   },
   "source": [
    "# Model manipulation\n",
    "\n",
    "As you noticed, after the topk we select the best chosen tails of current hypotheses. And the corresponding previous hypotheses ids can be mixed in order. *We must reorder the hidden buffers of our model. Otherwise, the decoding will be wrong.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdMugjnfzkfs"
   },
   "outputs": [],
   "source": [
    "def reorder_encoder_states(encoder_states, indices):\n",
    "        \"\"\"Reorder encoder states according to a new set of indices.\"\"\"\n",
    "        enc_out, hidden, attention_mask = encoder_states\n",
    "\n",
    "        # LSTM or GRU/RNN hidden state?\n",
    "        if isinstance(hidden, torch.Tensor):\n",
    "            hid, cell = hidden, None\n",
    "        else:\n",
    "            hid, cell = hidden\n",
    "\n",
    "        if not torch.is_tensor(indices):\n",
    "            # cast indices to a tensor if needed\n",
    "            indices = torch.LongTensor(indices).to(hid.device)\n",
    "\n",
    "        hid = hid.index_select(1, indices)\n",
    "        if cell is None:\n",
    "            hidden = hid\n",
    "        else:\n",
    "            cell = cell.index_select(1, indices)\n",
    "            hidden = (hid, cell)\n",
    "\n",
    "        enc_out = enc_out.index_select(0, indices)\n",
    "        attention_mask = attention_mask.index_select(0, indices)\n",
    "\n",
    "        return enc_out, hidden, attention_mask\n",
    "    \n",
    "    \n",
    "def reorder_decoder_incremental_state(incremental_state, inds):\n",
    "    if torch.is_tensor(incremental_state):\n",
    "        # gru or lstm\n",
    "        return torch.index_select(incremental_state, 1, inds).contiguous()\n",
    "    elif isinstance(incremental_state, tuple):\n",
    "        return tuple(\n",
    "            self.reorder_decoder_incremental_state(x, inds)\n",
    "            for x in incremental_state)\n",
    "\n",
    "def get_nbest_list_from_beam(beam, dictionary, n_best=None, add_length_penalty=False):\n",
    "    if n_best is None:\n",
    "        n_best = beam.min_n_best\n",
    "    nbest_list = beam._get_rescored_finished(n_best=n_best, add_length_penalty=add_length_penalty)\n",
    "    \n",
    "    nbest_list_text = [(dictionary.v2t(i[0].cpu().tolist()), i[1].item()) for i in nbest_list]\n",
    "    \n",
    "    return nbest_list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJa3QIWVzkfu"
   },
   "outputs": [],
   "source": [
    "def generate_with_beam(beam_size, min_n_best, model, batch, batch_size, \n",
    "                       previous_hyps=None, similarity_metric='hamming', \n",
    "                       similarity_threshold=0, verbose=False):\n",
    "    \"\"\"\n",
    "    This function takes a model, batch, beam settings and performs decoding with a beam. PRe beams_best_pick\n",
    "    \"\"\"\n",
    "    beams = [   Beam(beam_size, \n",
    "                     min_n_best=min_n_best, \n",
    "                     eos_token=chat_dict.word2ind['__end__'], \n",
    "                     padding_token=chat_dict.word2ind['__null__'], \n",
    "                     bos_token=chat_dict.word2ind['__start__'], \n",
    "                     device=current_device, \n",
    "                     similarity_metric=similarity_metric, \n",
    "                     similarity_threshold=similarity_threshold) for _ in range(batch_size)]\n",
    "    \n",
    "    repeated_inds = torch.arange(batch_size).to(current_device).unsqueeze(1).repeat(1, beam_size).view(-1)\n",
    "    \n",
    "    text_vecs = batch['text_vecs'].to(current_device)\n",
    "\n",
    "    encoder_states = model.encoder(text_vecs, batch['text_lens'])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    encoder_states = reorder_encoder_states(encoder_states, repeated_inds)  # no actual reordering here, but repeating beam size times each sample in the minibatch\n",
    "    encoder_output, encoder_hidden, attention_mask = encoder_states\n",
    "    \n",
    "    incr_state = encoder_hidden  # we init decoder hidden with last encoder_hidden\n",
    "    \n",
    "    # 1 is a start token id\n",
    "    starts = torch.Tensor([1]).long().to(model.decoder.embedding.weight.device).expand(batch_size*beam_size, 1).long()  # expand to batch_size * beam_size\n",
    "    decoder_input = starts\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ts in range(100):\n",
    "            if all((b.is_done() for b in beams)):\n",
    "                break\n",
    "            score, incr_state, attn_w_log = model.decoder(decoder_input, incr_state, encoder_states)\n",
    "            score = score[:, -1:, :]  # take last time step and eliminate the dimension\n",
    "            score = score.view(batch_size, beam_size, -1)\n",
    "            score = torch.log_softmax(score, dim=-1)\n",
    "         \n",
    "            for i, b in enumerate(beams):\n",
    "                if not b.is_done():\n",
    "                    # make mock previous_hyps if not used #\n",
    "                    if previous_hyps is None:\n",
    "                        previous_hyps = [{} for i in range(batch_size)]\n",
    "\n",
    "                    b.advance(score[i], previous_hyps[i])\n",
    "\n",
    "            incr_state_inds = torch.cat([beam_size * i + b.get_backtrack_from_current_step() for i, b in enumerate(beams)])\n",
    "            incr_state = reorder_decoder_incremental_state(incr_state, incr_state_inds)\n",
    "            selection = torch.cat([b.get_output_from_current_step() for b in beams]).unsqueeze(-1)\n",
    "            decoder_input = selection\n",
    "\n",
    "    beam_preds_scores = [list(b.get_top_hyp()) for b in beams]\n",
    "    beams_best_pick = get_nbest_list_from_beam(beams[0], chat_dict, n_best=1)[0][0]\n",
    "\n",
    "    if verbose:\n",
    "        for bi in range(batch_size):\n",
    "            print(f'batch {bi}')\n",
    "            for i in get_nbest_list_from_beam(beams[bi], chat_dict, n_best=min_n_best):\n",
    "                print(i)\n",
    "    \n",
    "    return beam_preds_scores, beams, beams_best_pick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_iwsZbxzkfx"
   },
   "source": [
    "# Generating some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1584066545953,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "LuQRVmWOzkfx",
    "outputId": "a3b341b5-203f-4e41-afd7-3e3818da008c"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "beam_size = 5\n",
    "beam_n_best = 5 #return top beam_n_best outputs\n",
    "\n",
    "valid_loader_single = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=batch_size)\n",
    "\n",
    "valid_sample = next(iter(valid_loader_single))\n",
    "\n",
    "beam_preds_scores, beams, _ = generate_with_beam(beam_size, \n",
    "                                                 beam_n_best, \n",
    "                                                 model, \n",
    "                                                 valid_sample, \n",
    "                                                 batch_size=batch_size, \n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-ckVtETzkfz"
   },
   "source": [
    "## Hm, why they are so similar? Lets make a visualization tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this from your terminal (if you haven't installed already)\n",
    "\n",
    "sudo apt install libgraphviz-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9787,
     "status": "ok",
     "timestamp": 1584066554717,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "SfrqBYOl1Nvv",
    "outputId": "1b5f73b9-2284-4b0f-a4d1-bd7cf497938f"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install --install-option=\"--include-path=/usr/local/include/graphviz/\" --install-option=\"--library-path=/usr/local/lib/graphviz\" pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRQ8XToqzkf0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "\n",
    "def get_beam_dot(beam: Beam, plot_size=30):\n",
    "        \"\"\"Create pydot graph representation of the beam.\n",
    "        \"\"\"\n",
    "\n",
    "        graph = nx.DiGraph()\n",
    "        outputs = numpy.array([i.tolist() for i in beams[0].outputs])\n",
    "        bookkeep = numpy.array([i.tolist() for i in beams[0].bookkeep])\n",
    "        all_scores = numpy.array([i.tolist() for i in beams[0].all_scores])\n",
    "        \n",
    "        max_ts = outputs.shape[0]\n",
    "        \n",
    "        labels_dict = {}\n",
    "        node_color_map = []\n",
    "\n",
    "        for i in range(max_ts):\n",
    "            if i == 0:\n",
    "                # only one start\n",
    "                start_node = f\"t_{0}__hid_{0}__tok_{outputs[i][0]}__sc_{all_scores[i][0]}\"\n",
    "                #start_node = {\"time\":0, \"hypid\": 0, \"token\": outputs[i][0], \"score\": all_scores[i][0]}\n",
    "                graph.add_node(start_node)\n",
    "                labels_dict[start_node] = chat_dict.ind2word[outputs[i][0]]\n",
    "                node_color_map.append('aliceblue')\n",
    "                continue\n",
    "\n",
    "            for hypid, token in enumerate(outputs[i]): # go over each token on this level\n",
    "                backtrack_hypid = bookkeep[i-1][hypid]\n",
    "                backtracked_node = f\"t_{i-1}__hid_{backtrack_hypid}__tok_{outputs[i-1][backtrack_hypid]}__sc_{all_scores[i-1][backtrack_hypid]}\"\n",
    "                current_score = all_scores[i][hypid]\n",
    "                node = f\"t_{i}__hid_{hypid}__tok_{token}__sc_{current_score}\"\n",
    "                graph.add_node(node)\n",
    "                graph.add_edge(backtracked_node, node)\n",
    "\n",
    "                if token == 2:\n",
    "                    node_color_map.append('pink')\n",
    "                    labels_dict[node] = \"__end__\\n{:.{prec}f}\".format(current_score, prec=4)\n",
    "                else:\n",
    "                    node_color_map.append('aliceblue')\n",
    "                    labels_dict[node] = chat_dict.ind2word[token]\n",
    "\n",
    "        # same layout using matplotlib with no labels\n",
    "        plt.figure(figsize=(plot_size,plot_size))\n",
    "        plt.title('Beam tree')\n",
    "        pos =graphviz_layout(graph, prog='dot')\n",
    "        nx.draw(graph, pos, labels=labels_dict, with_labels=True, arrows=True, font_size=24, node_size=5000, font_color='black', alpha=1.0, node_color=node_color_map)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11977,
     "status": "ok",
     "timestamp": 1584066556933,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "YXKZxJd6zkf2",
    "outputId": "ef97e217-5f3b-44a2-a2ab-76185b153aa9"
   },
   "outputs": [],
   "source": [
    "get_beam_dot(beams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11967,
     "status": "ok",
     "timestamp": 1584066556936,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "XUk24yVRzkf-",
    "outputId": "a5eb38e0-bda1-4541-91d6-254ba412dfb0"
   },
   "outputs": [],
   "source": [
    "# lets try bigger beam size\n",
    "\n",
    "batch_size = 1\n",
    "beam_size = 20\n",
    "beam_n_best = 20\n",
    "\n",
    "# shuffling to make different examples\n",
    "valid_loader_single = DataLoader(valid_dataset, shuffle=True, collate_fn=batchify, batch_size=batch_size)\n",
    "\n",
    "valid_sample = next(iter(valid_loader_single))\n",
    "\n",
    "print(f\"Input : {chat_dict.v2t(valid_sample['text_vecs'][0].tolist())}\\n\")\n",
    "\n",
    "beam_preds_scores, beams, _ = generate_with_beam(beam_size, beam_n_best, model, valid_sample, batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19105,
     "status": "ok",
     "timestamp": 1584066564088,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "vwjx899JzkgA",
    "outputId": "2401c416-0080-4ed3-ecfb-03fba31f06f2"
   },
   "outputs": [],
   "source": [
    "get_beam_dot(beams[0], 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEhmU8JBzkgD"
   },
   "source": [
    "# To be explored by the students on their own:\n",
    "\n",
    "### Iterative beam search: do not explore tokens you have already seen in the previous iteration\n",
    "\n",
    "There many strategies which work in a similar way: diverse beam search, RL-based rescoring of beam hypotheses, iterative beam search.\n",
    "Here we keep it simple: on each iteration we keep currently observed search space and block observed hypotheses tails of similar hypotheses in the following iterations.\n",
    "\n",
    "There can be many ways of how we qualify a hypothesis to be similar to other one. Here we define a similarity metric e.g. `edit distance` and a minimum threshold it should surpass to be a *different* one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pjNFAUCzkgE"
   },
   "source": [
    "def fill_prefixes(prefix_dict, history_hyps):\n",
    "    for hyp in history_hyps:\n",
    "        for j in range(len(hyp)):\n",
    "            _prefix = tuple(hyp[:j])\n",
    "            if _prefix in prefix_dict:\n",
    "                if hyp[j] in prefix_dict[_prefix]:\n",
    "                    continue\n",
    "                else:\n",
    "                    prefix_dict[_prefix].append(hyp[j])\n",
    "            else:\n",
    "                prefix_dict[_prefix] = [hyp[j]]\n",
    "\n",
    "def iterative_beam(num_iterations, beam_size, n_best_beam, model, batch, batch_size=1, similarity_metric='hamming', similarity_threshold=0, verbose=True):\n",
    "    \n",
    "    prefix_dict = [{} for i in range(batch_size)]\n",
    "    outputs = []\n",
    "    \n",
    "    for beam_iter in range(num_iterations):\n",
    "        beam_preds_scores, beams, _ = generate_with_beam(beam_size, n_best_beam, model, batch, batch_size=batch_size, previous_hyps=prefix_dict, similarity_metric=similarity_metric, similarity_threshold=similarity_threshold)\n",
    "        \n",
    "        \n",
    "        \n",
    "        outputs.append((beam_preds_scores, beams))\n",
    "        \n",
    "        for i, _dict in enumerate(prefix_dict):\n",
    "            fill_prefixes(_dict, beams[i].history_hyps)\n",
    "        \n",
    "    \n",
    "    if verbose:\n",
    "        for bi in range(batch_size):\n",
    "            for i in range(num_iterations):\n",
    "                print(f'Iter  {i}')\n",
    "                for j in get_nbest_list_from_beam(outputs[i][1][bi], chat_dict, n_best_beam):\n",
    "                    print(j)\n",
    "            \n",
    "    return outputs, prefix_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19080,
     "status": "ok",
     "timestamp": 1584066564089,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "8fOi2NzIzkgI",
    "outputId": "d70bfbd0-cd23-4d61-c5b3-05d76cec0c80"
   },
   "source": [
    "batch_size = 1\n",
    "iter_beam_size = 2\n",
    "beam_n_best = 2\n",
    "beam_iter = 10\n",
    "\n",
    "print(f\"Input : {chat_dict.v2t(valid_sample['text_vecs'][0].tolist())}\\n\")\n",
    "\n",
    "outputs, prefix_dict = iterative_beam(beam_iter, iter_beam_size, beam_n_best, model, valid_sample, batch_size=batch_size, similarity_metric='edit', similarity_threshold=3, verbose=True)\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "batch_size = 1\n",
    "iter_beam_size = 5\n",
    "beam_n_best = 5\n",
    "beam_iter = 4\n",
    "\n",
    "outputs, prefix_dict = iterative_beam(beam_iter, iter_beam_size, beam_n_best, model, valid_sample, batch_size=batch_size, similarity_metric='edit', similarity_threshold=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5BTUz8w0vSt"
   },
   "source": [
    "## Interactive Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzV860PDtYu5"
   },
   "outputs": [],
   "source": [
    "# REFERENCE:https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#run-evaluation\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "  '''\n",
    "  convert all letters to lowercase and trim all non-letter characters except for basic punctuation (normalizeString)\n",
    "  '''\n",
    "  s = unicodeToAscii(s.lower().strip())\n",
    "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "  s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "  return s\n",
    "\n",
    "def tokenHistory(history,input_sentence):\n",
    "  # add the input sentence to all previous history\n",
    "  history += ' \\n '+input_sentence\n",
    "  # parse input history\n",
    "  _inp_toked = RETOK.findall(history)\n",
    "  _inp_toked_id = chat_dict.t2v(_inp_toked)\n",
    "  input_vecs = torch.tensor([_inp_toked_id], dtype=torch.long)\n",
    "  length_input = torch.tensor([len(input_vecs[0])], dtype=torch.int64)\n",
    "  token_history ={'text_vecs': input_vecs, 'text_lens': length_input}\n",
    "  return history, token_history\n",
    "\n",
    "def ChatBot(model, persona, beam_size=5, prob_ns=0.5, decode_method=\"Beam\"):\n",
    "    assert( decode_method in ['Beam', 'NS', 'Greedy'])\n",
    "    history = persona # should be initialize with persona \"words\" string\n",
    "\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            while not input_sentence:\n",
    "                print('Prompt should not be empty!')\n",
    "                \n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            \n",
    "            # add the input sentence to all previous history\n",
    "            history += ' \\n '+input_sentence\n",
    "\n",
    "            # parse input history\n",
    "            _inp_toked = RETOK.findall(history)\n",
    "            _inp_toked_id = chat_dict.t2v(_inp_toked)\n",
    "            input_vecs = torch.tensor([_inp_toked_id], dtype=torch.long)\n",
    "            length_input = torch.tensor([len(input_vecs[0])], dtype=torch.int64)\n",
    "            batch_history ={'text_vecs': input_vecs, 'text_lens': length_input}\n",
    "\n",
    "\n",
    "            # Evaluate sentence\n",
    "            if decode_method==\"Beam\":\n",
    "              _, _, output_words = generate_with_beam(beam_size, beam_n_best, model, batch_history, batch_size=1, verbose=False)\n",
    "            elif decode_method==\"NS\": \n",
    "              _, _, _, output_words = sampling_with_nucleus(model, batch_history, 1, prob_ns, previous_hypo=None, verbose=None, sample=1)\n",
    "            elif decode_method==\"Greedy\": \n",
    "              output = greedy_search(model, batch_history, 1, max_len = 50)\n",
    "              output_words = chat_dict.v2t(output[0].tolist())\n",
    "            # add bot output to history\n",
    "            history += '\\n'+output_words\n",
    "\n",
    "            # Format and print response sentence\n",
    "            print('Bot:'+output_words.replace('__start__','').replace('__end__',''))\n",
    "            \n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GoOpLtBtYu7"
   },
   "source": [
    "### Interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-pO0PhbtYu7"
   },
   "outputs": [],
   "source": [
    "# input \"q\" or \"quit\" to quit the interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1584067564358,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "zqZ-3dHwWnjU",
    "outputId": "9272498b-a2e6-438d-d723-f8695a88dd07"
   },
   "outputs": [],
   "source": [
    "persona1 = \"your persona : i love to drink wine and dance in the moonlight . \\n your persona : i am very strong for my age . \\n your persona : i ' m 100 years old . \\n your persona : i feel like i might live forever .\"\n",
    "print(persona1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatBot(model, persona1, decode_method='Greedy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatBot(model, persona1, prob_ns = 0.3, decode_method='NS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 154299,
     "status": "ok",
     "timestamp": 1584067718600,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "Zuf7HUyfwlj2",
    "outputId": "230287a2-ce50-4d87-9b6b-c09e48a331c3"
   },
   "outputs": [],
   "source": [
    "ChatBot(model, persona1, beam_size=5, decode_method='Beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatBot(model, persona1, beam_size = 8, decode_method='Beam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e17_9-WgzkgM"
   },
   "source": [
    "## ChatBot Comparing All Decoding Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChatBotAll(model, persona, beam_size=5, prob_ns=0.5):\n",
    "  # three different history for all three decoding methods\n",
    "    history_beam, history_ns, history_greedy  = persona, persona, persona # initialize with persona \"words\" string\n",
    "    history_list = [history_beam, history_ns, history_greedy]\n",
    "  \n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            while not input_sentence:\n",
    "                print('Prompt should not be empty!')\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "\n",
    "            # Evaluate sentence\n",
    "            # \"Beam\":\n",
    "            history_beam, token_history = tokenHistory(history_beam,input_sentence)\n",
    "            _, _, output_words = generate_with_beam(beam_size, beam_n_best, model, token_history, batch_size=1, verbose=False)\n",
    "            # Format and print response sentence\n",
    "            print('Beam Bot:'+output_words.replace('__start__','').replace('__end__',''))\n",
    "            # add bot output to history\n",
    "            history_beam += '\\n'+output_words\n",
    "            # \"NS\":\n",
    "            history_ns, token_history = tokenHistory(history_ns,input_sentence)\n",
    "            _, _, _, output_words = sampling_with_nucleus(model, token_history, 1, prob_ns, previous_hypo=None, verbose=None, sample=1)\n",
    "            print('NS Bot:'+output_words.replace('__start__','').replace('__end__',''))\n",
    "            history_ns += '\\n'+output_words\n",
    "            # \"greedy\": \n",
    "            history_greedy, token_history = tokenHistory(history_greedy,input_sentence)\n",
    "            output = greedy_search(model, token_history, 1, max_len=100)\n",
    "            output_words = chat_dict.v2t(output[0].tolist())\n",
    "            print('Greedy Bot:'+output_words.replace('__start__','').replace('__end__',''))\n",
    "            history_greedy += '\\n'+output_words\n",
    "        \n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1584067734625,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "z5UVZZ_mwfW5",
    "outputId": "ec4b5f46-b6b4-44ef-b94b-d4094b9f8044"
   },
   "outputs": [],
   "source": [
    "persona2 = \"your persona : i love disneyland and mickey mouse . \\n your persona : i love to spend time with my family . \\n your persona : i ' m a baby delivery nurse . \\n your persona : i walk three miles every day .\"\n",
    "print(persona2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatBotAll(model, persona2, beam_size = 15, prob_ns=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135227,
     "status": "ok",
     "timestamp": 1584067873138,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "YlAikPXMyO7Y",
    "outputId": "f7f14c53-d4c8-4b78-da91-0796a807cfd9"
   },
   "outputs": [],
   "source": [
    "ChatBotAll(model, persona2, beam_size=5, prob_ns=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1584067880299,
     "user": {
      "displayName": "Gaomin Wu",
      "photoUrl": "",
      "userId": "05585133387136783154"
     },
     "user_tz": 240
    },
    "id": "VMjrBD8Fw3ct",
    "outputId": "f7c58881-baa7-4c39-d953-e6894a2941c9"
   },
   "outputs": [],
   "source": [
    "persona3 = \"your persona : i love to drink fancy tea . \\n your persona : i have a big library at home . \\n your persona : i ' m a museum tour guide . \\n your persona : i ' m partly deaf .\"\n",
    "print(persona3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPG2HHkawryC"
   },
   "outputs": [],
   "source": [
    "ChatBotAll(model, persona3, beam_size=5, prob_ns=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona4 = \"your persona : i study machine learning . \\n your persona : i live in kigali . \\n your persona : i ' m very smart . \\n your persona : i love nlp .\"\n",
    "print(persona3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatBotAll(model, persona4, beam_size=3, prob_ns=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chat_revised.ipynb",
   "provenance": [
    {
     "file_id": "1aT_Hga_mDgbtd-VRA5dPDrEhMtnST5PU",
     "timestamp": 1584046821973
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
