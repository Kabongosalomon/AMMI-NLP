{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Modeling\n",
    "\n",
    "In this lab, we will overview the **masked language modeling** objective, and how the **Transformer** architecture is used for large-scale masked language modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os, sys, glob, json, math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Recently, Devlin et al. published [BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf).\n",
    "\n",
    "\n",
    "**B**idirectional\n",
    "\n",
    "**E**ncoder\n",
    "\n",
    "**R**epresentations from\n",
    "\n",
    "**T**ransformers\n",
    "\n",
    "\n",
    "#### Goal: \n",
    "1. **pre-train** a model that produces language representations. \n",
    "2. **fine-tune** the model on a task.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Language Model Objective\n",
    "\n",
    "Randomly mask some of the tokens from the input, predict original vocabulary id of each masked token.\n",
    "\n",
    "- Given sequence $x_1,\\ldots,x_N$.\n",
    "\n",
    "- Form **mask** $m_1,\\ldots,m_N$ where $m_i\\in \\{0,1\\}$.\n",
    "    - E.g. $m_i=1$ with probability 0.15\n",
    "    \n",
    "- Form **masked sequence** $\\tilde{x}_1,\\ldots,\\tilde{x}_N$.\n",
    "    - $\\tilde{x}_i=\\begin{cases} x_i & m_i=0\\\\ \\texttt{[MASK]} & m_i=1\\end{cases}$\n",
    "\n",
    "\n",
    "#### $$\\mathcal{L}_{\\text{MLM}}=-\\sum_{\\underbrace{i | m_i=1}_{\\text{MASKED POSITIONS}}}\\log p_{\\theta}(\\underbrace{x_i}_{\\text{TRUE TOKEN}}|\\underbrace{\\tilde{x}_1,\\ldots,\\tilde{x}_N}_{\\text{MASKED SEQUENCE}})$$\n",
    "\n",
    "\n",
    "<!-- Below, we will discuss the exact form of $\\tilde{x}_i$ that the BERT authors used. -->\n",
    "\n",
    "\n",
    "<!-- #### Diagram of BERT Implementation -->\n",
    "<!-- ![](bert_overview.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "So far we have modeled a sequence by factorizing the joint distribution into conditionals, and **parameterizing each conditional with a recurrent network**:\n",
    "\n",
    "\n",
    "#### $$p_{\\theta}(x_1,\\ldots,x_T)=\\prod_{t=1}^T p_{\\theta}(x_t | x_{<t})$$\n",
    "\\begin{align}\n",
    "h_t &= RNN(x_{t-1}, h_t)\\\\\n",
    "p_{\\theta}(x_t | x_{<t}) &=\\text{softmax}\\left(Wh_t+b\\right),\n",
    "\\end{align}\n",
    "\n",
    "where $\\theta$ are the model parameters (RNN parameters, $W, b$, embedding matrix).\n",
    "\n",
    "\n",
    "#### Alternative\n",
    "\n",
    "An alternative proposed in [[Vaswani et al 2017](https://arxiv.org/pdf/1706.03762.pdf)] is to parameterize each conditional with a **particular feed-forward architecture** called the **Transformer**. With this model, it is possible to compute all conditionals with a **single feed-forward pass**:\n",
    "\\begin{align}\n",
    "(h_1,\\ldots,h_T) &= Transformer(x)\\\\\n",
    "p_{\\theta}(x_t | x_{<t}) &= \\text{softmax}\\left(Wh_t + b\\right)\n",
    "\\end{align}\n",
    "\n",
    "We will discuss briefly the key ideas, the overall Transformer architecture (encoder only), and how they are used in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Level View\n",
    "\n",
    "We can view the Transformer encoder as mapping a sequence to a sequence of vectors.\n",
    "\n",
    "<img src=\"img/high1.png\" alt=\"Drawing\" style=\"width: 35%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through the key ideas of how this mapping is designed, and discuss some of its resulting properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Idea 1: Position Embeddings\n",
    "\n",
    "Unlike RNNs which can learn positional information via the hidden state over time, the Transformer has no notion of time.\n",
    "\n",
    "Thus we encode inputs with **position** as well as **token** embeddings:\n",
    "\n",
    "<img src=\"img/high2.png\" alt=\"Drawing\" style=\"width: 35%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = ['<s>', 'my', 'pet', '[M]', '<s>']\n",
    "\n",
    "max_len = 10\n",
    "\n",
    "vocab = {'<s>': 0, 'my': 1, 'pet': 2, 'dog': 3, 'cat': 4, 'lion': 5, '[M]': 6}\n",
    "\n",
    "dim = 6\n",
    "\n",
    "token_embed = nn.Embedding(len(vocab), embedding_dim=dim)\n",
    "position_embed = nn.Embedding(max_len, embedding_dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = torch.tensor([vocab[x] for x in input_sequence]).unsqueeze(1)\n",
    "\n",
    "input_embeddings = token_embed(input_vector) + position_embed(torch.arange(len(input_vector)))\n",
    "input_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning!!** The pytorch Transformer classes accept input as `Length x Batch x Dim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Idea 2: Modularity\n",
    "The Transformer (encoder) is composed of a stack of **N identical layers**.\n",
    "\n",
    "<img src=\"img/layers.png\" alt=\"Drawing\" style=\"width: 35%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "nn.TransformerEncoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `forward` passes the input through the N layers, then normalizes it:\n",
    "\n",
    "**Warning!!** The forward function accepts input as `Length x Batch x Dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.TransformerEncoder.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=2, dim_feedforward=64, dropout=0.1)\n",
    "\n",
    "encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = encoder(input_embeddings)\n",
    "\n",
    "print(\"input size: \\t%s\" % str(tuple(input_embeddings.shape)))\n",
    "print(\"output size:\\t%s\" % str(tuple(outputs.shape)))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each layer has two parts, **self-attention** and a feed-forward transformation:\n",
    "\n",
    "<img src=\"img/layer.png\" alt=\"Drawing\" style=\"width: 65%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.TransformerEncoderLayer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.TransformerEncoderLayer.forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Idea 3: Self-Attention\n",
    "\n",
    "In the RNN, the hidden state contains information about previous tokens.\n",
    "The Transformer instead performs **attention** over all inputs at a given layer. 'Attention' computes an output vector by taking a weighted sum of input vectors. The weights are 'attention weights'. The Transformer uses **scaled dot-product attention**:\n",
    "#### $$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "and 'Multi-head Attention' refers to applying several of these operations in parallel.\n",
    "\n",
    "#### *Key Property*: Each output vector of a layer $n$ can using information from **all** inputs to the layer $n$.\n",
    "\n",
    "Thus each **final output vector** can incorporate information from **all input words**.\n",
    "\n",
    "(If we want to prevent information flow such as in left-to-right language modeling, we can use masking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = nn.MultiheadAttention(dim, 2, dropout=0.0)\n",
    "\n",
    "attn_outputs, attn_weights = attn.forward(query=outputs, key=outputs, value=outputs)\n",
    "\n",
    "print(\"input shape: %s\" % (str(tuple(outputs.size()))))\n",
    "print(\"output shape: %s\" % (str(tuple(attn_outputs.size()))))\n",
    "print(outputs)\n",
    "\n",
    "print(\"\\nattn weights shape: %s\" % (str(tuple(attn_weights.size()))))\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, dim=8, num_layers=4, nhead=2):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, dim)\n",
    "        self.position_embed = nn.Embedding(max_len, dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=nhead, dim_feedforward=64, dropout=0.0)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.projection = nn.Linear(dim, vocab_size)\n",
    "    \n",
    "    def features(self, token_indices):\n",
    "        pos = torch.arange(len(token_indices), device=token_indices.device).unsqueeze(1)\n",
    "        x = self.token_embed(token_indices) + self.position_embed(pos)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, token_indices):\n",
    "        x = self.features(token_indices)\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(vocab), max_len=100)\n",
    "\n",
    "model.features(input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Masked Language Modeling\n",
    "\n",
    "Recall the **key property** of Transformers: due to self-attention, each output vector can incorporate information from *all* input tokens.\n",
    "\n",
    "<img src=\"img/mlm.png\" alt=\"Drawing\" style=\"width: 45%;\"/>\n",
    "\n",
    "This is useful for masked language modeling, where we want to use information from the entire context when predicting the masked token(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLM on Persona-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "raw_datasets, datasets, vocab = utils.load_personachat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "trainloader = DataLoader(datasets['train'], batch_size=4, collate_fn=lambda x: utils.pad_collate_fn(vocab.get_id('<pad>'), x))\n",
    "validloader = DataLoader(datasets['valid'], batch_size=4, collate_fn=lambda x: utils.pad_collate_fn(vocab.get_id('<pad>'), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(trainloader.__iter__())\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(inputs, mask_prob, pad_token_id, mask_token_id, vsize):\n",
    "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\"\"\"\n",
    "    inputs = inputs.clone()\n",
    "    labels = inputs.clone()\n",
    "    # Sample tokens in each sequence for masked-LM training\n",
    "    masked_indices = torch.bernoulli(torch.full(labels.shape, mask_prob)).bool()\n",
    "    masked_indices = masked_indices & (inputs != pad_token_id)\n",
    "    labels[~masked_indices] = -1  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = mask_token_id\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(vsize, labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = mask_tokens(batch, mask_prob=0.15, mask_token_id=vocab.get_id('[M]'), pad_token_id=vocab.get_id('<pad>'), vsize=len(vocab))\n",
    "print(\"Mask token id: %d\" % vocab.get_id('[M]'))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(vocab), max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(inputs)\n",
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_ = logits.view(-1, logits.size(2))\n",
    "labels_ = labels.view(-1)\n",
    "\n",
    "criterion(logits_, labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import torch.optim as optim\n",
    "    from tqdm import tqdm, trange\n",
    "    from collections import defaultdict\n",
    "    from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "    trainloader = DataLoader(datasets['train'], batch_size=64, collate_fn=lambda x: utils.pad_collate_fn(vocab.get_id('<pad>'), x))\n",
    "    validloader = DataLoader(datasets['valid'], batch_size=64, collate_fn=lambda x: utils.pad_collate_fn(vocab.get_id('<pad>'), x))\n",
    "\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = Transformer(len(vocab), max_len=65, dim=256, nhead=8).to(device)\n",
    "\n",
    "    model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(model_parameters, lr=0.001)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1).to(device)\n",
    "\n",
    "    stats = defaultdict(list)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for step, batch in enumerate(trainloader):\n",
    "            model.train()        \n",
    "            # Mask the batch\n",
    "            inputs, labels = mask_tokens(batch, mask_prob=0.15, \n",
    "                                         pad_token_id=vocab.get_id('<pad>'),\n",
    "                                         mask_token_id=vocab.get_id('[M]'), \n",
    "                                         vsize=len(vocab))\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits_ = logits.view(-1, logits.size(2))\n",
    "            labels_ = labels.view(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(logits_, labels_)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            stats['train_loss'].append(loss.item())\n",
    "            stats['train_loss_log'].append(loss.item())\n",
    "            if (step % 500) == 0:\n",
    "                avg_loss = sum(stats['train_loss_log']) / len(stats['train_loss_log'])\n",
    "                print(\"Epoch %d Step %d\\tTrain Loss %.3f\" % (epoch, step, avg_loss))\n",
    "                stats['train_loss_log'] = []\n",
    "\n",
    "        for batch in validloader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Mask the batch\n",
    "                inputs, labels = mask_tokens(batch, mask_prob=0.15, \n",
    "                                             pad_token_id=vocab.get_id('<pad>'),\n",
    "                                             mask_token_id=vocab.get_id('[M]'), \n",
    "                                             vsize=len(vocab))\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                logits = model(inputs)\n",
    "                logits_ = logits.view(-1, logits.size(2))\n",
    "                labels_ = labels.view(-1)\n",
    "\n",
    "                loss = criterion(logits_, labels_)\n",
    "                stats['valid_loss'].append(loss.item())\n",
    "        print(\"=== Epoch %d\\tValid Loss %.3f\" % (epoch, stats['valid_loss'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Conditionals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "checkpoint = utils.load('model', 'model', best=True)\n",
    "options = checkpoint['options']\n",
    "stats = checkpoint['stats']\n",
    "\n",
    "\n",
    "model = utils.Transformer(len(vocab), options['max_len'], \n",
    "                          dim=options['dim'], \n",
    "                          nhead=options['nhead'])\n",
    "model.load_state_dict(checkpoint['model_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['<s>', 'i', 'have', 'a', 'pet', '[M]', '.', '<s>'],\n",
    "             ['<s>', 'i', 'have', 'two', 'pet', '[M]', '.', '<s>'],\n",
    "             ['<s>', 'my', '[M]', 'is', 'a', 'lawyer', '.', '<s>'],\n",
    "             ['<s>', 'my', '[M]', 'is', 'a', '[M]', '.', '<s>'],\n",
    "             ['<s>', 'i', '[M]', '[M]', '[M]', 'sometimes', '.' , '<s>']]\n",
    "\n",
    "\n",
    "def get_top_masked_tokens(tokens, vocab, device, top=10):\n",
    "    ids = torch.tensor([vocab.get_id(x) for x in tokens], device=device).unsqueeze(1)\n",
    "    masked = ids == vocab.get_id('[M]')\n",
    "\n",
    "    logits = model(ids)[masked]\n",
    "    probs = torch.softmax(logits, -1)\n",
    "\n",
    "    print(' '.join(tokens))\n",
    "    for ps in probs:\n",
    "        probs, idxs = ps.sort(descending=True)\n",
    "\n",
    "        for i in range(top):\n",
    "            print(\"\\t%s (%.4f)\" % (vocab.get_token(idxs[i].item()),\n",
    "                                   probs[i].item()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    get_top_masked_tokens(s, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to *BERT*\n",
    "\n",
    "**B**idirectional\n",
    "\n",
    "**E**ncoder\n",
    "\n",
    "**R**epresentations from\n",
    "\n",
    "**T**ransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Masked Language Modeling at scale\n",
    "\n",
    "#### - Learned representations are useful downstream\n",
    "\n",
    "<img src=\"img/bert_citations.png\" alt=\"Drawing\" style=\"width: 45%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great implementation in [transformers](https://github.com/huggingface/transformers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details -- Model Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\text{BERT}_{\\text{BASE}}$: 12 layers, hidden dimension 768, 12 attention heads (**110 million parameters**)\n",
    "- $\\text{BERT}_{\\text{LARGE}}$: 24 layers, hidden dimension 1024, 16 attention heads (**340 million parameters**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-cased', output_attentions=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details -- Input Implementation\n",
    "\n",
    "\n",
    "- `[CLS]` token: starts each sequence. Used as aggregate sequence representation.\n",
    "- `[SEP]` token: separates two segments (e.g. two sentences).\n",
    "- **Segment embedding**: learned embedding for every token indicating whether it belongs\n",
    "to sentence A or sentence B.\n",
    "- **Position embedding**: learned.\n",
    "\n",
    "\n",
    "<img src=\"img/bert_inputs.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "\n",
    "**Exercise:** Which downstream tasks would two sequences be useful for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "#### BERT represents text using **subword** tokens with a 30k token vocabulary.  \n",
    "\n",
    "\n",
    "\n",
    "(more info [here](https://github.com/google/sentencepiece) and in the papers mentioned there)\n",
    "\n",
    "<!-- - **Token embedding**: WordPiece embeddings with 30k token vocabulary. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Pretraining is cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"BERT represents text using subwords.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Learned Conditionals (& Representations)\n",
    "\n",
    "**Probing tasks** can be used to examine aspects of what the model has learned. \n",
    "\n",
    "Following [Petroni et al 2019](https://arxiv.org/pdf/1909.01066.pdf) we probe for '**knowledge**' that the model has learned by querying for masked out objects, e.g.:\n",
    "\n",
    "<img src=\"img/bert_kb.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "\n",
    "The task also illustrates some aspects of the **conditional distributions** and **contextualized representations** that the model has learned.\n",
    "\n",
    "(image from [Petroni et al 2019])\n",
    "\n",
    "\n",
    "**Exercise:** The authors only consider *single-token* prediction. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probing Task\n",
    "\n",
    "We use a dataset from [Petroni et al 2019](https://github.com/facebookresearch/LAMA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "data = utils.load_lama_squad(download=True)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "model.eval()\n",
    "for example in tqdm(data, total=len(data)):\n",
    "    sentence, label = example['masked_sentences'][0], example['obj_label']\n",
    "    inp = torch.tensor([\n",
    "        [tokenizer.cls_token_id] + \n",
    "        tokenizer.encode(sentence) + \n",
    "        [tokenizer.sep_token_id]\n",
    "    ], device=device)\n",
    "    \n",
    "    mask = (inp == tokenizer.vocab[tokenizer.mask_token])\n",
    "    out, attn = model(inp)\n",
    "    \n",
    "    probs, token_ids = out[mask].softmax(1).topk(10)\n",
    "    probs = probs[0].tolist()\n",
    "    token_ids = token_ids[0].tolist()\n",
    "\n",
    "    tokens = [tokenizer.ids_to_tokens[i] for i in token_ids]\n",
    "\n",
    "    results.append({\n",
    "        'sentence': sentence,\n",
    "        'label': label,\n",
    "        'top_tokens': tokens,\n",
    "        'top_probs': probs,\n",
    "        'correct@1': tokens[0] == label,\n",
    "        'attn': attn\n",
    "    })\n",
    "\n",
    "print(\"correct@1: %.3f\" % (\n",
    "    len([r for r in results if r['correct@1']]) / len(results)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "correct = [r for r in results if r['correct@1']]\n",
    "wrong = [r for r in results if not r['correct@1']]\n",
    "\n",
    "def show(idx=0, attn_layer=0, is_correct=True):\n",
    "    result = correct[idx] if is_correct else wrong[idx]\n",
    "\n",
    "    # --- format the result into a string\n",
    "    top_str = '\\n\\t'.join([\n",
    "        ('\\t%s\\t(%.4f)' % (tokens, probs)) \n",
    "        for tokens, probs in zip(result['top_tokens'], result['top_probs'])\n",
    "    ])\n",
    "    print(\"%s\\n\\tlabel:\\t%s\\n\\n\\ttop:%s\" % (\n",
    "        result['sentence'], \n",
    "        result['label'], \n",
    "        top_str\n",
    "    ))\n",
    "\n",
    "    # --- visualize attention\n",
    "    print(\"Attention weights (12 heads) from layer %d:\" % attn_layer)\n",
    "    fig, axs = plt.subplots(3, 4, figsize=(18, 12))\n",
    "\n",
    "    toks = ['[CLS]'] + tokenizer.tokenize(result['sentence']) + ['[SEP]']\n",
    "    for i, ax in enumerate(axs.reshape(-1)):\n",
    "        ax.matshow(result['attn'][attn_layer][0][i].data.cpu().numpy(), cmap='gray')\n",
    "\n",
    "        ax.set_xticks(range(len(toks)))\n",
    "        ax.set_xticklabels(toks, rotation=90, fontsize=15)\n",
    "        ax.set_yticks(range(len(toks)))\n",
    "        ax.set_yticklabels(toks, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "interactive(\n",
    "    show, \n",
    "    idx=(0, min(len(correct), len(wrong))-1), \n",
    "    attn_layer=range(12), \n",
    "    is_correct=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
