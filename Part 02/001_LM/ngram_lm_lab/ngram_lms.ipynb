{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ngram_lms.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/AMMI-NLP/blob/master/Part%2002/001_LM/ngram_lm_lab/ngram_lms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2pdf3Sakc8b",
        "colab_type": "text"
      },
      "source": [
        "# N-Gram Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SYTnNPFk3wI",
        "colab_type": "text"
      },
      "source": [
        "Uncomment the line of code bellow if you're on colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJCX8Z-8lkiQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "4a72d421-7688-4ea3-87e4-596a4ac218c7"
      },
      "source": [
        "!git clone https://github.com/Kabongosalomon/AMMI-NLP.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AMMI-NLP'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 267 (delta 21), reused 53 (delta 15), pack-reused 201\u001b[K\n",
            "Receiving objects: 100% (267/267), 165.63 MiB | 31.86 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n",
            "Checking out files: 100% (84/84), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwfrk0CvmSXf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "360abf70-ef14-46ce-f6e4-41c767562d28"
      },
      "source": [
        "!sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "liblzma-dev is already the newest version (5.2.2-1.3).\n",
            "liblzma-dev set to manually installed.\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n",
            "libbz2-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ8FJEOntd4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "00103a97-5343-44b0-fc4b-555abdf4af09"
      },
      "source": [
        "%cd "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvE8oictmWHy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a19e2e6-19f4-45b8-9db5-bd0db35b36b9"
      },
      "source": [
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "!mkdir kenlm/build\n",
        "%cd kenlm/build\n",
        "!cmake ..\n",
        "!make -j 4"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/kenlm/build\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   program_options\n",
            "--   system\n",
            "--   thread\n",
            "--   unit_test_framework\n",
            "--   chrono\n",
            "--   date_time\n",
            "--   atomic\n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/include (found version \"5.2.2\") \n",
            "-- Could NOT find Eigen3 (missing: EIGEN3_INCLUDE_DIR EIGEN3_VERSION_OK) (Required is at least version \"2.91.0\")\n",
            "CMake Warning at lm/interpolate/CMakeLists.txt:65 (message):\n",
            "  Not building interpolation.  Eigen3 was not found.\n",
            "\n",
            "\n",
            "-- To install Eigen3 in your home directory, copy paste this:\n",
            "export EIGEN3_ROOT=$HOME/eigen-eigen-07105f7124f9\n",
            "(cd $HOME; wget -O - https://bitbucket.org/eigen/eigen/get/3.2.8.tar.bz2 |tar xj)\n",
            "rm CMakeCache.txt\n",
            "\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_filter\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_util\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 18%] Built target kenlm_filter\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 43%] Built target kenlm_util\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target probing_hash_table_benchmark\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 63%] Built target probing_hash_table_benchmark\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fragment\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target build_binary\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_benchmark\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target query\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 77%] Built target fragment\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_builder\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "\u001b[35m\u001b[1mScanning dependencies of target phrase_table_vocab\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 82%] Built target query\n",
            "\u001b[35m\u001b[1mScanning dependencies of target filter\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 85%] Built target phrase_table_vocab\n",
            "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 92%] Built target kenlm_benchmark\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 93%] Built target filter\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 95%] Built target kenlm_builder\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lmplz\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target count_ngrams\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkqaz-5ineal",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "27b461cd-2ba5-46a9-e091-958adea093ee"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/kenlm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6-LfQ7PmnKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "34ca5ad3-77e3-42f4-e0ab-0051059b00f7"
      },
      "source": [
        "! python setup.py install"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing kenlm.egg-info/PKG-INFO\n",
            "writing dependency_links to kenlm.egg-info/dependency_links.txt\n",
            "writing top-level names to kenlm.egg-info/top_level.txt\n",
            "reading manifest file 'kenlm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'kenlm.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/kenlm.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for kenlm.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kenlm.py to kenlm.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kenlm.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kenlm.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kenlm.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kenlm.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.kenlm.cpython-36: module references __file__\n",
            "creating 'dist/kenlm-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing kenlm-0.0.0-py3.6-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.6/dist-packages/kenlm-0.0.0-py3.6-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.6/dist-packages/kenlm-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting kenlm-0.0.0-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "kenlm 0.0.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/kenlm-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for kenlm==0.0.0\n",
            "Finished processing dependencies for kenlm==0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlI-WhHrnzkS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fde9f4f5-53e2-4180-e884-9e2a08d69648"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpzmGozhs0wI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "088086d9-438c-44be-d5c5-121039a7cd9a"
      },
      "source": [
        "%cd AMMI-NLP/Part\\ 02/001_LM/ngram_lm_lab"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBUVJ9W6tF3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ce4aa28-3093-48c1-ee61-d1b4cdfbd6c2"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img  models  ngram_lms.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuCAHVpwkc8f",
        "colab_type": "text"
      },
      "source": [
        "#### Preliminaries: Install KenLM\n",
        "\n",
        "[KenLM](https://kheafield.com/code/kenlm/) is used in the latter part of this lab.\n",
        "\n",
        "To install it, run the following commands **in the jupyter terminal** (see screenshot):\n",
        "\n",
        "    sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
        "    \n",
        "    cd /home/jupyter\n",
        "    wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "    mkdir kenlm/build\n",
        "    cd kenlm/build\n",
        "    cmake ..\n",
        "    make -j 4\n",
        "    \n",
        "    cd /home/jupyter/kenlm\n",
        "    python setup.py install\n",
        "    \n",
        "<img src=\"https://github.com/Kabongosalomon/AMMI-NLP/blob/master/Part%2002/001_LM/ngram_lm_lab/img/instruction.png?raw=1\" alt=\"Drawing\" style=\"width: 35%;\"/>\n",
        "\n",
        "<img src=\"https://github.com/Kabongosalomon/AMMI-NLP/blob/master/Part%2002/001_LM/ngram_lm_lab/img/instruction2.png?raw=1\" alt=\"Drawing\" style=\"width: 70%;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3cKti6ikc8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "93b3a854-f8fc-4c41-ca80-99dadd65af69"
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afUQ3YeFkc8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b96db0a-f3be-4367-833c-a77fc0e1bbbf"
      },
      "source": [
        "%pylab inline\n",
        "import os, sys, glob, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "from collections import defaultdict\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV4Sgonekc8t",
        "colab_type": "text"
      },
      "source": [
        "### N-gram Language Modeling\n",
        "\n",
        "In **language modeling** we want to model the probability of variable length sequences, $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{<t}).$$\n",
        "\n",
        "An **n-gram language model** assumes that each word $w_t$ only depends on the preceding $n-1$ words, $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{t-n+1},\\ldots,w_{t-1}).$$\n",
        "\n",
        " \n",
        "\n",
        "#### Example\n",
        "For instance, when modeling the sentence $$\\texttt{the cat sat on the mat .}$$ a 3-gram language model assumes that $$p(\\texttt{mat}|\\texttt{the cat sat on the}) \\approx p(\\texttt{mat}|\\texttt{on the}).$$\n",
        "\n",
        "The sub-sequence $(\\texttt{on the mat})$ is a *3-gram* or *trigram*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GNHzT3Ckc8v",
        "colab_type": "text"
      },
      "source": [
        "### Count-based Estimation\n",
        "\n",
        "Given some dataset $D$ of sequences, we can estimate an n-gram model through counting, derived as follows:\n",
        "\n",
        "\\begin{align}\n",
        "p(w_t|w_{t-n+1},\\ldots,w_{t-1}) &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{p(w_{t-n+1},\\ldots,w_{t-1})} & \\text{definition of conditional probability}\\\\\n",
        "                       &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}p(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
        "                       &\\approx \\frac{\\frac{1}{N}\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\frac{1}{N}\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
        "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
        "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\text{count}(w_{t-n+1},\\ldots,w_{t-1})},\n",
        "\\end{align}\n",
        "\n",
        "where $N$ is the number of $n$-grams in the dataset.\n",
        "\n",
        "In Python, we can collect these counts into a dictionary mapping a prefix to a dictionary of counts:\n",
        "\n",
        "        count[(w_n+1,...,w_t-1)] = {wt1: count of (w_n+1,...,w_t1),\n",
        "                                    wt2: count of (w_n+1,...,w_t2),\n",
        "                                    ...\n",
        "                                   }\n",
        "                                   \n",
        "and for the denominator, maintain a dictionary of totals:\n",
        "\n",
        "        total[(w_n+1,...,w_t-1)] = count of w_n+1,...,w_t-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpX469K_kc8x",
        "colab_type": "raw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADnsOtuFkc8y",
        "colab_type": "text"
      },
      "source": [
        "### Simplified Language\n",
        "\n",
        "To get intuition, lets start by modeling a simple language called `ABC`. A word in this language is one of three tokens, $$w\\in \\{\\texttt{A, B, C}\\},$$\n",
        "and we'll denote a sentence as $\\textbf{w}=(w_1,\\ldots,w_{|\\textbf{w}|})$.\n",
        "\n",
        "\n",
        "Suppose we are given the following dataset, and want to estimate a **bigram model**: $$p(\\textbf{w})\\approx\\prod_{t=1}^{|\\textbf{w}|}p(w_t|w_{t-1})\\quad\\quad(*)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juu4O0lHkc8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_raw = [['A', 'A', 'B', 'B'],\n",
        "            ['A', 'A', 'B'],\n",
        "            ['A', 'A', 'B', 'C'],\n",
        "            ['A', 'A', 'A'],\n",
        "            ['A', 'A', 'A', 'A']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1UoSSBskc84",
        "colab_type": "text"
      },
      "source": [
        "Since our model is a probability distribution, the total probability of all possible strings in the language must sum to 1, i.e.: $$\\sum_{\\textbf{w}}p(\\textbf{w})=1.$$\n",
        "\n",
        "In order to satisfy this criterion it turns out that we need an additional **beginning token**, `<bos>`, and **end token**, `<eos>`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orO-6eNUkc86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "46b66599-7d81-427f-dc0a-4e619ccb1d10"
      },
      "source": [
        "data = [['<bos>'] + d + ['<eos>'] for d in data_raw]\n",
        "data"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<bos>', 'A', 'A', 'B', 'B', '<eos>'],\n",
              " ['<bos>', 'A', 'A', 'B', '<eos>'],\n",
              " ['<bos>', 'A', 'A', 'B', 'C', '<eos>'],\n",
              " ['<bos>', 'A', 'A', 'A', '<eos>'],\n",
              " ['<bos>', 'A', 'A', 'A', 'A', '<eos>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "261Nrt35kc8_",
        "colab_type": "text"
      },
      "source": [
        "Now let's estimate a bigram model:\n",
        "\n",
        "\\begin{align}\n",
        "p(w_t|w_{t-1}) &= \\frac{\\text{count}(w_{t-1}w_{t})}{\\sum_{w_{t'}}\\text{count}(w_{t-1}w_{t'})}\\\\\n",
        "               &= \\texttt{count[prefix][wt] / totals[prefix]}\n",
        "\\end{align} \n",
        "\n",
        "where $\\texttt{prefix}$ is $w_{t-1}$ in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWdOw7kwkc9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = defaultdict(lambda: defaultdict(float))\n",
        "total = defaultdict(float)\n",
        "\n",
        "n = 2\n",
        "for sequence in data:\n",
        "    for i in range(len(sequence)-n+1):         # for each ngram\n",
        "        ngram = tuple(sequence[i:i+n])\n",
        "        prefix, word = ngram[:-1], ngram[-1]\n",
        "        count[prefix][word] += 1               # count(w_{t-n+1}...w_t)\n",
        "        total[prefix] += 1                     # count(w_{t-n+1}...w_{t-1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4nfqF8nkc9E",
        "colab_type": "text"
      },
      "source": [
        "Let's see if the counts and totals make sense:\n",
        "\n",
        "- How many times did (A, B) occur? What about (B, B)?\n",
        "- How many times did (A) occur? What about (C)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjpmiW17kc9F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "9440e843-ef95-44b9-cbc0-b28e2694c8ef"
      },
      "source": [
        "print(\"Counts:\")\n",
        "pprint(dict(count))\n",
        "print(\"\\nTotals:\")\n",
        "pprint(dict(total))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counts:\n",
            "{('<bos>',): defaultdict(<class 'float'>, {'A': 5.0}),\n",
            " ('A',): defaultdict(<class 'float'>, {'A': 8.0, 'B': 3.0, '<eos>': 2.0}),\n",
            " ('B',): defaultdict(<class 'float'>, {'B': 1.0, '<eos>': 2.0, 'C': 1.0}),\n",
            " ('C',): defaultdict(<class 'float'>, {'<eos>': 1.0})}\n",
            "\n",
            "Totals:\n",
            "{('<bos>',): 5.0, ('A',): 13.0, ('B',): 4.0, ('C',): 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WngrJGDkkc9J",
        "colab_type": "text"
      },
      "source": [
        "#### Conditional probability queries\n",
        "\n",
        "We can now query a conditional probability:\n",
        "\n",
        "\\begin{align}\n",
        "\\texttt{p(word|prefix)} =&\\ \\texttt{count[prefix][word] / totals[prefix]}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1f4Elepkc9L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bd1e9ffb-aa78-4fd3-e27e-90118f364169"
      },
      "source": [
        "queries = [('<bos>', 'A'),\n",
        "           ('B', 'C')]\n",
        "\n",
        "for query in queries:\n",
        "    prefix, word = query[:-1], query[-1]\n",
        "    p = count[prefix][word] / total[prefix]  # We'll discuss the case when `total[prefix] = 0` below.\n",
        "    print(\"p( %s | %s) = \\t%.5f\" % (word, ', '.join(prefix), p))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p( A | <bos>) = \t1.00000\n",
            "p( C | B) = \t0.25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Sy3QL7kc9P",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: Look at the training set and convince yourself that these conditional probabilities are correct according to the count-based estimation procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RqfSC5Qkc9Q",
        "colab_type": "text"
      },
      "source": [
        "#### Sequence Probability\n",
        "\n",
        "We can compute the probability of a sequence using the conditional probabilities along with the chain rule of probability:\n",
        "\n",
        "\\begin{align}\n",
        "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-1})\n",
        "\\end{align}\n",
        "\n",
        "(Here $w_0$ is `<bos>` and $w_T$ is `<eos>`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhnbZ3jykc9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "d04cf372-5c76-4116-c18f-ad3a4dc894e3"
      },
      "source": [
        "sequence = ['<bos>', 'A', 'A', 'B', '<eos>']\n",
        "\n",
        "def sequence_p(sequence, log=False):\n",
        "    total_p = 1\n",
        "\n",
        "    for i in range(len(sequence)-n+1):\n",
        "        ngram = tuple(sequence[i:i+n])\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        p = count[prefix][word] / max(total[prefix], 1)\n",
        "        if log:\n",
        "            print(\"p(%s | %s) =\\t%.3f\" % (word, ', '.join(prefix), p))\n",
        "\n",
        "        total_p *= p\n",
        "    return total_p\n",
        "    \n",
        "\n",
        "print(\"\\nProduct: p(%s) = %.3f\" % (''.join(sequence[1:-1]), sequence_p(sequence, log=True)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p(A | <bos>) =\t1.000\n",
            "p(A | A) =\t0.615\n",
            "p(B | A) =\t0.231\n",
            "p(<eos> | B) =\t0.500\n",
            "\n",
            "Product: p(AAB) = 0.071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmhMZ8HZkc9V",
        "colab_type": "text"
      },
      "source": [
        "### Real Example: Dialogue Utterances\n",
        "\n",
        "Now lets use the same ideas on a more realistic text corpus.\n",
        "\n",
        "We will use utterances from a dialogue dataset called **Persona-Chat**. This dataset is relatively small and centers on a single domain, but it is simple and interpretable for our purposes here.\n",
        "\n",
        "We'll also use an off-the-shelf ngram modeling package called `KenLM`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfcaKA0zkc9W",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmJRo-2pkc9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "8fedaafe-ad93-4514-9e63-afe09ab567d5"
      },
      "source": [
        "import kenlm"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-b73c44c79f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kenlm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2vfTPw_kc9a",
        "colab_type": "code",
        "colab": {},
        "outputId": "3e3340ae-8506-4964-ab5f-6709c555cf12"
      },
      "source": [
        "train = []\n",
        "valid = []\n",
        "import jsonlines\n",
        "\n",
        "for ds, name in [(train, 'train'), (valid, 'valid')]:\n",
        "    for line in jsonlines.Reader(open('data/personachat/personachat_all_sentences_%s.jsonl' % name, 'r')):\n",
        "        ds.append(line['tokens'])\n",
        "        \n",
        "vocab = list(set([t for ts in train for t in ts]))      \n",
        "print(\"Number of train examples: %d\" % (len(train)))\n",
        "print(\"Number of valid examples: %d\" % (len(valid)))\n",
        "print(\"Vocab size: %d\" % (len(vocab)))\n",
        "\n",
        "print(\"\\nExamples:\")\n",
        "pprint(train[:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/personachat/personachat_all_sentences_train.jsonl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d8cef74bbb4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjsonlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/personachat/personachat_all_sentences_%s.jsonl'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/personachat/personachat_all_sentences_train.jsonl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76-Jc4iQkc9f",
        "colab_type": "text"
      },
      "source": [
        "### KenLM\n",
        "\n",
        "KenLM estimates n-gram language models using **modified Kneser-Ney smoothing**, and has a fast and memory-efficient implementation. \n",
        "- While we won't go into details here, **smoothing** is a technique used to account for ngrams that do not occur in the training corpus. \n",
        "- Normally, these ngrams would receive zero-probability mass. Smoothing ensures every ngram receives some probability.\n",
        "\n",
        "\n",
        "\n",
        "Please see page 48 of the [lecture note](https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf) for an overview of Kneser-Ney smoothing, and [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) for further details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iUGtxSmkc9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KENLM_DIR='/home/jupyter/kenlm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzrkwwj5kc9m",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUnjr7ndkc9n",
        "colab_type": "code",
        "colab": {},
        "outputId": "b12dd54a-8dd7-4a54-ccb2-42ce76bf04ae"
      },
      "source": [
        "if not os.path.exists('data/tokenized'):\n",
        "    os.makedirs('data/tokenized')\n",
        "with open('data/tokenized/pchat_train', 'w') as f:\n",
        "    for line in tqdm(train):\n",
        "        f.write(' '.join(line))\n",
        "        f.write('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 133176/133176 [00:00<00:00, 795531.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxnxlaCJkc9r",
        "colab_type": "text"
      },
      "source": [
        "#### Build kenlm n-gram models\n",
        "\n",
        "This uses the `kenlm` commands `lmplz` to estimate the language model, then `build_binary` to convert it to an efficient format. We load the resulting model using the `kenlm` python wrapper.\n",
        "\n",
        "We do this for n-gram models of order `2,3,4`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4wRuUvzkc9s",
        "colab_type": "code",
        "colab": {},
        "outputId": "3c44f0e8-a9a6-438e-c2b7-b0bf660ae329"
      },
      "source": [
        "import kenlm\n",
        "\n",
        "data_prefix = 'pchat'\n",
        "dataset = 'pchat_train'\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "models = {}\n",
        "for n in [2,3,4]:\n",
        "    model_temp = 'models/%s_%d.arpa' % (data_prefix, n)\n",
        "    model_name = 'models/%s_%d.klm' % (data_prefix, n)\n",
        "    ! cat ./data/tokenized/$dataset | $KENLM_DIR/build/bin/lmplz -o $n > $model_temp\n",
        "    ! $KENLM_DIR/build/bin/build_binary $model_temp $model_name\n",
        "    models[n] = kenlm.LanguageModel(model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
            "Unigram tokens 1602042 types 19156\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:229872 2:10928379904\n",
            "Statistics:\n",
            "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
            "2 231267 D1=0.708637 D2=1.06132 D3+=1.37629\n",
            "Memory estimate for binary LM:\n",
            "type      kB\n",
            "probing 4551 assuming -p 1.5\n",
            "probing 4626 assuming -r models -p 1.5\n",
            "trie    1747 without quantization\n",
            "trie    1099 assuming -q 8 -b 8 quantization \n",
            "trie    1747 assuming -a 22 array pointer compression\n",
            "trie    1099 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10874216 kB\tVmRSS:10720 kB\tRSSMax:2938548 kB\tuser:0.848\tsys:1.288\tCPU:2.13913\treal:2.13823\n",
            "Reading models/pchat_2.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
            "Unigram tokens 1602042 types 19156\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:229872 2:3801175808 3:7127204352\n",
            "Statistics:\n",
            "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
            "2 231267 D1=0.722265 D2=1.09097 D3+=1.44737\n",
            "3 617624 D1=0.798924 D2=1.0703 D3+=1.33013\n",
            "Memory estimate for binary LM:\n",
            "type       kB\n",
            "probing 16763 assuming -p 1.5\n",
            "probing 18193 assuming -r models -p 1.5\n",
            "trie     6683 without quantization\n",
            "trie     3625 assuming -q 8 -b 8 quantization \n",
            "trie     6354 assuming -a 22 array pointer compression\n",
            "trie     3296 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272 3:12352480\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272 3:12352480\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10866020 kB\tVmRSS:22604 kB\tRSSMax:2500264 kB\tuser:1.408\tsys:1.228\tCPU:2.63778\treal:2.52355\n",
            "Reading models/pchat_3.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
            "Unigram tokens 1602042 types 19156\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:229872 2:1860149760 3:3487780608 4:5580449280\n",
            "Statistics:\n",
            "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
            "2 231267 D1=0.722265 D2=1.09097 D3+=1.44737\n",
            "3 617624 D1=0.821472 D2=1.14168 D3+=1.40217\n",
            "4 932153 D1=0.863174 D2=1.12888 D3+=1.29333\n",
            "Memory estimate for binary LM:\n",
            "type       kB\n",
            "probing 36767 assuming -p 1.5\n",
            "probing 41816 assuming -r models -p 1.5\n",
            "trie    15838 without quantization\n",
            "trie     8356 assuming -q 8 -b 8 quantization \n",
            "trie    14567 assuming -a 22 array pointer compression\n",
            "trie     7085 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272 3:12352480 4:22371672\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:229872 2:3700272 3:12352480 4:22371672\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10865048 kB\tVmRSS:44524 kB\tRSSMax:2182940 kB\tuser:2.32\tsys:1.384\tCPU:3.7062\treal:3.30219\n",
            "Reading models/pchat_4.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27iI9JNPkc9v",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "### Perplexity\n",
        "\n",
        "Intuitively, a good model should assign high probabilities to sequences from the 'true' distribution that it is modeling.\n",
        "\n",
        "A common way of quantifying this is with **perplexity**, a metric inversely-proportional to the probability that the model assigns to a set of sequences, e.g. a 'test set':\n",
        "\n",
        "\\begin{align}\n",
        "\\large \\text{ppl}(p, D) &\\large\\ = 2^{-\\frac{1}{N_{total}}\\log_2 p(D)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "where $D=\\{(w_1,\\ldots,w_{N_i})_i\\}_{i=1}^M$ is a dataset of $M$ sequences with total length $N_{\\text{total}}=\\sum_{i}N_i$.\n",
        "\n",
        "Perplexity is defined on $[1,\\infty)$, with 1 being a perfect model (assigning probability 1 to $D$), and a 'worse' model as perplexity increases.\n",
        "\n",
        "Intuitively, _perplexity measures the average rank of the true next-token, when tokens are ordered by the model's conditional probabilities_.\n",
        "\n",
        "\n",
        "<!-- Section 1.3 of [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) has a concise summary of perplexity and its motivation. !-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt_zSYXSkc9w",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate Perplexity\n",
        "\n",
        "`kenlm` outputs log probabilities in **log base 10**. For the standard definition of perplexity we need **log base 2**. See the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB6fx7Lxkc9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity_kenlm(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        text = ' '.join(sequence)\n",
        "        logp_total += model.score(text)\n",
        "        n_total += len(sequence) + 1  # add 1 for </s>\n",
        "        \n",
        "    # Convert log10 to log2\n",
        "    log2p_total = logp_total / np.log10(2)\n",
        "    ppl = 2 ** (- (1.0 / n_total) * log2p_total)\n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBBeKGWYkc93",
        "colab_type": "code",
        "colab": {},
        "outputId": "acadb47e-0be7-4f58-8cb3-6e31a1031025"
      },
      "source": [
        "print(\"=== Train ===\")\n",
        "df = pd.DataFrame([(n, perplexity_kenlm(models[n], train)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
        "df.style.hide_index()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Train ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >n</th>        <th class=\"col_heading level0 col1\" >ppl</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                                <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row0_col0\" class=\"data row0 col0\" >2</td>\n",
              "                        <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row0_col1\" class=\"data row0 col1\" >39.9248</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                                <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row1_col0\" class=\"data row1 col0\" >3</td>\n",
              "                        <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row1_col1\" class=\"data row1 col1\" >15.462</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                                <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row2_col0\" class=\"data row2 col0\" >4</td>\n",
              "                        <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row2_col1\" class=\"data row2 col1\" >8.91961</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f4bbc9f9190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrLmLurBkc97",
        "colab_type": "code",
        "colab": {},
        "outputId": "8dfd6673-3e78-4885-e46d-f1a8e4e4c9ee"
      },
      "source": [
        "print(\"=== Valid ===\")\n",
        "df = pd.DataFrame([(n, perplexity_kenlm(models[n], valid)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
        "df.style.hide_index()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Valid ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >n</th>        <th class=\"col_heading level0 col1\" >ppl</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                                <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row0_col0\" class=\"data row0 col0\" >2</td>\n",
              "                        <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row0_col1\" class=\"data row0 col1\" >59.188</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                                <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row1_col0\" class=\"data row1 col0\" >3</td>\n",
              "                        <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row1_col1\" class=\"data row1 col1\" >44.9771</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                                <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row2_col0\" class=\"data row2 col0\" >4</td>\n",
              "                        <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row2_col1\" class=\"data row2 col1\" >43.2188</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f4bac0852d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoazaAElkc9-",
        "colab_type": "text"
      },
      "source": [
        "#### Sequence probabilities:\n",
        "\\begin{align}\n",
        "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-2},w_{t-1})\\\\\n",
        "&=\\sum_{t=1}^T \\log p(w_t|w_{t-2},w_{t-1}).\n",
        "\\end{align}\n",
        "\n",
        "where we use log probabilities in practice to avoid a product of many small numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beZ-ceQBkc9_",
        "colab_type": "code",
        "colab": {},
        "outputId": "978b574f-aacb-4226-83a1-c5beeb14d932"
      },
      "source": [
        "sentences = [\n",
        "    'i like my pet dog .',\n",
        "    'i like my pet zebra .',\n",
        "    'i like my pet lion .',\n",
        "    'i live in the united states .',\n",
        "    'i live in the united states of america .'\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "    for n in [2, 3, 4]:\n",
        "        log10p = models[n].score(sentence)\n",
        "        log2p = log10p / np.log10(2)\n",
        "        print(\"n: %d\\t logp: %.3f\" % (n, log2p))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i like my pet dog .\n",
            "n: 2\t logp: -29.216\n",
            "n: 3\t logp: -28.843\n",
            "n: 4\t logp: -29.219\n",
            "\n",
            "i like my pet zebra .\n",
            "n: 2\t logp: -32.303\n",
            "n: 3\t logp: -31.157\n",
            "n: 4\t logp: -32.101\n",
            "\n",
            "i like my pet lion .\n",
            "n: 2\t logp: -38.352\n",
            "n: 3\t logp: -41.017\n",
            "n: 4\t logp: -42.320\n",
            "\n",
            "i live in the united states .\n",
            "n: 2\t logp: -22.599\n",
            "n: 3\t logp: -20.095\n",
            "n: 4\t logp: -17.854\n",
            "\n",
            "i live in the united states of america .\n",
            "n: 2\t logp: -40.958\n",
            "n: 3\t logp: -26.627\n",
            "n: 4\t logp: -24.311\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}