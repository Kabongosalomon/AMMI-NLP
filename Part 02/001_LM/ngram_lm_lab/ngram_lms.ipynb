{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Kabongosalomon/AMMI-NLP/blob/master/Part%2002/001_LM/ngram_lm_lab/ngram_lms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A2pdf3Sakc8b"
   },
   "source": [
    "# N-Gram Language Models\n",
    "\n",
    "<font color='red'>Uncomment the code below if you're on Colab</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1SYTnNPFk3wI"
   },
   "source": [
    "## run code in this section if you're on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Cwfrk0CvmSXf",
    "outputId": "8306aab0-57fe-40fe-b96f-bb2c4ae4d13c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.4ubuntu1).\n",
      "liblzma-dev is already the newest version (5.2.2-1.3).\n",
      "liblzma-dev set to manually installed.\n",
      "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
      "zlib1g-dev set to manually installed.\n",
      "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
      "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
      "libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n",
      "libbz2-dev set to manually installed.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
      "--2020-03-21 00:54:59--  https://kheafield.com/code/kenlm.tar.gz\n",
      "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
      "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 490005 (479K) [application/x-gzip]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>] 478.52K  1007KB/s    in 0.5s    \n",
      "\n",
      "2020-03-21 00:54:59 (1007 KB/s) - written to stdout [490005/490005]\n",
      "\n",
      "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab/kenlm/build\n",
      "-- The C compiler identification is GNU 7.5.0\n",
      "-- The CXX compiler identification is GNU 7.5.0\n",
      "-- Check for working C compiler: /usr/bin/cc\n",
      "-- Check for working C compiler: /usr/bin/cc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++\n",
      "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Looking for pthread_create\n",
      "-- Looking for pthread_create - not found\n",
      "-- Looking for pthread_create in pthreads\n",
      "-- Looking for pthread_create in pthreads - not found\n",
      "-- Looking for pthread_create in pthread\n",
      "-- Looking for pthread_create in pthread - found\n",
      "-- Found Threads: TRUE  \n",
      "-- Boost version: 1.65.1\n",
      "-- Found the following Boost libraries:\n",
      "--   program_options\n",
      "--   system\n",
      "--   thread\n",
      "--   unit_test_framework\n",
      "--   chrono\n",
      "--   date_time\n",
      "--   atomic\n",
      "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
      "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n",
      "-- Looking for BZ2_bzCompressInit\n",
      "-- Looking for BZ2_bzCompressInit - found\n",
      "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Found LibLZMA: /usr/include (found version \"5.2.2\") \n",
      "-- Could NOT find Eigen3 (missing: EIGEN3_INCLUDE_DIR EIGEN3_VERSION_OK) (Required is at least version \"2.91.0\")\n",
      "CMake Warning at lm/interpolate/CMakeLists.txt:65 (message):\n",
      "  Not building interpolation.  Eigen3 was not found.\n",
      "\n",
      "\n",
      "-- To install Eigen3 in your home directory, copy paste this:\n",
      "export EIGEN3_ROOT=$HOME/eigen-eigen-07105f7124f9\n",
      "(cd $HOME; wget -O - https://bitbucket.org/eigen/eigen/get/3.2.8.tar.bz2 |tar xj)\n",
      "rm CMakeCache.txt\n",
      "\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab/kenlm/build\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm_filter\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm_util\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
      "[ 18%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
      "[ 18%] Built target kenlm_filter\n",
      "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
      "[ 43%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
      "[ 43%] Built target kenlm_util\n",
      "\u001b[35m\u001b[1mScanning dependencies of target probing_hash_table_benchmark\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
      "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
      "[ 65%] Built target probing_hash_table_benchmark\n",
      "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
      "[ 71%] Built target kenlm\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm_benchmark\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target fragment\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target build_binary\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target query\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
      "[ 77%] Built target fragment\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target kenlm_builder\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
      "[ 80%] Built target build_binary\n",
      "\u001b[35m\u001b[1mScanning dependencies of target phrase_table_vocab\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
      "[ 82%] Built target query\n",
      "\u001b[35m\u001b[1mScanning dependencies of target filter\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
      "[ 86%] Built target phrase_table_vocab\n",
      "[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
      "[ 92%] Built target kenlm_benchmark\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
      "[ 93%] Built target filter\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
      "[ 95%] Built target kenlm_builder\n",
      "\u001b[35m\u001b[1mScanning dependencies of target count_ngrams\u001b[0m\n",
      "\u001b[35m\u001b[1mScanning dependencies of target lmplz\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
      "[ 98%] Built target lmplz\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
      "[100%] Built target count_ngrams\n",
      "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab/kenlm\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating kenlm.egg-info\n",
      "writing kenlm.egg-info/PKG-INFO\n",
      "writing dependency_links to kenlm.egg-info/dependency_links.txt\n",
      "writing top-level names to kenlm.egg-info/top_level.txt\n",
      "writing manifest file 'kenlm.egg-info/SOURCES.txt'\n",
      "reading manifest file 'kenlm.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'kenlm.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_ext\n",
      "building 'kenlm' extension\n",
      "creating build/temp.linux-x86_64-3.6\n",
      "creating build/temp.linux-x86_64-3.6/util\n",
      "creating build/temp.linux-x86_64-3.6/lm\n",
      "creating build/temp.linux-x86_64-3.6/util/double-conversion\n",
      "creating build/temp.linux-x86_64-3.6/python\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/integer_to_string.cc -o build/temp.linux-x86_64-3.6/util/integer_to_string.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/parallel_read.cc -o build/temp.linux-x86_64-3.6/util/parallel_read.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/string_piece.cc -o build/temp.linux-x86_64-3.6/util/string_piece.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/float_to_string.cc -o build/temp.linux-x86_64-3.6/util/float_to_string.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/bit_packing.cc -o build/temp.linux-x86_64-3.6/util/bit_packing.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/read_compressed.cc -o build/temp.linux-x86_64-3.6/util/read_compressed.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/spaces.cc -o build/temp.linux-x86_64-3.6/util/spaces.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/murmur_hash.cc -o build/temp.linux-x86_64-3.6/util/murmur_hash.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/file_piece.cc -o build/temp.linux-x86_64-3.6/util/file_piece.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/pool.cc -o build/temp.linux-x86_64-3.6/util/pool.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/scoped.cc -o build/temp.linux-x86_64-3.6/util/scoped.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/exception.cc -o build/temp.linux-x86_64-3.6/util/exception.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/mmap.cc -o build/temp.linux-x86_64-3.6/util/mmap.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/file.cc -o build/temp.linux-x86_64-3.6/util/file.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/usage.cc -o build/temp.linux-x86_64-3.6/util/usage.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/ersatz_progress.cc -o build/temp.linux-x86_64-3.6/util/ersatz_progress.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/vocab.cc -o build/temp.linux-x86_64-3.6/lm/vocab.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/value_build.cc -o build/temp.linux-x86_64-3.6/lm/value_build.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/lm_exception.cc -o build/temp.linux-x86_64-3.6/lm/lm_exception.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/config.cc -o build/temp.linux-x86_64-3.6/lm/config.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/trie.cc -o build/temp.linux-x86_64-3.6/lm/trie.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/binary_format.cc -o build/temp.linux-x86_64-3.6/lm/binary_format.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/model.cc -o build/temp.linux-x86_64-3.6/lm/model.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/sizes.cc -o build/temp.linux-x86_64-3.6/lm/sizes.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/bhiksha.cc -o build/temp.linux-x86_64-3.6/lm/bhiksha.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/read_arpa.cc -o build/temp.linux-x86_64-3.6/lm/read_arpa.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/search_hashed.cc -o build/temp.linux-x86_64-3.6/lm/search_hashed.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/virtual_interface.cc -o build/temp.linux-x86_64-3.6/lm/virtual_interface.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/trie_sort.cc -o build/temp.linux-x86_64-3.6/lm/trie_sort.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/quantize.cc -o build/temp.linux-x86_64-3.6/lm/quantize.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c lm/search_trie.cc -o build/temp.linux-x86_64-3.6/lm/search_trie.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/fixed-dtoa.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/fixed-dtoa.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/strtod.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/strtod.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/bignum-dtoa.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/bignum-dtoa.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/double-conversion.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/double-conversion.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/bignum.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/bignum.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/diy-fp.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/diy-fp.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/cached-powers.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/cached-powers.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c util/double-conversion/fast-dtoa.cc -o build/temp.linux-x86_64-3.6/util/double-conversion/fast-dtoa.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c python/score_sentence.cc -o build/temp.linux-x86_64-3.6/python/score_sentence.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/include/python3.6m -c python/kenlm.cpp -o build/temp.linux-x86_64-3.6/python/kenlm.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 -DHAVE_ZLIB -DHAVE_BZLIB -DHAVE_XZLIB\n",
      "creating build/lib.linux-x86_64-3.6\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/util/integer_to_string.o build/temp.linux-x86_64-3.6/util/parallel_read.o build/temp.linux-x86_64-3.6/util/string_piece.o build/temp.linux-x86_64-3.6/util/float_to_string.o build/temp.linux-x86_64-3.6/util/bit_packing.o build/temp.linux-x86_64-3.6/util/read_compressed.o build/temp.linux-x86_64-3.6/util/spaces.o build/temp.linux-x86_64-3.6/util/murmur_hash.o build/temp.linux-x86_64-3.6/util/file_piece.o build/temp.linux-x86_64-3.6/util/pool.o build/temp.linux-x86_64-3.6/util/scoped.o build/temp.linux-x86_64-3.6/util/exception.o build/temp.linux-x86_64-3.6/util/mmap.o build/temp.linux-x86_64-3.6/util/file.o build/temp.linux-x86_64-3.6/util/usage.o build/temp.linux-x86_64-3.6/util/ersatz_progress.o build/temp.linux-x86_64-3.6/lm/vocab.o build/temp.linux-x86_64-3.6/lm/value_build.o build/temp.linux-x86_64-3.6/lm/lm_exception.o build/temp.linux-x86_64-3.6/lm/config.o build/temp.linux-x86_64-3.6/lm/trie.o build/temp.linux-x86_64-3.6/lm/binary_format.o build/temp.linux-x86_64-3.6/lm/model.o build/temp.linux-x86_64-3.6/lm/sizes.o build/temp.linux-x86_64-3.6/lm/bhiksha.o build/temp.linux-x86_64-3.6/lm/read_arpa.o build/temp.linux-x86_64-3.6/lm/search_hashed.o build/temp.linux-x86_64-3.6/lm/virtual_interface.o build/temp.linux-x86_64-3.6/lm/trie_sort.o build/temp.linux-x86_64-3.6/lm/quantize.o build/temp.linux-x86_64-3.6/lm/search_trie.o build/temp.linux-x86_64-3.6/util/double-conversion/fixed-dtoa.o build/temp.linux-x86_64-3.6/util/double-conversion/strtod.o build/temp.linux-x86_64-3.6/util/double-conversion/bignum-dtoa.o build/temp.linux-x86_64-3.6/util/double-conversion/double-conversion.o build/temp.linux-x86_64-3.6/util/double-conversion/bignum.o build/temp.linux-x86_64-3.6/util/double-conversion/diy-fp.o build/temp.linux-x86_64-3.6/util/double-conversion/cached-powers.o build/temp.linux-x86_64-3.6/util/double-conversion/fast-dtoa.o build/temp.linux-x86_64-3.6/python/score_sentence.o build/temp.linux-x86_64-3.6/python/kenlm.o -lstdc++ -lrt -lz -lbz2 -llzma -o build/lib.linux-x86_64-3.6/kenlm.cpython-36m-x86_64-linux-gnu.so\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "copying build/lib.linux-x86_64-3.6/kenlm.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "creating stub loader for kenlm.cpython-36m-x86_64-linux-gnu.so\n",
      "byte-compiling build/bdist.linux-x86_64/egg/kenlm.py to kenlm.cpython-36.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying kenlm.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying kenlm.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying kenlm.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying kenlm.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "__pycache__.kenlm.cpython-36: module references __file__\n",
      "creating dist\n",
      "creating 'dist/kenlm-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing kenlm-0.0.0-py3.6-linux-x86_64.egg\n",
      "creating /usr/local/lib/python3.6/dist-packages/kenlm-0.0.0-py3.6-linux-x86_64.egg\n",
      "Extracting kenlm-0.0.0-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
      "Adding kenlm 0.0.0 to easy-install.pth file\n",
      "\n",
      "Installed /usr/local/lib/python3.6/dist-packages/kenlm-0.0.0-py3.6-linux-x86_64.egg\n",
      "Processing dependencies for kenlm==0.0.0\n",
      "Finished processing dependencies for kenlm==0.0.0\n",
      "/content/AMMI-NLP/Part 02/001_LM/ngram_lm_lab\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/Kabongosalomon/AMMI-NLP.git\n",
    "\n",
    "# %cd AMMI-NLP/Part\\ 02/001_LM/ngram_lm_lab  \n",
    "\n",
    "# !sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
    "\n",
    "# !wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
    "# !mkdir kenlm/build\n",
    "# %cd kenlm/build\n",
    "# !cmake ..\n",
    "# !make -j 4\n",
    "\n",
    "# %cd ..\n",
    "\n",
    "# ! python setup.py install\n",
    "\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dBUVJ9W6tF3M",
    "outputId": "ab903d7b-9f79-4fa2-87e8-5ee0f95fe350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  img  kenlm  models  ngram_lms.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euWKGoeTebwo"
   },
   "source": [
    "<font color='red'>Please, restard runtime before running the code bellow</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KuCAHVpwkc8f"
   },
   "source": [
    "#### Preliminaries: Install KenLM\n",
    "\n",
    "[KenLM](https://kheafield.com/code/kenlm/) is used in the latter part of this lab.\n",
    "\n",
    "To install it, run the following commands **in the jupyter terminal** (see screenshot):\n",
    "\n",
    "    sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
    "    \n",
    "    cd /home/jupyter\n",
    "    wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
    "    mkdir kenlm/build\n",
    "    cd kenlm/build\n",
    "    cmake ..\n",
    "    make -j 4\n",
    "    \n",
    "    cd /home/jupyter/kenlm\n",
    "    python setup.py install\n",
    "    \n",
    "<img src=\"https://github.com/Kabongosalomon/AMMI-NLP/blob/master/Part%2002/001_LM/ngram_lm_lab/img/instruction.png?raw=1\" alt=\"Drawing\" style=\"width: 35%;\"/>\n",
    "\n",
    "<img src=\"https://github.com/Kabongosalomon/AMMI-NLP/blob/master/Part%2002/001_LM/ngram_lm_lab/img/instruction2.png?raw=1\" alt=\"Drawing\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "a3cKti6ikc8h",
    "outputId": "8a8927da-de52-421c-b7e9-8480d458470d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: six in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from jsonlines) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# %cd AMMI-NLP/Part\\ 02/001_LM/ngram_lm_lab  \n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "afUQ3YeFkc8o",
    "outputId": "e7dc173e-5119-40f7-9154-8d0fae612dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os, sys, glob, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fV4Sgonekc8t"
   },
   "source": [
    "### N-gram Language Modeling\n",
    "\n",
    "In **language modeling** we want to model the probability of variable length sequences, $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{<t}).$$\n",
    "\n",
    "An **n-gram language model** assumes that each word $w_t$ only depends on the preceding $n-1$ words, $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{t-n+1},\\ldots,w_{t-1}).$$\n",
    "\n",
    " \n",
    "\n",
    "#### Example\n",
    "For instance, when modeling the sentence $$\\texttt{the cat sat on the mat .}$$ a 3-gram language model assumes that $$p(\\texttt{mat}|\\texttt{the cat sat on the}) \\approx p(\\texttt{mat}|\\texttt{on the}).$$\n",
    "\n",
    "The sub-sequence $(\\texttt{on the mat})$ is a *3-gram* or *trigram*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GNHzT3Ckc8v"
   },
   "source": [
    "### Count-based Estimation\n",
    "\n",
    "Given some dataset $D$ of sequences, we can estimate an n-gram model through counting, derived as follows:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_t|w_{t-n+1},\\ldots,w_{t-1}) &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{p(w_{t-n+1},\\ldots,w_{t-1})} & \\text{definition of conditional probability}\\\\\n",
    "                       &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}p(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
    "                       &\\approx \\frac{\\frac{1}{N}\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\frac{1}{N}\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
    "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
    "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\text{count}(w_{t-n+1},\\ldots,w_{t-1})},\n",
    "\\end{align}\n",
    "\n",
    "where $N$ is the number of $n$-grams in the dataset.\n",
    "\n",
    "In Python, we can collect these counts into a dictionary mapping a prefix to a dictionary of counts:\n",
    "\n",
    "        count[(w_n+1,...,w_t-1)] = {wt1: count of (w_n+1,...,w_t1),\n",
    "                                    wt2: count of (w_n+1,...,w_t2),\n",
    "                                    ...\n",
    "                                   }\n",
    "                                   \n",
    "and for the denominator, maintain a dictionary of totals:\n",
    "\n",
    "        total[(w_n+1,...,w_t-1)] = count of w_n+1,...,w_t-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ADnsOtuFkc8y"
   },
   "source": [
    "### Simplified Language\n",
    "\n",
    "To get intuition, lets start by modeling a simple language called `ABC`. A word in this language is one of three tokens, $$w\\in \\{\\texttt{A, B, C}\\},$$\n",
    "and we'll denote a sentence as $\\textbf{w}=(w_1,\\ldots,w_{|\\textbf{w}|})$.\n",
    "\n",
    "\n",
    "Suppose we are given the following dataset, and want to estimate a **bigram model**: $$p(\\textbf{w})\\approx\\prod_{t=1}^{|\\textbf{w}|}p(w_t|w_{t-1})\\quad\\quad(*)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Juu4O0lHkc8z"
   },
   "outputs": [],
   "source": [
    "data_raw = [['A', 'A', 'B', 'B'],\n",
    "            ['A', 'A', 'B'],\n",
    "            ['A', 'A', 'B', 'C'],\n",
    "            ['A', 'A', 'A'],\n",
    "            ['A', 'A', 'A', 'A']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1UoSSBskc84"
   },
   "source": [
    "Since our model is a probability distribution, the total probability of all possible strings in the language must sum to 1, i.e.: $$\\sum_{\\textbf{w}}p(\\textbf{w})=1.$$\n",
    "\n",
    "In order to satisfy this criterion it turns out that we need an additional **beginning token**, `<bos>`, and **end token**, `<eos>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "orO-6eNUkc86",
    "outputId": "88c3ee08-4b6f-4e59-dfcd-ced193f2d50b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>', 'A', 'A', 'B', 'B', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'B', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'B', 'C', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'A', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'A', 'A', '<eos>']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['<bos>'] + d + ['<eos>'] for d in data_raw]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "261Nrt35kc8_"
   },
   "source": [
    "Now let's estimate a bigram model:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_t|w_{t-1}) &= \\frac{\\text{count}(w_{t-1}w_{t})}{\\sum_{w_{t'}}\\text{count}(w_{t-1}w_{t'})}\\\\\n",
    "               &= \\texttt{count[prefix][wt] / totals[prefix]}\n",
    "\\end{align} \n",
    "\n",
    "where $\\texttt{prefix}$ is $w_{t-1}$ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWdOw7kwkc9A"
   },
   "outputs": [],
   "source": [
    "count = defaultdict(lambda: defaultdict(float))\n",
    "total = defaultdict(float)\n",
    "\n",
    "n = 2\n",
    "for sequence in data:\n",
    "    for i in range(len(sequence)-n+1):         # for each ngram\n",
    "        ngram = tuple(sequence[i:i+n])\n",
    "        prefix, word = ngram[:-1], ngram[-1]\n",
    "        count[prefix][word] += 1               # count(w_{t-n+1}...w_t)\n",
    "        total[prefix] += 1                     # count(w_{t-n+1}...w_{t-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4nfqF8nkc9E"
   },
   "source": [
    "Let's see if the counts and totals make sense:\n",
    "\n",
    "- How many times did (A, B) occur? What about (B, B)?\n",
    "- How many times did (A) occur? What about (C)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "WjpmiW17kc9F",
    "outputId": "baec5aa9-100d-4e08-a914-071742517ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts:\n",
      "{('<bos>',): defaultdict(<class 'float'>, {'A': 5.0}),\n",
      " ('A',): defaultdict(<class 'float'>, {'A': 8.0, 'B': 3.0, '<eos>': 2.0}),\n",
      " ('B',): defaultdict(<class 'float'>, {'B': 1.0, '<eos>': 2.0, 'C': 1.0}),\n",
      " ('C',): defaultdict(<class 'float'>, {'<eos>': 1.0})}\n",
      "\n",
      "Totals:\n",
      "{('<bos>',): 5.0, ('A',): 13.0, ('B',): 4.0, ('C',): 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Counts:\")\n",
    "pprint(dict(count))\n",
    "print(\"\\nTotals:\")\n",
    "pprint(dict(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WngrJGDkkc9J"
   },
   "source": [
    "#### Conditional probability queries\n",
    "\n",
    "We can now query a conditional probability:\n",
    "\n",
    "\\begin{align}\n",
    "\\texttt{p(word|prefix)} =&\\ \\texttt{count[prefix][word] / totals[prefix]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "u1f4Elepkc9L",
    "outputId": "a19755bf-502f-4b74-fa6c-18e98d20dc32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p( A | <bos>) = \t1.00000\n",
      "p( C | B) = \t0.25000\n"
     ]
    }
   ],
   "source": [
    "queries = [('<bos>', 'A'),\n",
    "           ('B', 'C')]\n",
    "\n",
    "for query in queries:\n",
    "    prefix, word = query[:-1], query[-1]\n",
    "    p = count[prefix][word] / total[prefix]  # We'll discuss the case when `total[prefix] = 0` below.\n",
    "    print(\"p( %s | %s) = \\t%.5f\" % (word, ', '.join(prefix), p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9Sy3QL7kc9P"
   },
   "source": [
    "**Exercise**: Look at the training set and convince yourself that these conditional probabilities are correct according to the count-based estimation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RqfSC5Qkc9Q"
   },
   "source": [
    "#### Sequence Probability\n",
    "\n",
    "We can compute the probability of a sequence using the conditional probabilities along with the chain rule of probability:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-1})\n",
    "\\end{align}\n",
    "\n",
    "(Here $w_0$ is `<bos>` and $w_T$ is `<eos>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "nhnbZ3jykc9R",
    "outputId": "1b0c7489-3d82-41ea-aaeb-e643d495f92e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(A | <bos>) =\t1.000\n",
      "p(A | A) =\t0.615\n",
      "p(B | A) =\t0.231\n",
      "p(<eos> | B) =\t0.500\n",
      "\n",
      "Product: p(AAB) = 0.071\n"
     ]
    }
   ],
   "source": [
    "sequence = ['<bos>', 'A', 'A', 'B', '<eos>']\n",
    "\n",
    "def sequence_p(sequence, log=False):\n",
    "    total_p = 1\n",
    "\n",
    "    for i in range(len(sequence)-n+1):\n",
    "        ngram = tuple(sequence[i:i+n])\n",
    "        prefix = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        p = count[prefix][word] / max(total[prefix], 1)\n",
    "        if log:\n",
    "            print(\"p(%s | %s) =\\t%.3f\" % (word, ', '.join(prefix), p))\n",
    "\n",
    "        total_p *= p\n",
    "    return total_p\n",
    "    \n",
    "\n",
    "print(\"\\nProduct: p(%s) = %.3f\" % (''.join(sequence[1:-1]), sequence_p(sequence, log=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmhMZ8HZkc9V"
   },
   "source": [
    "### Real Example: Dialogue Utterances\n",
    "\n",
    "Now lets use the same ideas on a more realistic text corpus.\n",
    "\n",
    "We will use utterances from a dialogue dataset called **Persona-Chat**. This dataset is relatively small and centers on a single domain, but it is simple and interpretable for our purposes here.\n",
    "\n",
    "We'll also use an off-the-shelf ngram modeling package called `KenLM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfcaKA0zkc9W"
   },
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "MmJRo-2pkc9X",
    "outputId": "3866932f-32c4-48c5-816c-670a184b2d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  img  kenlm  models  ngram_lms.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "X2vfTPw_kc9a",
    "outputId": "c440c8ba-0e51-4ff0-b01c-dfef7eb1d17b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 133176\n",
      "Number of valid examples: 16181\n",
      "Vocab size: 19153\n",
      "\n",
      "Examples:\n",
      "[['i', 'am', 'doing', 'great', 'except', 'for', 'the', 'allergies', '.'],\n",
      " ['i', 'am', 'a', 'woman', 'what', 'about', 'you', '.'],\n",
      " ['i', 'thought', 'you', 'were', 'a', 'college', 'kid', '.']]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "valid = []\n",
    "import jsonlines\n",
    "\n",
    "for ds, name in [(train, 'train'), (valid, 'valid')]:\n",
    "    for line in jsonlines.Reader(open('data/personachat/personachat_all_sentences_%s.jsonl' % name, 'r')):\n",
    "        ds.append(line['tokens'])\n",
    "        \n",
    "vocab = list(set([t for ts in train for t in ts]))      \n",
    "print(\"Number of train examples: %d\" % (len(train)))\n",
    "print(\"Number of valid examples: %d\" % (len(valid)))\n",
    "print(\"Vocab size: %d\" % (len(vocab)))\n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "pprint(train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76-Jc4iQkc9f"
   },
   "source": [
    "### KenLM\n",
    "\n",
    "KenLM estimates n-gram language models using **modified Kneser-Ney smoothing**, and has a fast and memory-efficient implementation. \n",
    "- While we won't go into details here, **smoothing** is a technique used to account for ngrams that do not occur in the training corpus. \n",
    "- Normally, these ngrams would receive zero-probability mass. Smoothing ensures every ngram receives some probability.\n",
    "\n",
    "\n",
    "\n",
    "Please see page 48 of the [lecture note](https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf) for an overview of Kneser-Ney smoothing, and [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0iUGtxSmkc9g"
   },
   "outputs": [],
   "source": [
    "KENLM_DIR='./kenlm' # for colab and local computers\n",
    "# KENLM_DIR='/home/jupyter/kenlm' # for GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzrkwwj5kc9m"
   },
   "source": [
    "#### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BUnjr7ndkc9n",
    "outputId": "528b10ae-2268-4b69-dc95-4f0f9bc6b569"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133176/133176 [00:00<00:00, 711179.93it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('data/tokenized'):\n",
    "    os.makedirs('data/tokenized')\n",
    "with open('data/tokenized/pchat_train', 'w') as f:\n",
    "    for line in tqdm(train):\n",
    "        f.write(' '.join(line))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxnxlaCJkc9r"
   },
   "source": [
    "#### Build kenlm n-gram models\n",
    "\n",
    "This uses the `kenlm` commands `lmplz` to estimate the language model, then `build_binary` to convert it to an efficient format. We load the resulting model using the `kenlm` python wrapper.\n",
    "\n",
    "We do this for n-gram models of order `2,3,4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "x4wRuUvzkc9s",
    "outputId": "c0e17ab5-8da3-4755-dc9b-e0b445fd7d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "Unigram tokens 1602042 types 19156\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:229872 2:13355060224\n",
      "/home/aims/Desktop/kenlm/util/scoped.cc:20 in void* util::{anonymous}::InspectAddr(void*, std::size_t, const char*) threw MallocException because `!addr && requested'.\n",
      "Cannot allocate memory for 13355060224 bytes in malloc\n",
      "Aborted\n",
      "Reading models/pchat_2.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "End of file Byte: 0\n",
      "ERROR\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "Unigram tokens 1602042 types 19156\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:229872 2:4645238272 3:8709822464\n",
      "Statistics:\n",
      "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
      "2 231267 D1=0.722265 D2=1.09097 D3+=1.44737\n",
      "3 617624 D1=0.798924 D2=1.0703 D3+=1.33013\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 16763 assuming -p 1.5\n",
      "probing 18193 assuming -r models -p 1.5\n",
      "trie     6683 without quantization\n",
      "trie     3625 assuming -q 8 -b 8 quantization \n",
      "trie     6354 assuming -a 22 array pointer compression\n",
      "trie     3296 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272 3:12352480\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272 3:12352480\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13193612 kB\tVmRSS:23260 kB\tRSSMax:3049068 kB\tuser:1.2624\tsys:0.816377\tCPU:2.07881\treal:1.97458\n",
      "Reading models/pchat_3.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "Unigram tokens 1602042 types 19156\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:229872 2:2273201664 3:4262253056 4:6819604992\n",
      "Statistics:\n",
      "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
      "2 231267 D1=0.722265 D2=1.09097 D3+=1.44737\n",
      "3 617624 D1=0.821472 D2=1.14168 D3+=1.40217\n",
      "4 932153 D1=0.863174 D2=1.12888 D3+=1.29333\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 36767 assuming -p 1.5\n",
      "probing 41816 assuming -r models -p 1.5\n",
      "trie    15838 without quantization\n",
      "trie     8356 assuming -q 8 -b 8 quantization \n",
      "trie    14567 assuming -a 22 array pointer compression\n",
      "trie     7085 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272 3:12352480 4:22371672\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272 3:12352480 4:22371672\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13210000 kB\tVmRSS:45052 kB\tRSSMax:2658428 kB\tuser:1.80083\tsys:0.928808\tCPU:2.72967\treal:2.42089\n",
      "Reading models/pchat_4.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "data_prefix = 'pchat'\n",
    "dataset = 'pchat_train'\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "models = {}\n",
    "for n in [2,3,4]:\n",
    "    model_temp = 'models/%s_%d.arpa' % (data_prefix, n)\n",
    "    model_name = 'models/%s_%d.klm' % (data_prefix, n)\n",
    "    ! cat ./data/tokenized/$dataset | $KENLM_DIR/build/bin/lmplz -o $n > $model_temp\n",
    "    ! $KENLM_DIR/build/bin/build_binary $model_temp $model_name\n",
    "    models[n] = kenlm.LanguageModel(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27iI9JNPkc9v"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "Intuitively, a good model should assign high probabilities to sequences from the 'true' distribution that it is modeling.\n",
    "\n",
    "A common way of quantifying this is with **perplexity**, a metric inversely-proportional to the probability that the model assigns to a set of sequences, e.g. a 'test set':\n",
    "\n",
    "\\begin{align}\n",
    "\\large \\text{ppl}(p, D) &\\large\\ = 2^{-\\frac{1}{N_{total}}\\log_2 p(D)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $D=\\{(w_1,\\ldots,w_{N_i})_i\\}_{i=1}^M$ is a dataset of $M$ sequences with total length $N_{\\text{total}}=\\sum_{i}N_i$.\n",
    "\n",
    "Perplexity is defined on $[1,\\infty)$, with 1 being a perfect model (assigning probability 1 to $D$), and a 'worse' model as perplexity increases.\n",
    "\n",
    "Intuitively, _perplexity measures the average rank of the true next-token, when tokens are ordered by the model's conditional probabilities_.\n",
    "\n",
    "\n",
    "<!-- Section 1.3 of [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) has a concise summary of perplexity and its motivation. !-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bt_zSYXSkc9w"
   },
   "source": [
    "#### Evaluate Perplexity\n",
    "\n",
    "`kenlm` outputs log probabilities in **log base 10**. For the standard definition of perplexity we need **log base 2**. See the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TB6fx7Lxkc9y"
   },
   "outputs": [],
   "source": [
    "def perplexity_kenlm(model, sequences):\n",
    "    n_total = 0\n",
    "    logp_total = 0\n",
    "    for sequence in sequences:\n",
    "        text = ' '.join(sequence)\n",
    "        logp_total += model.score(text)\n",
    "        n_total += len(sequence) + 1  # add 1 for </s>\n",
    "        \n",
    "    # Convert log10 to log2\n",
    "    log2p_total = logp_total / np.log10(2)\n",
    "    ppl = 2 ** (- (1.0 / n_total) * log2p_total)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "DBBeKGWYkc93",
    "outputId": "bdb50d3e-8e1e-400a-979e-84082caad1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_e5837c9e_6c96_11ea_ad9b_5c879cd0a093\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >n</th>        <th class=\"col_heading level0 col1\" >ppl</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_e5837c9e_6c96_11ea_ad9b_5c879cd0a093row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "                        <td id=\"T_e5837c9e_6c96_11ea_ad9b_5c879cd0a093row0_col1\" class=\"data row0 col1\" >39.9248</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_e5837c9e_6c96_11ea_ad9b_5c879cd0a093row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "                        <td id=\"T_e5837c9e_6c96_11ea_ad9b_5c879cd0a093row1_col1\" class=\"data row1 col1\" >15.462</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_e5837c9e_6c96_11ea_ad9b_5c879cd0a093row2_col0\" class=\"data row2 col0\" >4</td>\n",
       "                        <td id=\"T_e5837c9e_6c96_11ea_ad9b_5c879cd0a093row2_col1\" class=\"data row2 col1\" >8.91961</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff697dc12d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Train ===\")\n",
    "df = pd.DataFrame([(n, perplexity_kenlm(models[n], train)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
    "df.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "YrLmLurBkc97",
    "outputId": "550a83a0-06dd-4bf6-c8d3-a85ac189bab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Valid ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_eb9c1c6c_6c96_11ea_ad9b_5c879cd0a093\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >n</th>        <th class=\"col_heading level0 col1\" >ppl</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_eb9c1c6c_6c96_11ea_ad9b_5c879cd0a093row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "                        <td id=\"T_eb9c1c6c_6c96_11ea_ad9b_5c879cd0a093row0_col1\" class=\"data row0 col1\" >59.188</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_eb9c1c6c_6c96_11ea_ad9b_5c879cd0a093row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "                        <td id=\"T_eb9c1c6c_6c96_11ea_ad9b_5c879cd0a093row1_col1\" class=\"data row1 col1\" >44.9771</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_eb9c1c6c_6c96_11ea_ad9b_5c879cd0a093row2_col0\" class=\"data row2 col0\" >4</td>\n",
       "                        <td id=\"T_eb9c1c6c_6c96_11ea_ad9b_5c879cd0a093row2_col1\" class=\"data row2 col1\" >43.2188</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff69616bc10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Valid ===\")\n",
    "df = pd.DataFrame([(n, perplexity_kenlm(models[n], valid)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
    "df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoazaAElkc9-"
   },
   "source": [
    "#### Sequence probabilities:\n",
    "\\begin{align}\n",
    "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-2},w_{t-1})\\\\\n",
    "&=\\sum_{t=1}^T \\log p(w_t|w_{t-2},w_{t-1}).\n",
    "\\end{align}\n",
    "\n",
    "where we use log probabilities in practice to avoid a product of many small numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "colab_type": "code",
    "id": "beZ-ceQBkc9_",
    "outputId": "a2e4c376-41d3-4c3b-ec36-ce4ff0f3b3b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like my pet dog .\n",
      "n: 2\t logp: -29.216\n",
      "n: 3\t logp: -28.843\n",
      "n: 4\t logp: -29.219\n",
      "\n",
      "i like my pet zebra .\n",
      "n: 2\t logp: -32.303\n",
      "n: 3\t logp: -31.157\n",
      "n: 4\t logp: -32.101\n",
      "\n",
      "i like my pet lion .\n",
      "n: 2\t logp: -38.352\n",
      "n: 3\t logp: -41.017\n",
      "n: 4\t logp: -42.320\n",
      "\n",
      "i live in the united states .\n",
      "n: 2\t logp: -22.599\n",
      "n: 3\t logp: -20.095\n",
      "n: 4\t logp: -17.854\n",
      "\n",
      "i live in the united states of america .\n",
      "n: 2\t logp: -40.958\n",
      "n: 3\t logp: -26.627\n",
      "n: 4\t logp: -24.311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'i like my pet dog .',\n",
    "    'i like my pet zebra .',\n",
    "    'i like my pet lion .',\n",
    "    'i live in the united states .',\n",
    "    'i live in the united states of america .'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    for n in [2, 3, 4]:\n",
    "        log10p = models[n].score(sentence)\n",
    "        log2p = log10p / np.log10(2)\n",
    "        print(\"n: %d\\t logp: %.3f\" % (n, log2p))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "ngram_lms.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
