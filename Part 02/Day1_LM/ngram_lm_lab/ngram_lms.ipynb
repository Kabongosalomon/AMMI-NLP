{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries: Install KenLM\n",
    "\n",
    "[KenLM](https://kheafield.com/code/kenlm/) is used in the latter part of this lab.\n",
    "\n",
    "To install it, run the following commands **in the jupyter terminal** (see screenshot):\n",
    "\n",
    "    sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
    "    \n",
    "    cd /home/jupyter\n",
    "    wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
    "    mkdir kenlm/build\n",
    "    cd kenlm/build\n",
    "    cmake ..\n",
    "    make -j 4\n",
    "    \n",
    "    cd /home/jupyter/kenlm\n",
    "    python setup.py install\n",
    "    \n",
    "<img src=\"img/instruction.png\" alt=\"Drawing\" style=\"width: 35%;\"/>\n",
    "\n",
    "<img src=\"img/instruction2.png\" alt=\"Drawing\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os, sys, glob, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Language Modeling\n",
    "\n",
    "In **language modeling** we want to model the probability of variable length sequences, $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{<t}).$$\n",
    "\n",
    "An **n-gram language model** assumes that each word $w_t$ only depends on the preceding $n-1$ words, $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{t-n+1},\\ldots,w_{t-1}).$$\n",
    "\n",
    " \n",
    "\n",
    "#### Example\n",
    "For instance, when modeling the sentence $$\\texttt{the cat sat on the mat .}$$ a 3-gram language model assumes that $$p(\\texttt{mat}|\\texttt{the cat sat on the}) \\approx p(\\texttt{mat}|\\texttt{on the}).$$\n",
    "\n",
    "The sub-sequence $(\\texttt{on the mat})$ is a *3-gram* or *trigram*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based Estimation\n",
    "\n",
    "Given some dataset $D$ of sequences, we can estimate an n-gram model through counting, derived as follows:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_t|w_{t-n+1},\\ldots,w_{t-1}) &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{p(w_{t-n+1},\\ldots,w_{t-1})} & \\text{definition of conditional probability}\\\\\n",
    "                       &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}p(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
    "                       &\\approx \\frac{\\frac{1}{N}\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\frac{1}{N}\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
    "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
    "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\text{count}(w_{t-n+1},\\ldots,w_{t-1})},\n",
    "\\end{align}\n",
    "\n",
    "where $N$ is the number of $n$-grams in the dataset.\n",
    "\n",
    "In Python, we can collect these counts into a dictionary mapping a prefix to a dictionary of counts:\n",
    "\n",
    "        count[(w_n+1,...,w_t-1)] = {wt1: count of (w_n+1,...,w_t1),\n",
    "                                    wt2: count of (w_n+1,...,w_t2),\n",
    "                                    ...\n",
    "                                   }\n",
    "                                   \n",
    "and for the denominator, maintain a dictionary of totals:\n",
    "\n",
    "        total[(w_n+1,...,w_t-1)] = count of w_n+1,...,w_t-1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Language\n",
    "\n",
    "To get intuition, lets start by modeling a simple language called `ABC`. A word in this language is one of three tokens, $$w\\in \\{\\texttt{A, B, C}\\},$$\n",
    "and we'll denote a sentence as $\\textbf{w}=(w_1,\\ldots,w_{|\\textbf{w}|})$.\n",
    "\n",
    "\n",
    "Suppose we are given the following dataset, and want to estimate a **bigram model**: $$p(\\textbf{w})\\approx\\prod_{t=1}^{|\\textbf{w}|}p(w_t|w_{t-1})\\quad\\quad(*)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = [['A', 'A', 'B', 'B'],\n",
    "            ['A', 'A', 'B'],\n",
    "            ['A', 'A', 'B', 'C'],\n",
    "            ['A', 'A', 'A'],\n",
    "            ['A', 'A', 'A', 'A']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is a probability distribution, the total probability of all possible strings in the language must sum to 1, i.e.: $$\\sum_{\\textbf{w}}p(\\textbf{w})=1.$$\n",
    "\n",
    "In order to satisfy this criterion it turns out that we need an additional **beginning token**, `<bos>`, and **end token**, `<eos>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>', 'A', 'A', 'B', 'B', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'B', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'B', 'C', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'A', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'A', 'A', '<eos>']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['<bos>'] + d + ['<eos>'] for d in data_raw]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's estimate a bigram model:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_t|w_{t-1}) &= \\frac{\\text{count}(w_{t-1}w_{t})}{\\sum_{w_{t'}}\\text{count}(w_{t-1}w_{t'})}\\\\\n",
    "               &= \\texttt{count[prefix][wt] / totals[prefix]}\n",
    "\\end{align} \n",
    "\n",
    "where $\\texttt{prefix}$ is $w_{t-1}$ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = defaultdict(lambda: defaultdict(float))\n",
    "total = defaultdict(float)\n",
    "\n",
    "n = 2\n",
    "for sequence in data:\n",
    "    for i in range(len(sequence)-n+1):         # for each ngram\n",
    "        ngram = tuple(sequence[i:i+n])\n",
    "        prefix, word = ngram[:-1], ngram[-1]\n",
    "        count[prefix][word] += 1               # count(w_{t-n+1}...w_t)\n",
    "        total[prefix] += 1                     # count(w_{t-n+1}...w_{t-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the counts and totals make sense:\n",
    "\n",
    "- How many times did (A, B) occur? What about (B, B)?\n",
    "- How many times did (A) occur? What about (C)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts:\n",
      "{('<bos>',): defaultdict(<class 'float'>, {'A': 5.0}),\n",
      " ('A',): defaultdict(<class 'float'>, {'A': 8.0, 'B': 3.0, '<eos>': 2.0}),\n",
      " ('B',): defaultdict(<class 'float'>, {'B': 1.0, '<eos>': 2.0, 'C': 1.0}),\n",
      " ('C',): defaultdict(<class 'float'>, {'<eos>': 1.0})}\n",
      "\n",
      "Totals:\n",
      "{('<bos>',): 5.0, ('A',): 13.0, ('B',): 4.0, ('C',): 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Counts:\")\n",
    "pprint(dict(count))\n",
    "print(\"\\nTotals:\")\n",
    "pprint(dict(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional probability queries\n",
    "\n",
    "We can now query a conditional probability:\n",
    "\n",
    "\\begin{align}\n",
    "\\texttt{p(word|prefix)} =&\\ \\texttt{count[prefix][word] / totals[prefix]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p( A | <bos>) = \t1.00000\n",
      "p( C | B) = \t0.25000\n"
     ]
    }
   ],
   "source": [
    "queries = [('<bos>', 'A'),\n",
    "           ('B', 'C')]\n",
    "\n",
    "for query in queries:\n",
    "    prefix, word = query[:-1], query[-1]\n",
    "    p = count[prefix][word] / total[prefix]  # We'll discuss the case when `total[prefix] = 0` below.\n",
    "    print(\"p( %s | %s) = \\t%.5f\" % (word, ', '.join(prefix), p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Look at the training set and convince yourself that these conditional probabilities are correct according to the count-based estimation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Probability\n",
    "\n",
    "We can compute the probability of a sequence using the conditional probabilities along with the chain rule of probability:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-1})\n",
    "\\end{align}\n",
    "\n",
    "(Here $w_0$ is `<bos>` and $w_T$ is `<eos>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(A | <bos>) =\t1.000\n",
      "p(A | A) =\t0.615\n",
      "p(B | A) =\t0.231\n",
      "p(<eos> | B) =\t0.500\n",
      "\n",
      "Product: p(AAB) = 0.071\n"
     ]
    }
   ],
   "source": [
    "sequence = ['<bos>', 'A', 'A', 'B', '<eos>']\n",
    "\n",
    "def sequence_p(sequence, log=False):\n",
    "    total_p = 1\n",
    "\n",
    "    for i in range(len(sequence)-n+1):\n",
    "        ngram = tuple(sequence[i:i+n])\n",
    "        prefix = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        p = count[prefix][word] / max(total[prefix], 1)\n",
    "        if log:\n",
    "            print(\"p(%s | %s) =\\t%.3f\" % (word, ', '.join(prefix), p))\n",
    "\n",
    "        total_p *= p\n",
    "    return total_p\n",
    "    \n",
    "\n",
    "print(\"\\nProduct: p(%s) = %.3f\" % (''.join(sequence[1:-1]), sequence_p(sequence, log=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Example: Dialogue Utterances\n",
    "\n",
    "Now lets use the same ideas on a more realistic text corpus.\n",
    "\n",
    "We will use utterances from a dialogue dataset called **Persona-Chat**. This dataset is relatively small and centers on a single domain, but it is simple and interpretable for our purposes here.\n",
    "\n",
    "We'll also use an off-the-shelf ngram modeling package called `KenLM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "valid = []\n",
    "import jsonlines\n",
    "\n",
    "for ds, name in [(train, 'train'), (valid, 'valid')]:\n",
    "    for line in jsonlines.Reader(open('data/personachat/personachat_all_sentences_%s.jsonl' % name, 'r')):\n",
    "        ds.append(line['tokens'])\n",
    "        \n",
    "vocab = list(set([t for ts in train for t in ts]))      \n",
    "print(\"Number of train examples: %d\" % (len(train)))\n",
    "print(\"Number of valid examples: %d\" % (len(valid)))\n",
    "print(\"Vocab size: %d\" % (len(vocab)))\n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "pprint(train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KenLM\n",
    "\n",
    "KenLM estimates n-gram language models using **modified Kneser-Ney smoothing**, and has a fast and memory-efficient implementation. \n",
    "- While we won't go into details here, **smoothing** is a technique used to account for ngrams that do not occur in the training corpus. \n",
    "- Normally, these ngrams would receive zero-probability mass. Smoothing ensures every ngram receives some probability.\n",
    "\n",
    "\n",
    "\n",
    "Please see page 48 of the [lecture note](https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf) for an overview of Kneser-Ney smoothing, and [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KENLM_DIR='/home/jupyter/kenlm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/tokenized'):\n",
    "    os.makedirs('data/tokenized')\n",
    "with open('data/tokenized/pchat_train', 'w') as f:\n",
    "    for line in tqdm(train):\n",
    "        f.write(' '.join(line))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build kenlm n-gram models\n",
    "\n",
    "This uses the `kenlm` commands `lmplz` to estimate the language model, then `build_binary` to convert it to an efficient format. We load the resulting model using the `kenlm` python wrapper.\n",
    "\n",
    "We do this for n-gram models of order `2,3,4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "\n",
    "data_prefix = 'pchat'\n",
    "dataset = 'pchat_train'\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "models = {}\n",
    "for n in [2,3,4]:\n",
    "    model_temp = 'models/%s_%d.arpa' % (data_prefix, n)\n",
    "    model_name = 'models/%s_%d.klm' % (data_prefix, n)\n",
    "    ! cat ./data/tokenized/$dataset | $KENLM_DIR/build/bin/lmplz -o $n > $model_temp\n",
    "    ! $KENLM_DIR/build/bin/build_binary $model_temp $model_name\n",
    "    models[n] = kenlm.LanguageModel(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "Intuitively, a good model should assign high probabilities to sequences from the 'true' distribution that it is modeling.\n",
    "\n",
    "A common way of quantifying this is with **perplexity**, a metric inversely-proportional to the probability that the model assigns to a set of sequences, e.g. a 'test set':\n",
    "\n",
    "\\begin{align}\n",
    "\\large \\text{ppl}(p, D) &\\large\\ = 2^{-\\frac{1}{N_{total}}\\log_2 p(D)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $D=\\{(w_1,\\ldots,w_{N_i})_i\\}_{i=1}^M$ is a dataset of $M$ sequences with total length $N_{\\text{total}}=\\sum_{i}N_i$.\n",
    "\n",
    "Perplexity is defined on $[1,\\infty)$, with 1 being a perfect model (assigning probability 1 to $D$), and a 'worse' model as perplexity increases.\n",
    "\n",
    "Intuitively, _perplexity measures the average rank of the true next-token, when tokens are ordered by the model's conditional probabilities_.\n",
    "\n",
    "\n",
    "<!-- Section 1.3 of [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) has a concise summary of perplexity and its motivation. !-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Perplexity\n",
    "\n",
    "`kenlm` outputs log probabilities in **log base 10**. For the standard definition of perplexity we need **log base 2**. See the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_kenlm(model, sequences):\n",
    "    n_total = 0\n",
    "    logp_total = 0\n",
    "    for sequence in sequences:\n",
    "        text = ' '.join(sequence)\n",
    "        logp_total += model.score(text)\n",
    "        n_total += len(sequence) + 1  # add 1 for </s>\n",
    "        \n",
    "    # Convert log10 to log2\n",
    "    log2p_total = logp_total / np.log10(2)\n",
    "    ppl = 2 ** (- (1.0 / n_total) * log2p_total)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Train ===\")\n",
    "df = pd.DataFrame([(n, perplexity_kenlm(models[n], train)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
    "df.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Valid ===\")\n",
    "df = pd.DataFrame([(n, perplexity_kenlm(models[n], valid)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
    "df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence probabilities:\n",
    "\\begin{align}\n",
    "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-2},w_{t-1})\\\\\n",
    "&=\\sum_{t=1}^T \\log p(w_t|w_{t-2},w_{t-1}).\n",
    "\\end{align}\n",
    "\n",
    "where we use log probabilities in practice to avoid a product of many small numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'i like my pet dog .',\n",
    "    'i like my pet zebra .',\n",
    "    'i like my pet lion .',\n",
    "    'i live in the united states .',\n",
    "    'i live in the united states of america .'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    for n in [2, 3, 4]:\n",
    "        log10p = models[n].score(sentence)\n",
    "        log2p = log10p / np.log10(2)\n",
    "        print(\"n: %d\\t logp: %.3f\" % (n, log2p))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
