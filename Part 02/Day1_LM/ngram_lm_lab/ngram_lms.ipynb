{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries: Install KenLM\n",
    "\n",
    "[KenLM](https://kheafield.com/code/kenlm/) is used in the latter part of this lab.\n",
    "\n",
    "To install it, run the following commands **in the jupyter terminal** (see screenshot):\n",
    "\n",
    "    sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
    "    \n",
    "    cd /home/jupyter\n",
    "    wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
    "    mkdir kenlm/build\n",
    "    cd kenlm/build\n",
    "    cmake ..\n",
    "    make -j 4\n",
    "    \n",
    "    cd /home/jupyter/kenlm\n",
    "    python setup.py install\n",
    "    \n",
    "<img src=\"img/instruction.png\" alt=\"Drawing\" style=\"width: 35%;\"/>\n",
    "\n",
    "<img src=\"img/instruction2.png\" alt=\"Drawing\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /opt/anaconda3/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from jsonlines) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os, sys, glob, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Language Modeling\n",
    "\n",
    "In **language modeling** we want to model the probability of variable length sequences, $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{<t}).$$\n",
    "\n",
    "An **n-gram language model** assumes that each word $w_t$ only depends on the preceding $n-1$ words, $$\\large p(w_1,\\ldots,w_T)=\\prod_{t=1}^T p(w_t|w_{t-n+1},\\ldots,w_{t-1}).$$\n",
    "\n",
    " \n",
    "\n",
    "#### Example\n",
    "For instance, when modeling the sentence $$\\texttt{the cat sat on the mat .}$$ a 3-gram language model assumes that $$p(\\texttt{mat}|\\texttt{the cat sat on the}) \\approx p(\\texttt{mat}|\\texttt{on the}).$$\n",
    "\n",
    "The sub-sequence $(\\texttt{on the mat})$ is a *3-gram* or *trigram*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based Estimation\n",
    "\n",
    "Given some dataset $D$ of sequences, we can estimate an n-gram model through counting, derived as follows:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_t|w_{t-n+1},\\ldots,w_{t-1}) &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{p(w_{t-n+1},\\ldots,w_{t-1})} & \\text{definition of conditional probability}\\\\\n",
    "                       &= \\frac{p(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}p(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
    "                       &\\approx \\frac{\\frac{1}{N}\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\frac{1}{N}\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
    "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\sum_{w_{t'}}\\text{count}(w_{t-n+1},\\ldots,w_{t-1},w_{t'})}\\\\\n",
    "                       &= \\frac{\\text{count}(w_{t-n+1},\\ldots,w_t)}{\\text{count}(w_{t-n+1},\\ldots,w_{t-1})},\n",
    "\\end{align}\n",
    "\n",
    "where $N$ is the number of $n$-grams in the dataset.\n",
    "\n",
    "In Python, we can collect these counts into a dictionary mapping a prefix to a dictionary of counts:\n",
    "\n",
    "        count[(w_n+1,...,w_t-1)] = {wt1: count of (w_n+1,...,w_t1),\n",
    "                                    wt2: count of (w_n+1,...,w_t2),\n",
    "                                    ...\n",
    "                                   }\n",
    "                                   \n",
    "and for the denominator, maintain a dictionary of totals:\n",
    "\n",
    "        total[(w_n+1,...,w_t-1)] = count of w_n+1,...,w_t-1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Language\n",
    "\n",
    "To get intuition, lets start by modeling a simple language called `ABC`. A word in this language is one of three tokens, $$w\\in \\{\\texttt{A, B, C}\\},$$\n",
    "and we'll denote a sentence as $\\textbf{w}=(w_1,\\ldots,w_{|\\textbf{w}|})$.\n",
    "\n",
    "\n",
    "Suppose we are given the following dataset, and want to estimate a **bigram model**: $$p(\\textbf{w})\\approx\\prod_{t=1}^{|\\textbf{w}|}p(w_t|w_{t-1})\\quad\\quad(*)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = [['A', 'A', 'B', 'B'],\n",
    "            ['A', 'A', 'B'],\n",
    "            ['A', 'A', 'B', 'C'],\n",
    "            ['A', 'A', 'A'],\n",
    "            ['A', 'A', 'A', 'A']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is a probability distribution, the total probability of all possible strings in the language must sum to 1, i.e.: $$\\sum_{\\textbf{w}}p(\\textbf{w})=1.$$\n",
    "\n",
    "In order to satisfy this criterion it turns out that we need an additional **beginning token**, `<bos>`, and **end token**, `<eos>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>', 'A', 'A', 'B', 'B', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'B', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'B', 'C', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'A', '<eos>'],\n",
       " ['<bos>', 'A', 'A', 'A', 'A', '<eos>']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['<bos>'] + d + ['<eos>'] for d in data_raw]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's estimate a bigram model:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_t|w_{t-1}) &= \\frac{\\text{count}(w_{t-1}w_{t})}{\\sum_{w_{t'}}\\text{count}(w_{t-1}w_{t'})}\\\\\n",
    "               &= \\texttt{count[prefix][wt] / totals[prefix]}\n",
    "\\end{align} \n",
    "\n",
    "where $\\texttt{prefix}$ is $w_{t-1}$ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = defaultdict(lambda: defaultdict(float))\n",
    "total = defaultdict(float)\n",
    "\n",
    "n = 2\n",
    "for sequence in data:\n",
    "    for i in range(len(sequence)-n+1):         # for each ngram\n",
    "        ngram = tuple(sequence[i:i+n])\n",
    "        prefix, word = ngram[:-1], ngram[-1]\n",
    "        count[prefix][word] += 1               # count(w_{t-n+1}...w_t)\n",
    "        total[prefix] += 1                     # count(w_{t-n+1}...w_{t-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the counts and totals make sense:\n",
    "\n",
    "- How many times did (A, B) occur? What about (B, B)?\n",
    "- How many times did (A) occur? What about (C)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts:\n",
      "{('<bos>',): defaultdict(<class 'float'>, {'A': 5.0}),\n",
      " ('A',): defaultdict(<class 'float'>, {'A': 8.0, 'B': 3.0, '<eos>': 2.0}),\n",
      " ('B',): defaultdict(<class 'float'>, {'B': 1.0, '<eos>': 2.0, 'C': 1.0}),\n",
      " ('C',): defaultdict(<class 'float'>, {'<eos>': 1.0})}\n",
      "\n",
      "Totals:\n",
      "{('<bos>',): 5.0, ('A',): 13.0, ('B',): 4.0, ('C',): 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Counts:\")\n",
    "pprint(dict(count))\n",
    "print(\"\\nTotals:\")\n",
    "pprint(dict(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional probability queries\n",
    "\n",
    "We can now query a conditional probability:\n",
    "\n",
    "\\begin{align}\n",
    "\\texttt{p(word|prefix)} =&\\ \\texttt{count[prefix][word] / totals[prefix]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p( A | <bos>) = \t1.00000\n",
      "p( C | B) = \t0.25000\n"
     ]
    }
   ],
   "source": [
    "queries = [('<bos>', 'A'),\n",
    "           ('B', 'C')]\n",
    "\n",
    "for query in queries:\n",
    "    prefix, word = query[:-1], query[-1]\n",
    "    p = count[prefix][word] / total[prefix]  # We'll discuss the case when `total[prefix] = 0` below.\n",
    "    print(\"p( %s | %s) = \\t%.5f\" % (word, ', '.join(prefix), p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Look at the training set and convince yourself that these conditional probabilities are correct according to the count-based estimation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Probability\n",
    "\n",
    "We can compute the probability of a sequence using the conditional probabilities along with the chain rule of probability:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-1})\n",
    "\\end{align}\n",
    "\n",
    "(Here $w_0$ is `<bos>` and $w_T$ is `<eos>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(A | <bos>) =\t1.000\n",
      "p(A | A) =\t0.615\n",
      "p(B | A) =\t0.231\n",
      "p(<eos> | B) =\t0.500\n",
      "\n",
      "Product: p(AAB) = 0.071\n"
     ]
    }
   ],
   "source": [
    "sequence = ['<bos>', 'A', 'A', 'B', '<eos>']\n",
    "\n",
    "def sequence_p(sequence, log=False):\n",
    "    total_p = 1\n",
    "\n",
    "    for i in range(len(sequence)-n+1):\n",
    "        ngram = tuple(sequence[i:i+n])\n",
    "        prefix = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        p = count[prefix][word] / max(total[prefix], 1)\n",
    "        if log:\n",
    "            print(\"p(%s | %s) =\\t%.3f\" % (word, ', '.join(prefix), p))\n",
    "\n",
    "        total_p *= p\n",
    "    return total_p\n",
    "    \n",
    "\n",
    "print(\"\\nProduct: p(%s) = %.3f\" % (''.join(sequence[1:-1]), sequence_p(sequence, log=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Example: Dialogue Utterances\n",
    "\n",
    "Now lets use the same ideas on a more realistic text corpus.\n",
    "\n",
    "We will use utterances from a dialogue dataset called **Persona-Chat**. This dataset is relatively small and centers on a single domain, but it is simple and interpretable for our purposes here.\n",
    "\n",
    "We'll also use an off-the-shelf ngram modeling package called `KenLM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 133176\n",
      "Number of valid examples: 16181\n",
      "Vocab size: 19153\n",
      "\n",
      "Examples:\n",
      "[['i', 'am', 'doing', 'great', 'except', 'for', 'the', 'allergies', '.'],\n",
      " ['i', 'am', 'a', 'woman', 'what', 'about', 'you', '.'],\n",
      " ['i', 'thought', 'you', 'were', 'a', 'college', 'kid', '.']]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "valid = []\n",
    "import jsonlines\n",
    "\n",
    "for ds, name in [(train, 'train'), (valid, 'valid')]:\n",
    "    for line in jsonlines.Reader(open('data/personachat/personachat_all_sentences_%s.jsonl' % name, 'r')):\n",
    "        ds.append(line['tokens'])\n",
    "        \n",
    "vocab = list(set([t for ts in train for t in ts]))      \n",
    "print(\"Number of train examples: %d\" % (len(train)))\n",
    "print(\"Number of valid examples: %d\" % (len(valid)))\n",
    "print(\"Vocab size: %d\" % (len(vocab)))\n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "pprint(train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KenLM\n",
    "\n",
    "KenLM estimates n-gram language models using **modified Kneser-Ney smoothing**, and has a fast and memory-efficient implementation. \n",
    "- While we won't go into details here, **smoothing** is a technique used to account for ngrams that do not occur in the training corpus. \n",
    "- Normally, these ngrams would receive zero-probability mass. Smoothing ensures every ngram receives some probability.\n",
    "\n",
    "\n",
    "\n",
    "Please see page 48 of the [lecture note](https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf) for an overview of Kneser-Ney smoothing, and [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "KENLM_DIR='/home/jupyter/kenlm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133176/133176 [00:00<00:00, 795531.18it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('data/tokenized'):\n",
    "    os.makedirs('data/tokenized')\n",
    "with open('data/tokenized/pchat_train', 'w') as f:\n",
    "    for line in tqdm(train):\n",
    "        f.write(' '.join(line))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build kenlm n-gram models\n",
    "\n",
    "This uses the `kenlm` commands `lmplz` to estimate the language model, then `build_binary` to convert it to an efficient format. We load the resulting model using the `kenlm` python wrapper.\n",
    "\n",
    "We do this for n-gram models of order `2,3,4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "Unigram tokens 1602042 types 19156\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:229872 2:10928379904\n",
      "Statistics:\n",
      "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
      "2 231267 D1=0.708637 D2=1.06132 D3+=1.37629\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 4551 assuming -p 1.5\n",
      "probing 4626 assuming -r models -p 1.5\n",
      "trie    1747 without quantization\n",
      "trie    1099 assuming -q 8 -b 8 quantization \n",
      "trie    1747 assuming -a 22 array pointer compression\n",
      "trie    1099 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:10874216 kB\tVmRSS:10720 kB\tRSSMax:2938548 kB\tuser:0.848\tsys:1.288\tCPU:2.13913\treal:2.13823\n",
      "Reading models/pchat_2.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "Unigram tokens 1602042 types 19156\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:229872 2:3801175808 3:7127204352\n",
      "Statistics:\n",
      "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
      "2 231267 D1=0.722265 D2=1.09097 D3+=1.44737\n",
      "3 617624 D1=0.798924 D2=1.0703 D3+=1.33013\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 16763 assuming -p 1.5\n",
      "probing 18193 assuming -r models -p 1.5\n",
      "trie     6683 without quantization\n",
      "trie     3625 assuming -q 8 -b 8 quantization \n",
      "trie     6354 assuming -a 22 array pointer compression\n",
      "trie     3296 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272 3:12352480\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272 3:12352480\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:10866020 kB\tVmRSS:22604 kB\tRSSMax:2500264 kB\tuser:1.408\tsys:1.228\tCPU:2.63778\treal:2.52355\n",
      "Reading models/pchat_3.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "Unigram tokens 1602042 types 19156\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:229872 2:1860149760 3:3487780608 4:5580449280\n",
      "Statistics:\n",
      "1 19156 D1=0.588857 D2=1.0348 D3+=1.3388\n",
      "2 231267 D1=0.722265 D2=1.09097 D3+=1.44737\n",
      "3 617624 D1=0.821472 D2=1.14168 D3+=1.40217\n",
      "4 932153 D1=0.863174 D2=1.12888 D3+=1.29333\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 36767 assuming -p 1.5\n",
      "probing 41816 assuming -r models -p 1.5\n",
      "trie    15838 without quantization\n",
      "trie     8356 assuming -q 8 -b 8 quantization \n",
      "trie    14567 assuming -a 22 array pointer compression\n",
      "trie     7085 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272 3:12352480 4:22371672\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:229872 2:3700272 3:12352480 4:22371672\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:10865048 kB\tVmRSS:44524 kB\tRSSMax:2182940 kB\tuser:2.32\tsys:1.384\tCPU:3.7062\treal:3.30219\n",
      "Reading models/pchat_4.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "data_prefix = 'pchat'\n",
    "dataset = 'pchat_train'\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "models = {}\n",
    "for n in [2,3,4]:\n",
    "    model_temp = 'models/%s_%d.arpa' % (data_prefix, n)\n",
    "    model_name = 'models/%s_%d.klm' % (data_prefix, n)\n",
    "    ! cat ./data/tokenized/$dataset | $KENLM_DIR/build/bin/lmplz -o $n > $model_temp\n",
    "    ! $KENLM_DIR/build/bin/build_binary $model_temp $model_name\n",
    "    models[n] = kenlm.LanguageModel(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "Intuitively, a good model should assign high probabilities to sequences from the 'true' distribution that it is modeling.\n",
    "\n",
    "A common way of quantifying this is with **perplexity**, a metric inversely-proportional to the probability that the model assigns to a set of sequences, e.g. a 'test set':\n",
    "\n",
    "\\begin{align}\n",
    "\\large \\text{ppl}(p, D) &\\large\\ = 2^{-\\frac{1}{N_{total}}\\log_2 p(D)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $D=\\{(w_1,\\ldots,w_{N_i})_i\\}_{i=1}^M$ is a dataset of $M$ sequences with total length $N_{\\text{total}}=\\sum_{i}N_i$.\n",
    "\n",
    "Perplexity is defined on $[1,\\infty)$, with 1 being a perfect model (assigning probability 1 to $D$), and a 'worse' model as perplexity increases.\n",
    "\n",
    "Intuitively, _perplexity measures the average rank of the true next-token, when tokens are ordered by the model's conditional probabilities_.\n",
    "\n",
    "\n",
    "<!-- Section 1.3 of [[Chen & Goodman 1998]](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1) has a concise summary of perplexity and its motivation. !-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Perplexity\n",
    "\n",
    "`kenlm` outputs log probabilities in **log base 10**. For the standard definition of perplexity we need **log base 2**. See the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_kenlm(model, sequences):\n",
    "    n_total = 0\n",
    "    logp_total = 0\n",
    "    for sequence in sequences:\n",
    "        text = ' '.join(sequence)\n",
    "        logp_total += model.score(text)\n",
    "        n_total += len(sequence) + 1  # add 1 for </s>\n",
    "        \n",
    "    # Convert log10 to log2\n",
    "    log2p_total = logp_total / np.log10(2)\n",
    "    ppl = 2 ** (- (1.0 / n_total) * log2p_total)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >n</th>        <th class=\"col_heading level0 col1\" >ppl</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "                        <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row0_col1\" class=\"data row0 col1\" >39.9248</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "                        <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row1_col1\" class=\"data row1 col1\" >15.462</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row2_col0\" class=\"data row2 col0\" >4</td>\n",
       "                        <td id=\"T_b293d362_69ec_11ea_9a4f_490ad3ff38a1row2_col1\" class=\"data row2 col1\" >8.91961</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4bbc9f9190>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Train ===\")\n",
    "df = pd.DataFrame([(n, perplexity_kenlm(models[n], train)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
    "df.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Valid ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >n</th>        <th class=\"col_heading level0 col1\" >ppl</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "                        <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row0_col1\" class=\"data row0 col1\" >59.188</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "                        <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row1_col1\" class=\"data row1 col1\" >44.9771</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row2_col0\" class=\"data row2 col0\" >4</td>\n",
       "                        <td id=\"T_b2ac47da_69ec_11ea_9a4f_490ad3ff38a1row2_col1\" class=\"data row2 col1\" >43.2188</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4bac0852d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Valid ===\")\n",
    "df = pd.DataFrame([(n, perplexity_kenlm(models[n], valid)) for n in [2, 3, 4]], columns=['n', 'ppl'])\n",
    "df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence probabilities:\n",
    "\\begin{align}\n",
    "p(w_1,\\ldots,w_T)&\\approx\\prod_{t=1}^T p(w_t|w_{t-2},w_{t-1})\\\\\n",
    "&=\\sum_{t=1}^T \\log p(w_t|w_{t-2},w_{t-1}).\n",
    "\\end{align}\n",
    "\n",
    "where we use log probabilities in practice to avoid a product of many small numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like my pet dog .\n",
      "n: 2\t logp: -29.216\n",
      "n: 3\t logp: -28.843\n",
      "n: 4\t logp: -29.219\n",
      "\n",
      "i like my pet zebra .\n",
      "n: 2\t logp: -32.303\n",
      "n: 3\t logp: -31.157\n",
      "n: 4\t logp: -32.101\n",
      "\n",
      "i like my pet lion .\n",
      "n: 2\t logp: -38.352\n",
      "n: 3\t logp: -41.017\n",
      "n: 4\t logp: -42.320\n",
      "\n",
      "i live in the united states .\n",
      "n: 2\t logp: -22.599\n",
      "n: 3\t logp: -20.095\n",
      "n: 4\t logp: -17.854\n",
      "\n",
      "i live in the united states of america .\n",
      "n: 2\t logp: -40.958\n",
      "n: 3\t logp: -26.627\n",
      "n: 4\t logp: -24.311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'i like my pet dog .',\n",
    "    'i like my pet zebra .',\n",
    "    'i like my pet lion .',\n",
    "    'i live in the united states .',\n",
    "    'i live in the united states of america .'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    for n in [2, 3, 4]:\n",
    "        log10p = models[n].score(sentence)\n",
    "        log2p = log10p / np.log10(2)\n",
    "        print(\"n: %d\\t logp: %.3f\" % (n, log2p))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
