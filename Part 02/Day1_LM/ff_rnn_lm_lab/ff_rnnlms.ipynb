{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxZ97MQPbj90"
   },
   "source": [
    "# Feed-forward and Recurrent Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jsonlines tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RSqJllXkKGZR",
    "outputId": "e3ec5aa0-bbf9-4851-aad1-6c1ab1fdf4f1"
   },
   "outputs": [],
   "source": [
    "# download pretrained models and data\n",
    "### DOWNLOADING THE FILES\n",
    "import os\n",
    "\n",
    "### persona chat dataset\n",
    "if not os.path.exists('personachat_all_sentences_train.jsonl'):\n",
    "    !wget \"https://nyu.box.com/shared/static/q4nvswb0szelivhgyx87vd1056ttqfyi.jsonl\" -O 'personachat_all_sentences_train.jsonl'\n",
    "if not os.path.exists('personachat_all_sentences_valid.jsonl'):\n",
    "    !wget \"https://nyu.box.com/shared/static/8krcizo8sms1m0ppy7uiwfcx4a3l5nsq.jsonl\" -O 'personachat_all_sentences_valid.jsonl'\n",
    "if not os.path.exists('personachat_all_sentences_test.jsonl'):\n",
    "    !wget \"https://nyu.box.com/shared/static/im7we9k2gcf8kslqnfamsimicgosuw9y.jsonl\" -O 'personachat_all_sentences_test.jsonl'\n",
    "\n",
    "## wikitext-2 dataset\n",
    "if not os.path.exists('wikitext2-sentencized.json'):\n",
    "    !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O 'wikitext2-sentencized.json'\n",
    "\n",
    "### pretrained ffnn model\n",
    "if not os.path.exists('personachat_ffnn_lm.pt'):\n",
    "    !wget \"https://nyu.box.com/shared/static/looczvct7ssh30x0gwqmvencple8xok9.pt\" -O 'personachat_ffnn_lm.pt'\n",
    "\n",
    "### pretrained rnn model\n",
    "if not os.path.exists('personachat_rnn_lm.pt'):\n",
    "    !wget \"https://nyu.box.com/shared/static/3jl9erctnvbefnczgmetkdil0xwpcxzb.pt\" -O 'personachat_rnn_lm.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0PkAzAkKHZw"
   },
   "source": [
    "## N-gram modeling as classification\n",
    "\n",
    "In the previous n-gram language models lab, the probability of a word given a history ('prefix') was computed using n-gram **counts**.\n",
    "\n",
    "In this lab, our goal will be the same: modeling the probability of variable length sequences, $$p(x_1,\\ldots,x_T)=\\prod_{t=1}^T p(x_t|x_{<t}),$$\n",
    "\n",
    "by assuming that each word $x_t$ only depends on the preceding $n-1$ words, $$p(x_1,\\ldots,x_T)=\\prod_{t=1}^T p(x_t|x_{t-n+1},\\ldots,x_{t-1}).$$\n",
    "\n",
    "\n",
    "However, _instead of counts_, we will now use a **neural network** to **classify** each history, where each class is the next word.\n",
    "\n",
    "<img src=\"img/fflm.png\" alt=\"Drawing\" style=\"width: 35%;\"/>\n",
    "\n",
    "[Diagram: modified from the [NLP Lecture Note](https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf) by Kyunghyun Cho]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rHGuy-zbj-R"
   },
   "source": [
    "## Modeling chat utterances\n",
    "\n",
    "We will build a **feed-forward neural n-gram model** to model chat utterances.\n",
    "\n",
    "### 1. Prepare the data\n",
    "\n",
    "To load and preprocess the data, we'll use utility functions from `data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsLd-pAlbj-Y"
   },
   "outputs": [],
   "source": [
    "import data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYIBNnOwO9wa"
   },
   "outputs": [],
   "source": [
    "datasets = data_utils.load_personachat('./')\n",
    "dictionary = data_utils.Dictionary(datasets, include_valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example from the dataset (note that in addition to `train` we also have `valid`, `test` splits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "split = 'train'\n",
    "encoded = dictionary.encode_token_seq(datasets[split][index])\n",
    "decoded = dictionary.decode_idx_seq(encoded)\n",
    "\n",
    "print(\"Dataset size %d (train), %d (valid), %d (test).\\n\" %\n",
    "     (len(datasets['train']), \n",
    "      len(datasets['valid']), \n",
    "      len(datasets['test'])))\n",
    "print(' '.join(datasets[split][index]))\n",
    "print(f'\\n word indices - {encoded}')\n",
    "print(f'\\n words - {decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add `<bos>`, `<eos>`, and turn each **sequence** into **n-grams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_order = 5\n",
    "tokenized_datasets = data_utils.tokenize_dataset(\n",
    "    datasets, dictionary, ngram_order\n",
    ")\n",
    "ngram_datasets = data_utils.slice_into_ngrams(\n",
    "    tokenized_datasets, ngram_order\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    decoded = dictionary.decode_idx_seq(ngram_datasets[split][index+i])\n",
    "    print(f'\\n {i} - {decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a **Pytorch** `Dataset` and `DataLoader` that can be used for training a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "401zpVGvNcmR",
    "outputId": "eec5cc33-1daa-45d4-e457-f13f73e74a05"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "datasets = {}\n",
    "dataloaders = {}\n",
    "for split, dataset in ngram_datasets.items():\n",
    "    dataset_ = data_utils.NgramDataset(dataset)\n",
    "    datasets[split] = dataset_\n",
    "    dataloaders[split] = DataLoader(\n",
    "        dataset_, \n",
    "        batch_size=2048, \n",
    "        shuffle=(split == 'train'), \n",
    "        collate_fn=data_utils.batchify\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining the model\n",
    "\n",
    "\n",
    "#### How do we represent the input $x_t$? \n",
    "\n",
    "Each $x_t$ is an **integer** (token id), and we transform it into a **vector** $f_{\\text{embed}}(x_t)\\in\\mathbb{R}^d$ using an `Embedding` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlUBUtnhbj-f"
   },
   "source": [
    "An embedding layer serves as a differentiable look-up table. Given an integer token id, it returns a vector representation which we call an **embedding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7wBvXaYbj-g"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Embedding\n",
    "# Embedding??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPUY40tXbj-m"
   },
   "outputs": [],
   "source": [
    "lookup = Embedding(\n",
    "    num_embeddings=len(dictionary), \n",
    "    embedding_dim=16, \n",
    "    padding_idx=dictionary.get_id('<pad>')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eSDJy98Jbj-o",
    "outputId": "3e1b3726-d1ae-439e-8a2b-432257b52651"
   },
   "outputs": [],
   "source": [
    "lookup.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3WMCuASKbj-s",
    "outputId": "91c5996e-fd32-4f4b-841d-9ac5c7a8716c"
   },
   "outputs": [],
   "source": [
    "input_ = dictionary.encode_token_seq(['hello', 'world'])\n",
    "print(f'token ids: {[input_]}')\n",
    "\n",
    "input_embedded = lookup(torch.tensor([input_], dtype=torch.long))\n",
    "print(f'tokens embedding size: {input_embedded.size()}')\n",
    "\n",
    "print(\"embedding for 'hello' (179):\")\n",
    "input_embedded[0, 0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the neural network\n",
    "\n",
    "There are three components:\n",
    "1. `Embedding` $$\\large f_{embed}:\\text{vocab}\\rightarrow \\mathbb{R}^d$$\n",
    "2. Hidden layer $$\\large f_{hidden}: \\mathbb{R}^{nd}\\rightarrow \\mathbb{R}^h$$\n",
    "3. Projection $$\\large f_{project}: \\mathbb{R}^{h}\\rightarrow \\mathbb{R}^{|vocab|}$$\n",
    "\n",
    "The conditional distribution is then: $$\\large p(x_t|x_{t-1},\\ldots,x_{t-n-1})=\\text{softmax}(f_{project}(f_{hidden}(f_{embed}(x_{t-1},\\ldots,x_{t-n-1}))).$$\n",
    "\n",
    "\n",
    "We implement the hidden layer and projection with Pytorch `nn.Linear` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvlP70FiOrJ3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNgramLM(nn.Module):\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        self.lookup = nn.Embedding(\n",
    "            num_embeddings=options['num_embeddings'], \n",
    "            embedding_dim=options['embedding_dim']\n",
    "        )\n",
    "        self.hidden_layer = nn.Linear(\n",
    "            options['input_size'], \n",
    "            options['hidden_size'], \n",
    "            bias=True\n",
    "        )\n",
    "        self.projection = nn.Linear(\n",
    "            options['hidden_size'], \n",
    "            options['num_embeddings']\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        embedded = self.lookup(inp)\n",
    "        embedded_timeflat = embedded.view(embedded.size(0), -1)\n",
    "        hidden_out = self.hidden_layer(embedded_timeflat)\n",
    "        hidden_out = torch.tanh(hidden_out)\n",
    "        logits = self.projection(hidden_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the model\n",
    "\n",
    "#### Initialize the model, criterion (loss), and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfjWvd6jTDjh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "load_pretrained = True\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "if load_pretrained:\n",
    "    if not os.path.exists('personachat_ffnn_lm.pt'):\n",
    "        raise EOFError('No model downloaded!')\n",
    "    model_dict = torch.load(\n",
    "        'personachat_ffnn_lm.pt',\n",
    "        map_location=torch.device(current_device)\n",
    "    )\n",
    "    \n",
    "    options = model_dict['options']\n",
    "    model = NeuralNgramLM(options).to(current_device)\n",
    "    model.load_state_dict(\n",
    "        model_dict['model_dict']\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    embedding_size = 256\n",
    "    ngram_order = 4\n",
    "    input_size = embedding_size * ngram_order\n",
    "    hidden_size = 512\n",
    "\n",
    "    options = {\n",
    "        'num_embeddings': len(persona_dict),\n",
    "        'embedding_dim': embedding_size,\n",
    "        'input_size': input_size,\n",
    "        'hidden_size': hidden_size,\n",
    "    }\n",
    "    model = NeuralNgramLM(options).to(current_device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=dictionary.get_id('<pad>'), \n",
    "    reduction='sum'\n",
    ")\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(model_parameters, lr=0.01, momentum=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "dHeMz2Q2ULYQ",
    "outputId": "be100b2c-b6bd-454d-d86b-9080e25977cb"
   },
   "outputs": [],
   "source": [
    "plot_cache = []\n",
    "\n",
    "if not load_pretrained:\n",
    "    for epoch_number in range(10):\n",
    "        avg_loss = -1\n",
    "\n",
    "        # -- Training\n",
    "        model.train()\n",
    "\n",
    "        train_loss_cache = 0\n",
    "        train_non_pad_tokens_cache = 0\n",
    "\n",
    "        # == Iterate through batches from the dataset.\n",
    "        for i, (inp, target) in enumerate(dataloaders['train']):\n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "\n",
    "            # == Forward pass.\n",
    "            logits = model(inp)\n",
    "\n",
    "            # == Computed (summed) loss.\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            train_loss_cache += loss.item()\n",
    "\n",
    "            # == Compute the number of non `<pad>` tokens in the target.\n",
    "            non_pad_tokens = target.view(-1).ne(dictionary.get_id('<pad>')).sum().item()\n",
    "\n",
    "            # == Normalize the (summed) loss so that it is an average.\n",
    "            loss /= non_pad_tokens  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_non_pad_tokens_cache += non_pad_tokens\n",
    "            if i % 100 == 0:\n",
    "                avg_loss = train_loss_cache/train_non_pad_tokens_cache\n",
    "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
    "\n",
    "        # -- Validation\n",
    "        valid_loss_cache = 0\n",
    "        valid_non_pad_tokens_cache = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            non_pad_tokens = 0\n",
    "            for i, (inp, target) in enumerate(dataloaders['valid']):\n",
    "                inp = inp.to(current_device)\n",
    "                target = target.to(current_device)\n",
    "\n",
    "                # == Forward pass and loss\n",
    "                logits = model(inp)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "                valid_loss_cache += loss.item()            \n",
    "\n",
    "                # == Normalize the loss\n",
    "                non_pad_tokens = target.view(-1).ne(dictionary.get_id('<pad>')).sum().item()\n",
    "                valid_non_pad_tokens_cache += non_pad_tokens\n",
    "\n",
    "            avg_val_loss = valid_loss_cache / valid_non_pad_tokens_cache\n",
    "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
    "\n",
    "        plot_cache.append((avg_loss, avg_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEjiLvdFmjrg"
   },
   "outputs": [],
   "source": [
    "if load_pretrained:\n",
    "    plot_cache = model_dict['loss_cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "C0nNouO4Z-TJ",
    "outputId": "1f9b3a4f-b801-4d7a-dc38-70c1baa488f1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "epochs = numpy.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [i[0] for i in plot_cache], label='Train loss')\n",
    "plt.plot(epochs, [i[1] for i in plot_cache], label='Valid loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Loss curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTyb2zZYbj_O"
   },
   "source": [
    "### 4. Evaluate the model\n",
    "\n",
    "#### Recap: Perplexity\n",
    "Recall **perplexity** from the preceding lab:\n",
    "\n",
    "\\begin{align}\n",
    "\\large \\text{ppl}(p, D) &\\large = 2^{-\\frac{1}{N_{total}}\\log_2 p(D)}\n",
    "\\end{align}\n",
    "\n",
    "where $D=\\{(w_1,\\ldots,w_{N_i})_i\\}_{i=1}^M$ is a dataset of $M$ sequences with total length $N_{\\text{total}}=\\sum_{i}N_i$.\n",
    "\n",
    "#### Computing perplexity\n",
    "\n",
    "Our model's loss is the (negative) log probability of every token in the output sequence, which can be used to compute perplexity.\n",
    "\n",
    "We define perplexity using the **base 2** logarithm. By default `torch.log` uses base `e`. \n",
    "\n",
    "To **convert to base 2** we use: `log_2 p = log_e p / log_e 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "dcuSFVZvbmWR",
    "outputId": "7441ed5d-59fc-44b3-96fb-726cfcec92cc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "epochs = numpy.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in plot_cache], label='Train ppl')\n",
    "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in plot_cache], label='Valid ppl')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PPL curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9t_WHUYcOe_8"
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "if False:\n",
    "    torch.save({\n",
    "        'options': options,\n",
    "        'loss_cache': plot_cache,\n",
    "        'model_dict': model.state_dict()\n",
    "    }, './persona_ffnn_lm.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional probability queries\n",
    "We can now query the model for conditional probabilities:\n",
    "\n",
    "\\begin{align}\n",
    "\\large p(x_t|x_{t-1},\\ldots,x_{t-4}) \n",
    "\\end{align} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def next_token_probabilities(prefix, token):\n",
    "    inp = dictionary.encode_token_seq(prefix.split(' '))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(\n",
    "            torch.tensor([inp], dtype=torch.long).to(current_device)\n",
    "        )\n",
    "    token_id = dictionary.get_id(token)\n",
    "    p_next_token = torch.softmax(logits, dim=-1)\n",
    "    return p_next_token[0, token_id]\n",
    "\n",
    "prefix = '<bos> i have a'\n",
    "\n",
    "for w in ['dog', 'zebra', 'donut', 'velociraptor']:\n",
    "    prob = next_token_probabilities(prefix, w)\n",
    "    print('p( %s  | %s )\\t= %.3E' % (w, prefix, prob))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "And sort by the highest probability next-tokens:\n",
    "\n",
    "$\\large \\texttt{sort}(p(\\cdot|\\underbrace{x_{<t}}_{\\text{prefix}}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "l4zVv--owi3d",
    "outputId": "2c64752d-d913-4dfa-814d-ba7042982462"
   },
   "outputs": [],
   "source": [
    "def get_top_token_given_prefix(prefix, top=10):\n",
    "    inp = dictionary.encode_token_seq(prefix.split(' '))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(\n",
    "            torch.tensor([inp], dtype=torch.long).to(current_device)\n",
    "        )\n",
    "        \n",
    "    p_next_token = torch.softmax(logits, dim=-1)\n",
    "    sorted_probs, sorted_token_ids = torch.topk(p_next_token[0], top)\n",
    "\n",
    "    for i in range(sorted_token_ids.size(0)):\n",
    "        print('p( %s  | %s )\\t= %.5f' % \n",
    "              (dictionary.decode_idx_seq([sorted_token_ids[i]])[0], \n",
    "               prefix, \n",
    "               sorted_probs[i])\n",
    "        )\n",
    "\n",
    "get_top_token_given_prefix('<bos> i have a')  # '<bos> the weather is'  '<bos> <bos> <bos> hello'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eb9Va-p4bj96"
   },
   "source": [
    "# Recurrent neural network language model\n",
    "Recall that in **language modeling**, we want to model the probability of variable length sequences, $$p(x_1,\\ldots,x_T)=\\prod_{t=1}^T p(x_t|x_{<t}).$$\n",
    "\n",
    "*Unlike n-gram models*, we will now define a model that allows a **full history** $x_{<t}$.\n",
    "\n",
    "To do so, we will use a function called a **recurrent neural network** to compute each distribution $p(x_t|x_{<t})$:\n",
    "\\begin{align}\n",
    "h_t &= f^{\\text{RNN}}_{\\theta}(x_{t-1}, h_{t-1}) & \\mathbb{R}^d\\\\\n",
    "s_t &= Wh_t & \\mathbb{R}^{\\text{|vocab|}}\\\\\n",
    "p_{\\theta}(x_t|x_{<t}) &= \\text{softmax}(s_t) & [0,1]^{\\text{|vocab|}}\n",
    "\\end{align}\n",
    "\n",
    "where $h_t$ is an internal (hidden) state of the model at time step $t$, and $x_t$ is an input at time step $t$.\n",
    "\n",
    "<img src=\"img/rnn.png\" alt=\"Drawing\" style=\"width: 35%; margin-left: 29%; padding: 3em;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the simplest form used in practice, $f_{\\theta}^{\\text{RNN}}$ is defined as:\n",
    " $$f_{\\theta}^{\\text{RNN}}(x_{t-1}, h_{t-1}) = \\text{tanh}(a_t)\\\\ a_t = b + W \\cdot h_{t-1} + U \\cdot x_{t-1},$$\n",
    " \n",
    "but there are other variants (e.g. [GRU](https://arxiv.org/pdf/1406.1078.pdf), [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf)).\n",
    "\n",
    "Similar to the n-gram neural network above, the goal at time $t$ is to **predict the next token $x_t$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLTwdNF-bj97"
   },
   "source": [
    "### Pytorch implementation: `RNNCell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1WfgVLybj98"
   },
   "outputs": [],
   "source": [
    "from torch.nn import RNNCell\n",
    "# RNNCell??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OseW45d9bj9_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "hidden_size = 16\n",
    "embedding_dim = 8\n",
    "\n",
    "rnn_cell = RNNCell(embedding_dim, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "UnPEPW5Nbj-B",
    "outputId": "4dbe89a7-989f-4db7-e89e-bd2d0d0fe13d"
   },
   "outputs": [],
   "source": [
    "hidden = torch.zeros(1, hidden_size)\n",
    "print(hidden)\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "9TIjbLsMbj-C",
    "outputId": "a3dd9a6a-fa65-4ff6-b499-d5acc650a94a"
   },
   "outputs": [],
   "source": [
    "random_input = torch.rand(1, embedding_dim)\n",
    "print(random_input)\n",
    "print(random_input.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "1n2k_CEBbj-E",
    "outputId": "3a521311-9bca-404d-8928-958f369c1129"
   },
   "outputs": [],
   "source": [
    "hidden = rnn_cell(random_input, hidden)\n",
    "print(hidden)\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4QaNlS2bj-O"
   },
   "source": [
    "### Pytorch implementation: `RNN`\n",
    "\n",
    "`RNNCell` is a building block of an RNN model. The Pytorch `RNN` model supports multi-layer networks by creating separate `RNNCell`s for every layer. \n",
    "\n",
    "`RNN` can also process a whole sequence of data by iteratively applying `RNNCell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCvackw4bj-O"
   },
   "outputs": [],
   "source": [
    "from torch.nn import RNNBase, RNN\n",
    "# RNNBase.__init__??\n",
    "# RNN.__init__??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHqYjtCYbj--"
   },
   "source": [
    "### 1. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdlUbyiybj_A"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lookup = nn.Embedding(\n",
    "            num_embeddings=options['num_embeddings'], \n",
    "            embedding_dim=options['embedding_dim'], \n",
    "            padding_idx=options['padding_idx']\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            options['input_size'], \n",
    "            options['hidden_size'], \n",
    "            options['num_layers'], \n",
    "            dropout=options['rnn_dropout'], \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.projection = nn.Linear(\n",
    "            options['hidden_size'], \n",
    "            options['num_embeddings']\n",
    "        )\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        embeddings = self.lookup(token_ids)\n",
    "        output, hidden = self.rnn(embeddings)\n",
    "        logits = self.projection(output) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and Dataloader\n",
    "\n",
    "See `data_utils.py` for the implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_ = data_utils.load_personachat('./')\n",
    "dictionary = data_utils.Dictionary(datasets_, include_valid=True)\n",
    "datasets_ = data_utils.tokenize_dataset(datasets_, dictionary)\n",
    "\n",
    "datasets = {}\n",
    "for split, dataset in datasets_.items():\n",
    "    datasets[split] = data_utils.TensoredDataset(dataset, dictionary.get_id('<pad>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **input** is now a **full sequence**.\n",
    "- The **output** is the **next token** for each position of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, out = datasets['train'][0]\n",
    "\n",
    "print(\"Input: \\n\\t%s\\n\\t(%s)\" %\n",
    "      (inp.data[0], dictionary.decode_idx_seq(inp.data[0])))\n",
    "\n",
    "print(\"Output: \\n\\t%s\\n\\t(%s)\" %\n",
    "      (out.data[0], dictionary.decode_idx_seq(out.data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "batch_size = 128\n",
    "\n",
    "for split, dataset in datasets.items():\n",
    "    dataloaders[split] = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=dataset.pad_collate_fn  # pads the sequences in the batch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding to handle variable sequence lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target = next(dataloaders['train'].__iter__())\n",
    "\n",
    "print(\"Batch size %s\" % str(inp.shape))\n",
    "print(\"Batch element 1: \\n\\t%s\\n\\t(%s)\" %\n",
    "      (inp.data[0], ' '.join(dictionary.decode_idx_seq(inp.data[0]))))\n",
    "print(\"Batch element 2: \\n\\t%s\\n\\t(%s)\" %\n",
    "      (inp.data[1], ' '.join(dictionary.decode_idx_seq(inp.data[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the model\n",
    "\n",
    "#### Initializing model, criterion, and optimizer (similar to the feedforward model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQB-RbIhbj_B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "load_pretrained = True\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "if load_pretrained:\n",
    "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
    "        raise EOFError('Download pretrained model!')\n",
    "    model_dict = torch.load(\n",
    "        'personachat_rnn_lm.pt',\n",
    "        map_location=torch.device(current_device)\n",
    "    )\n",
    "    \n",
    "    options = model_dict['options']\n",
    "    model = RNNLanguageModel(options).to(current_device)\n",
    "    model.load_state_dict(model_dict['model_dict'])\n",
    "    \n",
    "else:\n",
    "    embedding_size = 256\n",
    "    hidden_size = 512\n",
    "    num_layers = 3\n",
    "    rnn_dropout = 0.3\n",
    "\n",
    "    options = {\n",
    "        'num_embeddings': len(dictionary),\n",
    "        'embedding_dim': embedding_size,\n",
    "        'padding_idx': dictionary.get_id('<pad>'),\n",
    "        'input_size': embedding_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'rnn_dropout': rnn_dropout,\n",
    "    }\n",
    "    model = RNNLanguageModel(options).to(current_device)\n",
    "    \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=dictionary.get_id('<pad>'), \n",
    "    reduction='sum'\n",
    ")\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(model_parameters, lr=0.001, momentum=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "qmMLIrhjbj_D",
    "outputId": "88d88e38-f723-4946-dde9-0c8a8834e746"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwBqO3z7bj_L"
   },
   "source": [
    "#### Training loop (same as the feedforward model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fw28_6qXbj_M",
    "outputId": "bbfb150f-55d0-4d46-c715-7430d7204f17"
   },
   "outputs": [],
   "source": [
    "plot_cache = []\n",
    "\n",
    "if not load_pretrained:\n",
    "    for epoch_number in range(100):\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        train_loss_cache = 0\n",
    "        train_non_pad_tokens_cache = 0\n",
    "\n",
    "        for i, (inp, target) in enumerate(persona_loaders['train']):\n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            train_loss_cache += loss.item()  # still sum here\n",
    "\n",
    "            non_pad_tokens = target.view(-1).ne(persona_dict.get_id('<pad>')).sum().item()            \n",
    "            train_non_pad_tokens_cache += non_pad_tokens\n",
    "\n",
    "            loss /= non_pad_tokens \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                avg_loss = train_loss_cache / train_non_pad_tokens_cache\n",
    "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
    "                train_log_cache = []\n",
    "\n",
    "        valid_loss_cache = 0\n",
    "        valid_non_pad_tokens_cache = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inp, target) in enumerate(persona_loaders['valid']):\n",
    "                inp = inp.to(current_device)\n",
    "                target = target.to(current_device)\n",
    "                logits = model(inp)\n",
    "\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "                valid_loss_cache += loss.item()  # still sum here\n",
    "                non_pad_tokens = target.view(-1).ne(persona_dict.get_id('<pad>')).sum().item()\n",
    "\n",
    "                valid_non_pad_tokens_cache += non_pad_tokens\n",
    "\n",
    "            avg_val_loss = valid_loss_cache / valid_non_pad_tokens_cache\n",
    "\n",
    "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
    "\n",
    "        plot_cache.append((avg_loss, avg_val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAV_hzMvbj_O"
   },
   "outputs": [],
   "source": [
    "if load_pretrained:\n",
    "    plot_cache = model_dict['loss_cache']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "fhTtmqBfufyE",
    "outputId": "41019bb9-79c9-440f-a6d7-ecc5817769db"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "epochs = numpy.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [i[0] for i in plot_cache], label='Train loss')\n",
    "plt.plot(epochs, [i[1] for i in plot_cache], label='Valid loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Loss curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "R_A4w1QFufyN",
    "outputId": "23fe62f5-96e3-4893-b4b0-72cd63d4ccd6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "epochs = numpy.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in plot_cache], label='Train ppl')\n",
    "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in plot_cache], label='Valid ppl')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PPL curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAvP_FXgbj_W"
   },
   "source": [
    "### Conditional probability queries\n",
    "\n",
    "We can query different probabilities:\n",
    "\n",
    "$\\large p(\\underbrace{x_t}_{\\text{token}}|\\underbrace{x_{<t}}_{\\text{prefix}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "TMG0r9zwbj_X",
    "outputId": "95a284e7-3913-46ca-e43f-3505dacb6555"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def next_token_probabilities(prefix, token):\n",
    "    inp = dictionary.encode_token_seq(prefix.split(' '))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(\n",
    "            torch.tensor([inp], dtype=torch.long).to(current_device)\n",
    "        )\n",
    "        \n",
    "    token_id = dictionary.get_id(token)\n",
    "    p_next_token = torch.softmax(logits[0, -1], dim=-1)\n",
    "    return p_next_token[token_id]\n",
    "\n",
    "prefix = '<bos> i have a'\n",
    "\n",
    "for w in ['dog', 'zebra', 'donut', 'velociraptor']:\n",
    "    prob = next_token_probabilities(prefix, w)\n",
    "    print('p( %s  | %s )\\t= %.3E' % (w, prefix, prob))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "And sort by the highest probability next-tokens:\n",
    "\n",
    "$\\large \\texttt{sort}(p(\\cdot|\\underbrace{x_{<t}}_{\\text{prefix}}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "l4zVv--owi3d",
    "outputId": "2c64752d-d913-4dfa-814d-ba7042982462"
   },
   "outputs": [],
   "source": [
    "def get_top_token_given_prefix(prefix, top=10):\n",
    "    inp = dictionary.encode_token_seq(prefix.split(' '))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(\n",
    "            torch.tensor([inp], dtype=torch.long).to(current_device)\n",
    "        )\n",
    "        \n",
    "    p_next_token = torch.softmax(logits[0, -1], dim=-1)\n",
    "    sorted_probs, sorted_token_ids = torch.topk(p_next_token, top)\n",
    "\n",
    "    for i in range(sorted_token_ids.size(0)):\n",
    "        print('p( %s  | %s )\\t= %.5f' % \n",
    "              (dictionary.decode_idx_seq([sorted_token_ids[i]])[0], \n",
    "               prefix, \n",
    "               sorted_probs[i])\n",
    "        )\n",
    "\n",
    "get_top_token_given_prefix('<bos> the weather is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (Sampling)\n",
    "\n",
    "Finally, we can generate sentences using the model's conditional distribution:\n",
    "\n",
    "            context = [<bos>]\n",
    "            until <eos> is generated:\n",
    "                wt ~ p(wt | context)\n",
    "                context += [wt]\n",
    "                \n",
    "                \n",
    "Here, the `~` symbol stands for sampling from a categorical distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sample(model, prefix, num_samples, temperature=1.0, max_len=50):\n",
    "    inp = torch.tensor([\n",
    "        dictionary.encode_token_seq(prefix.split(' '))\n",
    "    ], dtype=torch.long).to(current_device).repeat([num_samples, 1])\n",
    "\n",
    "    sampled = [prefix.split(' ') for _ in range(num_samples)]\n",
    "    done = torch.tensor([False for _ in range(num_samples)])\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for t in range(max_len):\n",
    "            logits, hidden = step(model, inp, hidden)\n",
    "            logits_ = (logits[:, -1, :] / temperature)\n",
    "            inp = logits_.softmax(-1).multinomial(1)            \n",
    "            for i in range(num_samples):\n",
    "                tok_ti = dictionary.get_token(inp[i, 0].item())\n",
    "                if tok_ti == '<eos>' and not done[i]:\n",
    "                    done[i] = True\n",
    "                    sampled[i].append(tok_ti)\n",
    "                elif not done[i]:\n",
    "                    sampled[i].append(tok_ti)\n",
    "                else:\n",
    "                    pass\n",
    "            if done.sum() == num_samples:\n",
    "                break\n",
    "        \n",
    "    return sampled\n",
    "\n",
    "def step(rnn, token_ids, hidden):\n",
    "    embeddings = rnn.lookup(token_ids)\n",
    "    output, hidden = rnn.rnn(embeddings, hidden)\n",
    "    logits = rnn.projection(output) \n",
    "    return logits, hidden\n",
    "\n",
    "\n",
    "prefix = '<bos> i have a'\n",
    "\n",
    "for temp in [1.0, 0.5, 0.1]:\n",
    "    print(\"Temperature %.2f\" % temp)\n",
    "    samples = sample(model, prefix, 5, temperature=temp)\n",
    "    for s in samples:\n",
    "        print(' '.join(s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzGqlrKhbj_g"
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "if False:\n",
    "    torch.save({\n",
    "        'options': options,\n",
    "        'loss_cache': plot_cache,\n",
    "        'model_dict': model.state_dict()\n",
    "    }, './static_files/persona_rnn_lm.pt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large-scale Language Modeling: GPT-2\n",
    "\n",
    "The language model above is relatively small in terms of the number of parameters, and is trained on a (very) small dataset.\n",
    "\n",
    "Recently, researchers have trained **large-scale language models** on **large amounts of data**, resulting in improved language modeling and generation ability. \n",
    "\n",
    "A representative example is **GPT-2** [[paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)], [[blog](https://openai.com/blog/better-language-models/)]. We won't currently discuss the details of this model, but we can easily **use a pre-trained version to generate text**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "\n",
    "device = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda:0')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate (Sampling)\n",
    "\n",
    "We'll talk about other ways to generate later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'My favorite food is'\n",
    "\n",
    "max_length = 100\n",
    "num_samples = 5\n",
    "\n",
    "inp = torch.tensor([tokenizer.encode(prefix)], device=device).repeat(num_samples, 1)\n",
    "\n",
    "out = model.generate(\n",
    "    input_ids=inp,\n",
    "    do_sample=True,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = tokenizer.decode(out[i])\n",
    "    print(sample)\n",
    "    print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab4_rnnlm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
