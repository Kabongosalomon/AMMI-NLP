{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIRSEQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://fairseq.readthedocs.io/en/latest/getting_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks.\" It provides reference implementations of various sequence-to-sequence models making our life much more easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: fairseq in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from fairseq) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: cython in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from fairseq) (0.29.15)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from fairseq) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: regex in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from fairseq) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: sacrebleu in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from fairseq) (1.4.4)\n",
      "Requirement already satisfied, skipping upgrade: torch in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from fairseq) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from fairseq) (4.37.0)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cffi->fairseq) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: portalocker in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacrebleu->fairseq) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: typing in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacrebleu->fairseq) (3.7.4.1)\n",
      "Requirement already satisfied: sacremoses in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (0.0.35)\n",
      "Requirement already satisfied: subword_nmt in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (0.3.7)\n",
      "Requirement already satisfied: click in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacremoses) (7.0)\n",
      "Requirement already satisfied: tqdm in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacremoses) (4.37.0)\n",
      "Requirement already satisfied: joblib in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacremoses) (0.14.0)\n",
      "Requirement already satisfied: six in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacremoses) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade fairseq\n",
    "!pip install sacremoses subword_nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading some data and required scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning Moses github repository (for tokenization scripts)...\n",
      "Cloning into 'mosesdecoder'...\n",
      "remote: Enumerating objects: 30, done.\u001b[K\n",
      "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
      "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
      "remote: Total 147544 (delta 12), reused 15 (delta 5), pack-reused 147514\u001b[K\n",
      "Receiving objects: 100% (147544/147544), 129.75 MiB | 1.91 MiB/s, done.\n",
      "Resolving deltas: 100% (113998/113998), done.\n",
      "Checking out files: 100% (3467/3467), done.\n",
      "Cloning Subword NMT repository (for BPE pre-processing)...\n",
      "Cloning into 'subword-nmt'...\n",
      "remote: Enumerating objects: 42, done.\u001b[K\n",
      "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
      "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
      "remote: Total 551 (delta 14), reused 23 (delta 7), pack-reused 509\u001b[K\n",
      "Receiving objects: 100% (551/551), 328.51 KiB | 1001.00 KiB/s, done.\n",
      "Resolving deltas: 100% (320/320), done.\n",
      "--2020-03-24 11:33:47--  http://statmt.org/wmt14/training-parallel-nc-v9.tgz\n",
      "Resolving statmt.org (statmt.org)... 129.215.197.184\n",
      "Connecting to statmt.org (statmt.org)|129.215.197.184|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 80418416 (77M) [application/x-gzip]\n",
      "Saving to: ‘training-parallel-nc-v9.tgz’\n",
      "\n",
      "training-parallel-n 100%[===================>]  76.69M  90.1KB/s    in 11m 57s \n",
      "\n",
      "2020-03-24 11:45:45 (110 KB/s) - ‘training-parallel-nc-v9.tgz’ saved [80418416/80418416]\n",
      "\n",
      "http://statmt.org/wmt14/training-parallel-nc-v9.tgz successfully downloaded.\n",
      "training/\n",
      "training/news-commentary-v9.fr-en.en\n",
      "training/news-commentary-v9.ru-en.en\n",
      "training/news-commentary-v9.ru-en.ru\n",
      "training/news-commentary-v9.cs-en.cs\n",
      "training/news-commentary-v9.de-en.en\n",
      "training/news-commentary-v9.de-en.de\n",
      "training/news-commentary-v9.cs-en.en\n",
      "training/news-commentary-v9.fr-en.fr\n",
      "--2020-03-24 11:45:48--  http://statmt.org/wmt14/test-full.tgz\n",
      "Resolving statmt.org (statmt.org)... 129.215.197.184\n",
      "Connecting to statmt.org (statmt.org)|129.215.197.184|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3255445 (3.1M) [application/x-gzip]\n",
      "Saving to: ‘test-full.tgz’\n",
      "\n",
      "test-full.tgz       100%[===================>]   3.10M   753KB/s    in 5.7s    \n",
      "\n",
      "2020-03-24 11:45:54 (561 KB/s) - ‘test-full.tgz’ saved [3255445/3255445]\n",
      "\n",
      "http://statmt.org/wmt14/test-full.tgz successfully downloaded.\n",
      "test-full/\n",
      "test-full/newstest2014-deen-src.en.sgm\n",
      "test-full/newstest2014-csen-ref.cs.sgm\n",
      "test-full/newstest2014-ruen-ref.ru.sgm\n",
      "test-full/newstest2014-deen-ref.de.sgm\n",
      "test-full/newstest2014-deen-src.de.sgm\n",
      "test-full/newstest2014-hien-ref.en.sgm\n",
      "test-full/newstest2014-fren-ref.fr.sgm\n",
      "test-full/newstest2014-csen-src.cs.sgm\n",
      "test-full/newstest2014-deen-ref.en.sgm\n",
      "test-full/newstest2014-fren-ref.en.sgm\n",
      "test-full/newstest2014-ruen-src.ru.sgm\n",
      "test-full/newstest2014-ruen-src.en.sgm\n",
      "test-full/newstest2014-hien-src.hi.sgm\n",
      "test-full/newstest2014-hien-src.en.sgm\n",
      "test-full/newstest2014-csen-ref.en.sgm\n",
      "test-full/newstest2014-ruen-ref.en.sgm\n",
      "test-full/newstest2014-fren-src.fr.sgm\n",
      "test-full/newstest2014-hien-ref.hi.sgm\n",
      "test-full/newstest2014-fren-src.en.sgm\n",
      "test-full/newstest2014-csen-src.en.sgm\n",
      "gzip: giga-fren.release2.fixed.*.gz: No such file or directory\n",
      "pre-processing train data...\n",
      "rm: cannot remove 'wmt14_en_fr/tmp/train.tags.en-fr.tok.en': No such file or directory\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "rm: cannot remove 'wmt14_en_fr/tmp/train.tags.en-fr.tok.fr': No such file or directory\n",
      "Tokenizer Version 1.1\n",
      "Language: fr\n",
      "Number of threads: 8\n",
      "pre-processing test data...\n",
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "\n",
      "Tokenizer Version 1.1\n",
      "Language: fr\n",
      "Number of threads: 8\n",
      "\n",
      "splitting train and valid...\n",
      "learn_bpe.py on wmt14_en_fr/tmp/train.fr-en...\n",
      "apply_bpe.py to train.en...\n",
      "apply_bpe.py to valid.en...\n",
      "apply_bpe.py to test.en...\n",
      "apply_bpe.py to train.fr...\n",
      "apply_bpe.py to valid.fr...\n",
      "apply_bpe.py to test.fr...\n",
      "clean-corpus.perl: processing wmt14_en_fr/tmp/bpe.train.en & .fr to wmt14_en_fr/train, cutoff 1-250, ratio 1.5\n",
      "..........(100000)........\n",
      "Input sentences: 183114  Output sentences:  167997\n",
      "clean-corpus.perl: processing wmt14_en_fr/tmp/bpe.valid.en & .fr to wmt14_en_fr/valid, cutoff 1-250, ratio 1.5\n",
      "\n",
      "Input sentences: 137  Output sentences:  126\n"
     ]
    }
   ],
   "source": [
    "! bash data/prepare-wmt14en2fr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see how to evaluate a pretrained model in fairseq. We'll download a pretrained model along with it's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0 1909M    0  146k    0     0  79493      0  6:59:48  0:00:01  6:59:47 79451wmt14.en-fr.fconv-py/\n",
      "wmt14.en-fr.fconv-py/model.pt\n",
      " 99 1909M   99 1907M    0     0  5443k      0  0:05:59  0:05:58  0:00:01 5247k  0     0  5934k      0  0:05:29  0:01:30  0:03:59 4321k 0     0  4706k      0  0:06:55  0:02:58  0:03:57 3300k 5089k      0  0:06:24  0:04:38  0:01:46 7822kwmt14.en-fr.fconv-py/dict.en.txt\n",
      "wmt14.en-fr.fconv-py/dict.fr.txt\n",
      "100 1909M  100 1909M    0     0  5444k      0  0:05:59  0:05:59 --:--:-- 5091k\n",
      "wmt14.en-fr.fconv-py/bpecodes\n",
      "wmt14.en-fr.fconv-py/README.md\n"
     ]
    }
   ],
   "source": [
    "! curl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf -"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This model uses a Byte Pair Encoding (BPE) vocabulary, so we’ll have to apply the encoding to the source text before it can be translated. This can be done with the apply_bpe.py script using the wmt14.en-fr.fconv-cuda/bpecodes file. @@ is used as a continuation marker and the original text can be easily recovered with e.g. sed s/@@ //g or by passing the --remove-bpe flag to fairseq-generate. Prior to BPE, input text needs to be tokenized using tokenizer.perl from mosesdecoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written a script to do it, but as a fun example, let's do it in Jupyter Notebook for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Why is it rare to discover new marine mammal species ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is it rare to discover new marine mam@@ mal species ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: en\n",
      "Number of threads: 8\n",
      "data/subword-nmt/apply_bpe.py:345: ResourceWarning: unclosed file <_io.TextIOWrapper name='wmt14.en-fr.fconv-py/bpecodes' mode='r' encoding='UTF-8'>\n",
      "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$sentence\"\n",
    "SCRIPTS=data/mosesdecoder/scripts\n",
    "TOKENIZER=$SCRIPTS/tokenizer/tokenizer.perl\n",
    "CLEAN=$SCRIPTS/training/clean-corpus-n.perl\n",
    "NORM_PUNC=$SCRIPTS/tokenizer/normalize-punctuation.perl\n",
    "REM_NON_PRINT_CHAR=$SCRIPTS/tokenizer/remove-non-printing-char.perl\n",
    "BPEROOT=data/subword-nmt\n",
    "BPE_TOKENS=40000\n",
    "src=en\n",
    "tgt=fr\n",
    "echo $1 | \\\n",
    "            perl $NORM_PUNC $src | \\\n",
    "            perl $REM_NON_PRINT_CHAR | \\\n",
    "            perl $TOKENIZER -threads 8 -a -l $src > temp_tokenized.out         \n",
    "prep=wmt14.en-fr.fconv-py\n",
    "BPE_CODE=$prep/bpecodes\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < temp_tokenized.out > final_result.out\n",
    "rm temp_tokenized.out\n",
    "cat final_result.out\n",
    "rm final_result.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the very cool interactive feature of fairseq. Open shell, cd to this directory and type the copy the following command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MODEL_DIR=wmt14.en-fr.fconv-py\n",
    "fairseq-interactive \\\n",
    "    --path $MODEL_DIR/model.pt $MODEL_DIR \\\n",
    "    --beam 1 --source-lang en --target-lang fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(beam=1, bpe=None, buffer_size=1, cpu=False, criterion='cross_entropy', data='wmt14.en-fr.fconv-py', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', input='-', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=1, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14.en-fr.fconv-py/model.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='fr', task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 43771 types\n",
      "| [fr] dictionary: 43807 types\n",
      "| loading model(s) from wmt14.en-fr.fconv-py/model.pt\n",
      "| Type the input sentence and press return:\n",
      "S-0\tWhy is it rare to discover new marine mam@@ mal species ?\n",
      "H-0\t-0.15250445902347565\tPourquoi est @-@ il rare de découvrir de nouvelles espèces de mammifères marins ?\n",
      "P-0\t-0.2221 -0.3122 -0.1289 -0.2673 -0.1711 -0.1930 -0.1101 -0.1660 -0.1003 -0.0740 -0.1101 -0.0814 -0.1238 -0.0985 -0.1288\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_DIR=wmt14.en-fr.fconv-py\n",
    "echo \"Why is it rare to discover new marine mam@@ mal species ?\" | fairseq-interactive \\\n",
    "    --path $MODEL_DIR/model.pt $MODEL_DIR \\\n",
    "    --beam 1 --source-lang en --target-lang fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generation script produces three types of outputs: a line prefixed with O is a copy of the original source sentence; H is the hypothesis along with an average log-likelihood; and P is the positional score per token position, including the end-of-sentence marker which is omitted from the text. Let's do this in bash again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is it rare to discover new marine mammal species ?\n"
     ]
    }
   ],
   "source": [
    "!  echo \"Why is it rare to discover new marine mam@@ mal species ?\" | sed -r 's/(@@ )|(@@ ?$)//g' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Good! Now let's train a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairseq contains example pre-processing scripts for several translation datasets: IWSLT 2014 (German-English), WMT 2014 (English-French) and WMT 2014 (English-German). We will work with a part of WMT 2014 like we did in the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pre-process and binarize the IWSLT dataset run <code>bash prepare-wmt14en2fr.sh</code> like we did for the previous section. This will download the data, tokenize it, perform byte pair encoding and do a test train split on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Binaize the data, we do the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "TEXT=data/wmt14_en_fr\n",
    "fairseq-preprocess --source-lang en --target-lang fr \\\n",
    "  --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
    "  --destdir data-bin/wmt14_en_fr --thresholdtgt 5 --thresholdsrc 5 \\\n",
    "  --workers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ofcourse, we cannot see what is inside the binary line, but let's check what is in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'data-bin/wmt14_en_fr/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! ls data-bin/wmt14_en_fr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'data-bin/wmt14_en_fr/dict.fr.txt' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! head -5 data-bin/wmt14_en_fr/dict.fr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de 241097\n",
      ", 209932\n",
      ". 163838\n",
      "la 142626\n",
      "les 109031\n"
     ]
    }
   ],
   "source": [
    "! head -5 data-bin/wmt14_en_fr/dict.fr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairseq provides a lot of predefined architectures to choose from. For English-French, we will choose an architecure known to work well for the problem. In the next section, we will see how to define custom models in Fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p fairseq_models/checkpoints/fconv_wmt_en_fr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! fairseq-train data-bin/wmt14_en_fr \\\n",
    "  --lr 0.5 --clip-norm 0.1 --dropout 0.1 --max-tokens 3000 \\\n",
    "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "  --lr-scheduler fixed --force-anneal 50 \\\n",
    "  --arch fconv_wmt_en_fr --save-dir fairseq_models/checkpoints/fconv_wmt_en_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and Checking BLEU for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (1.4.4)\n",
      "Requirement already satisfied: typing in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacrebleu) (3.7.4.1)\n",
      "Requirement already satisfied: portalocker in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacrebleu) (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p fairseq_models/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: fairseq-generate [-h] [--no-progress-bar] [--log-interval N]\n",
      "                        [--log-format {json,none,simple,tqdm}]\n",
      "                        [--tensorboard-logdir DIR] [--seed N] [--cpu] [--fp16]\n",
      "                        [--memory-efficient-fp16]\n",
      "                        [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                        [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                        [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                        [--min-loss-scale D]\n",
      "                        [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\n",
      "                        [--user-dir USER_DIR]\n",
      "                        [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                        [--criterion {cross_entropy,sentence_ranking,legacy_masked_lm_loss,nat_loss,label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,adaptive_loss,binary_cross_entropy,masked_lm,sentence_prediction,composite_loss}]\n",
      "                        [--tokenizer {nltk,space,moses}]\n",
      "                        [--bpe {subword_nmt,sentencepiece,fastbpe,bert,gpt2}]\n",
      "                        [--optimizer {sgd,adadelta,adafactor,nag,adam,adagrad,adamax}]\n",
      "                        [--lr-scheduler {cosine,fixed,reduce_lr_on_plateau,polynomial_decay,inverse_sqrt,triangular,tri_stage}]\n",
      "                        [--task TASK] [--num-workers N]\n",
      "                        [--skip-invalid-size-inputs-valid-test]\n",
      "                        [--max-tokens N] [--max-sentences N]\n",
      "                        [--required-batch-size-multiple N]\n",
      "                        [--dataset-impl FORMAT] [--gen-subset SPLIT]\n",
      "                        [--num-shards N] [--shard-id ID] [--path FILE]\n",
      "                        [--remove-bpe [REMOVE_BPE]] [--quiet]\n",
      "                        [--model-overrides DICT] [--results-path RESDIR]\n",
      "                        [--beam N] [--nbest N] [--max-len-a N] [--max-len-b N]\n",
      "                        [--min-len N] [--match-source-len] [--no-early-stop]\n",
      "                        [--unnormalized] [--no-beamable-mm] [--lenpen LENPEN]\n",
      "                        [--unkpen UNKPEN] [--replace-unk [REPLACE_UNK]]\n",
      "                        [--sacrebleu] [--score-reference] [--prefix-size PS]\n",
      "                        [--no-repeat-ngram-size N] [--sampling]\n",
      "                        [--sampling-topk PS] [--sampling-topp PS]\n",
      "                        [--temperature N] [--diverse-beam-groups N]\n",
      "                        [--diverse-beam-strength N] [--print-alignment]\n",
      "                        [--print-step] [--iter-decode-eos-penalty N]\n",
      "                        [--iter-decode-max-iter N]\n",
      "                        [--iter-decode-force-max-iter] [--retain-iter-history]\n",
      "                        [--decoding-format {unigram,ensemble,vote,dp,bs}]\n",
      "                        [--momentum M] [--weight-decay WD] [--force-anneal N]\n",
      "                        [--lr-shrink LS] [--warmup-updates N] [-s SRC]\n",
      "                        [-t TARGET] [--lazy-load] [--raw-text]\n",
      "                        [--load-alignments] [--left-pad-source BOOL]\n",
      "                        [--left-pad-target BOOL] [--max-source-positions N]\n",
      "                        [--max-target-positions N]\n",
      "                        [--upsample-primary UPSAMPLE_PRIMARY]\n",
      "                        [--truncate-source]\n",
      "                        data\n",
      "fairseq-generate: error: unrecognized arguments: --force\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'fairseq-generate data-bin/wmt14_en_fr  \\\\\\n  --path fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt \\\\\\n  --beam 1 --batch-size 128 --remove-bpe --sacrebleu --force >> fairseq_models/logs/our_model.out\\n'' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3f5952d87f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fairseq-generate data-bin/wmt14_en_fr  \\\\\\n  --path fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt \\\\\\n  --beam 1 --batch-size 128 --remove-bpe --sacrebleu --force >> fairseq_models/logs/our_model.out\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'fairseq-generate data-bin/wmt14_en_fr  \\\\\\n  --path fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt \\\\\\n  --beam 1 --batch-size 128 --remove-bpe --sacrebleu --force >> fairseq_models/logs/our_model.out\\n'' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fairseq-generate data-bin/wmt14_en_fr  \\\n",
    "  --path fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt \\\n",
    "  --beam 1 --batch-size 128 --remove-bpe --sacrebleu --force >> fairseq_models/logs/our_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(beam=1, bpe=None, cpu=False, criterion='cross_entropy', data='data-bin/wmt14_en_fr', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 21744 types\n",
      "| [fr] dictionary: 24256 types\n",
      "| loaded 3003 examples from: data-bin/wmt14_en_fr/test.en-fr.en\n",
      "| loaded 3003 examples from: data-bin/wmt14_en_fr/test.en-fr.fr\n",
      "| data-bin/wmt14_en_fr test en-fr 3003 examples\n",
      "| loading model(s) from fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt\n",
      "Namespace(beam=1, bpe=None, cpu=False, criterion='cross_entropy', data='data-bin/wmt14_en_fr', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='fairseq_models/checkpoints/fconv_wmt_en_fr/checkpoint_best.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 21744 types\n",
      "| [fr] dictionary: 24256 types\n"
     ]
    }
   ],
   "source": [
    "! head -10 fairseq_models/logs/our_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Translated 3003 sentences (97117 tokens) in 54.2s (55.45 sentences/s, 1793.20 tokens/s)\n",
      "| Generate test with beam=1: BLEU = 26.30 54.1/32.2/21.2/13.9 (BP = 0.982 ratio = 0.982 hyp_len = 101496 ref_len = 103343)\n"
     ]
    }
   ],
   "source": [
    "! tail -2 fairseq_models/logs/our_model.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating and Checking BLEU for the large Pretrained Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! curl https://dl.fbaipublicfiles.com/fairseq/data/wmt14.v2.en-fr.newstest2014.tar.bz2 | tar xvjf - -C data-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fairseq-generate data-bin/wmt14.en-fr.newstest2014  \\\n",
    "  --path wmt14.en-fr.fconv-py/model.pt \\\n",
    "  --beam 1 --batch-size 128 --remove-bpe --sacrebleu >> fairseq_models/logs/pretrained_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(beam=1, bpe=None, cpu=False, criterion='cross_entropy', data='data-bin/wmt14.en-fr.newstest2014', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14.en-fr.fconv-py/model.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 43771 types\n",
      "| [fr] dictionary: 43807 types\n",
      "| loaded 3003 examples from: data-bin/wmt14.en-fr.newstest2014/test.en-fr.en\n",
      "| loaded 3003 examples from: data-bin/wmt14.en-fr.newstest2014/test.en-fr.fr\n",
      "| data-bin/wmt14.en-fr.newstest2014 test en-fr 3003 examples\n",
      "| loading model(s) from wmt14.en-fr.fconv-py/model.pt\n",
      "S-647\tAir Raid Against Military Installations in Syria\n",
      "T-647\tRaid aérien contre des installations militaires en Syrie\n",
      "H-647\t-0.25782185792922974\tRaid aérien contre les installations militaires en Syrie\n"
     ]
    }
   ],
   "source": [
    "! head -10 fairseq_models/logs/pretrained_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Translated 3003 sentences (95125 tokens) in 52.2s (57.52 sentences/s, 1822.08 tokens/s)\n",
      "| Generate test with beam=1: BLEU = 43.12 69.0/50.5/39.0/30.4 (BP = 0.956 ratio = 0.957 hyp_len = 95480 ref_len = 99747)\n"
     ]
    }
   ],
   "source": [
    "! tail -2 fairseq_models/logs/pretrained_model.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing A Custom Model in FAIRSEQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extend fairseq by adding a new FairseqModel that encodes a source sentence with an LSTM and then passes the final hidden state to a second LSTM that decodes the target sentence (without attention)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we’ll define a simple LSTM Encoder and Decoder. All Encoders should implement the FairseqEncoder interface and Decoders should implement the FairseqDecoder interface. These interfaces themselves extend torch.nn.Module, so FairseqEncoders and FairseqDecoders can be written and used in the same ways as ordinary PyTorch Modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Encoder will embed the tokens in the source sentence, feed them to a torch.nn.LSTM and return the final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Decoder will predict the next word, conditioned on the Encoder’s final hidden state and an embedded representation of the previous target word – which is sometimes called input feeding or teacher forcing. More specifically, we’ll use a torch.nn.LSTM to produce a sequence of hidden states that we’ll project to the size of the output vocabulary to predict each target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve defined our Encoder and Decoder we must register our model with fairseq using the register_model() function decorator. Once the model is registered we’ll be able to use it with the existing Command-line Tools.\n",
    "\n",
    "All registered models must implement the BaseFairseqModel interface. For sequence-to-sequence models (i.e., any model with a single Encoder and Decoder), we can instead implement the FairseqModel interface.\n",
    "\n",
    "Create a small wrapper class in the same file and register it in fairseq with the name 'simple_lstm':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let’s define a named architecture with the configuration for our model. This is done with the register_model_architecture() function decorator. Thereafter this named architecture can be used with the --arch command-line argument, e.g., --arch tutorial_simple_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/fairseq/models\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "import os\n",
    "\n",
    "fairseq_file = os.path.dirname(fairseq.__file__)\n",
    "fairseq_path = os.path.join(fairseq_file, 'models')\n",
    "print(fairseq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$fairseq_path\"\n",
    "cp fairseq_models/custom_models/simple_lstm.py $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.py\n",
      "simple_lstm.py\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$fairseq_path\"\n",
    "ls $1 | grep lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just to show you how to train a custom model so we'll only train it for 3 epochs.\n",
    "Note that WMT dataset is large so you should train it for a long time. As we only trained for 3 epochs, the BLEU may be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p fairseq_models/checkpoints/tutorial_simple_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "fairseq-train data-bin/wmt14_en_fr \\\n",
    "  --arch tutorial_simple_lstm \\\n",
    "  --encoder-dropout 0.2 --decoder-dropout 0.2 \\\n",
    "  --optimizer adam --lr 0.005 --lr-shrink 0.5 \\\n",
    "  --max-tokens 12000 \\\n",
    "  --max-epoch 3 --save-dir fairseq_models/checkpoints/tutorial_simple_lstm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "\n",
    "fairseq-generate data-bin/wmt14_en_fr \\\n",
    "  --path fairseq_models/checkpoints/tutorial_simple_lstm/checkpoint_best.pt \\\n",
    "  --beam 5 --batch-size 128 \\\n",
    "  --remove-bpe --sacrebleu >> fairseq_models/logs/custom_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(beam=5, bpe=None, cpu=False, criterion='cross_entropy', data='data-bin/wmt14_en_fr', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='fairseq_models/checkpoints/tutorial_simple_lstm/checkpoint_best.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 21744 types\n",
      "| [fr] dictionary: 24256 types\n",
      "| loaded 3003 examples from: data-bin/wmt14_en_fr/test.en-fr.en\n",
      "| loaded 3003 examples from: data-bin/wmt14_en_fr/test.en-fr.fr\n",
      "| data-bin/wmt14_en_fr test en-fr 3003 examples\n",
      "| loading model(s) from fairseq_models/checkpoints/tutorial_simple_lstm/checkpoint_best.pt\n",
      "SimpleLSTMModel(\n",
      "  (encoder): SimpleLSTMEncoder(\n",
      "    (embed_tokens): Embedding(21744, 256, padding_idx=1)\n"
     ]
    }
   ],
   "source": [
    "!head -10 fairseq_models/logs/custom_model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Translated 3003 sentences (100167 tokens) in 29.1s (103.16 sentences/s, 3441.03 tokens/s)\n",
      "| Generate test with beam=5: BLEU = 4.37 17.9/5.9/3.3/1.1 (BP = 1.000 ratio = 1.191 hyp_len = 123060 ref_len = 103343)\n"
     ]
    }
   ],
   "source": [
    "!tail -2 fairseq_models/logs/custom_model.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
