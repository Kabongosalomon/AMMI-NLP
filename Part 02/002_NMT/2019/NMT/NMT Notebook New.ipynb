{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('pyfiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import global_variables\n",
    "import dataset_helper\n",
    "import nnet_models_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_saved_models_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a English to French Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = 'eng'\n",
    "target_name = 'fra'\n",
    "path_to_train_data = 'data/%s-%s_train.txt'%(source_name, target_name)\n",
    "path_to_val_data = 'data/%s-%s_val.txt'%(source_name, target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models_dir = os.path.join(base_saved_models_dir, source_name+'2'+target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think we may have something that you'd be interested in buying.\tJe pense que nous avons peut-être quelque chose dont vous seriez intéressés de faire l'acquisition.\n",
      "They got it.\tIls l'ont eue.\n",
      "I'm glad to see you.\tJe suis enchanté de vous rencontrer.\n",
      "He got into his car in a hurry.\tIl monta en vitesse dans sa voiture.\n",
      "Do you like Mozart's music?\tAimez-vous la musique de Mozart ?\n"
     ]
    }
   ],
   "source": [
    "## See first 5 records\n",
    "\n",
    "! head -5 'data/eng-fra_train.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and making PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to make it a pair - (source, target) sentence pair. For this, we have to read the file and parse it accordingly. We might have to take care of some details there, like making sure that we strip off any non-required special characters or extra space. All those boring details aside (which you can see in dataset_helper.py) what are the other things we have to do?\n",
    "\n",
    "We have to make a vocabulary and tokenize like we have been doing. Here, we are writing a Language Class, like we did in the previous labs to take care of this for you. Once we have done all this and tokenized, we write a pytorch dataset object to help as handle this efficiently during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_language_model_dir = os.path.join(saved_models_dir, 'lang_obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {'train': dataset_helper.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                    filepath = path_to_train_data, \n",
    "                    lang_obj_path = saved_language_model_dir), \n",
    "\n",
    "                'val': dataset_helper.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                    filepath = path_to_val_data, \n",
    "                    lang_obj_path = saved_language_model_dir)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LanguagePair object we built has a DataFrame underneath. We see the first 5 rows of the dataframe below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_data</th>\n",
       "      <th>target_data</th>\n",
       "      <th>source_tokenized</th>\n",
       "      <th>source_len</th>\n",
       "      <th>target_tokenized</th>\n",
       "      <th>target_len</th>\n",
       "      <th>source_indized</th>\n",
       "      <th>target_indized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i think we may have something that you d be in...</td>\n",
       "      <td>je pense que nous avons peut etre quelque chos...</td>\n",
       "      <td>[i, think, we, may, have, something, that, you...</td>\n",
       "      <td>15</td>\n",
       "      <td>[je, pense, que, nous, avons, peut, etre, quel...</td>\n",
       "      <td>19</td>\n",
       "      <td>[5, 66, 15, 93, 20, 98, 23, 6, 143, 27, 458, 1...</td>\n",
       "      <td>[7, 68, 8, 17, 36, 90, 24, 74, 71, 296, 6, 665...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they got it .</td>\n",
       "      <td>ils l ont eue .</td>\n",
       "      <td>[they, got, it, .]</td>\n",
       "      <td>5</td>\n",
       "      <td>[ils, l, ont, eue, .]</td>\n",
       "      <td>6</td>\n",
       "      <td>[45, 117, 11, 4, 1]</td>\n",
       "      <td>[52, 9, 88, 2946, 4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i m glad to see you .</td>\n",
       "      <td>je suis enchante de vous rencontrer .</td>\n",
       "      <td>[i, m, glad, to, see, you, .]</td>\n",
       "      <td>8</td>\n",
       "      <td>[je, suis, enchante, de, vous, rencontrer, .]</td>\n",
       "      <td>8</td>\n",
       "      <td>[5, 13, 478, 7, 92, 6, 4, 1]</td>\n",
       "      <td>[7, 35, 5158, 5, 6, 570, 4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he got into his car in a hurry .</td>\n",
       "      <td>il monta en vitesse dans sa voiture .</td>\n",
       "      <td>[he, got, into, his, car, in, a, hurry, .]</td>\n",
       "      <td>10</td>\n",
       "      <td>[il, monta, en, vitesse, dans, sa, voiture, .]</td>\n",
       "      <td>9</td>\n",
       "      <td>[12, 117, 67, 76, 108, 10, 14, 410, 4, 1]</td>\n",
       "      <td>[12, 4992, 18, 1354, 29, 155, 125, 4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do you like mozart s music ?</td>\n",
       "      <td>aimez vous la musique de mozart ?</td>\n",
       "      <td>[do, you, like, mozart, s, music, ?]</td>\n",
       "      <td>8</td>\n",
       "      <td>[aimez, vous, la, musique, de, mozart, ?]</td>\n",
       "      <td>8</td>\n",
       "      <td>[19, 6, 72, 2, 22, 349, 16, 1]</td>\n",
       "      <td>[633, 6, 13, 356, 5, 2, 19, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         source_data  \\\n",
       "0  i think we may have something that you d be in...   \n",
       "1                                      they got it .   \n",
       "2                              i m glad to see you .   \n",
       "3                   he got into his car in a hurry .   \n",
       "4                       do you like mozart s music ?   \n",
       "\n",
       "                                         target_data  \\\n",
       "0  je pense que nous avons peut etre quelque chos...   \n",
       "1                                    ils l ont eue .   \n",
       "2              je suis enchante de vous rencontrer .   \n",
       "3              il monta en vitesse dans sa voiture .   \n",
       "4                  aimez vous la musique de mozart ?   \n",
       "\n",
       "                                    source_tokenized  source_len  \\\n",
       "0  [i, think, we, may, have, something, that, you...          15   \n",
       "1                                 [they, got, it, .]           5   \n",
       "2                      [i, m, glad, to, see, you, .]           8   \n",
       "3         [he, got, into, his, car, in, a, hurry, .]          10   \n",
       "4               [do, you, like, mozart, s, music, ?]           8   \n",
       "\n",
       "                                    target_tokenized  target_len  \\\n",
       "0  [je, pense, que, nous, avons, peut, etre, quel...          19   \n",
       "1                              [ils, l, ont, eue, .]           6   \n",
       "2      [je, suis, enchante, de, vous, rencontrer, .]           8   \n",
       "3     [il, monta, en, vitesse, dans, sa, voiture, .]           9   \n",
       "4          [aimez, vous, la, musique, de, mozart, ?]           8   \n",
       "\n",
       "                                      source_indized  \\\n",
       "0  [5, 66, 15, 93, 20, 98, 23, 6, 143, 27, 458, 1...   \n",
       "1                                [45, 117, 11, 4, 1]   \n",
       "2                       [5, 13, 478, 7, 92, 6, 4, 1]   \n",
       "3          [12, 117, 67, 76, 108, 10, 14, 410, 4, 1]   \n",
       "4                     [19, 6, 72, 2, 22, 349, 16, 1]   \n",
       "\n",
       "                                      target_indized  \n",
       "0  [7, 68, 8, 17, 36, 90, 24, 74, 71, 296, 6, 665...  \n",
       "1                            [52, 9, 88, 2946, 4, 1]  \n",
       "2                     [7, 35, 5158, 5, 6, 570, 4, 1]  \n",
       "3           [12, 4992, 18, 1354, 29, 155, 125, 4, 1]  \n",
       "4                     [633, 6, 13, 356, 5, 2, 19, 1]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['train'].main_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabulary sizes and sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocab:  4969 target vocab:  6788\n"
     ]
    }
   ],
   "source": [
    "### vocabulary sizes\n",
    "print('source vocab: ', dataset_dict['train'].source_lang_obj.n_words , \n",
    "      'target vocab: ', dataset_dict['train'].target_lang_obj.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len:  51 min len:  3\n"
     ]
    }
   ],
   "source": [
    "### vocabulary sizes\n",
    "print('max len: ', dataset_dict['train'].main_df['source_len'].max(), \n",
    "      'min len: ', dataset_dict['train'].main_df['source_len'].min() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.500     8.0\n",
       "0.750    10.0\n",
       "0.900    12.0\n",
       "0.950    13.0\n",
       "0.990    17.0\n",
       "0.999    22.0\n",
       "Name: source_len, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['train'].main_df['source_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51 looks like a very long sentence and at the $99.9$th percentile is 22. We probably don't want that much. How do we get rid of rest of the words or clip sentence at some MAX LEN? We can use the collate function of pytorch that we had seen earlier to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = int(dataset_dict['train'].main_df['source_len'].quantile(0.999))\n",
    "batchSize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_dict = {'train': DataLoader(dataset_dict['train'], batch_size = batchSize, \n",
    "                            collate_fn = partial(dataset_helper.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
    "                            shuffle = True, num_workers=0), \n",
    "                    'val': DataLoader(dataset_dict['val'], batch_size = batchSize, \n",
    "                            collate_fn = partial(dataset_helper.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
    "                            shuffle = True, num_workers=0) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of usually of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence. Essentially, all we need is some mechanism to read the source sentence and create an encoding and some mechanism to read the encoding and decode it to the target language. \n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder is anything which takes in a sentence and gives us a representation for the sentence. \n",
    "\n",
    "Usually, the encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "However, we will first start with a BoW encoder and then move on to RNN based encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### configuration\n",
    "\n",
    "source_vocab = dataset_dict['train'].source_lang_obj.n_words;\n",
    "target_vocab = dataset_dict['train'].target_lang_obj.n_words;\n",
    "hidden_size = 512\n",
    "rnn_layers = 1\n",
    "lr = 0.25;\n",
    "longest_label = 1;\n",
    "gradient_clip = 0.3;\n",
    "use_cuda = True\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BagOfWords Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_bow = nnet_models_new.BagOfWords(input_size = source_vocab,\n",
    "                                    hidden_size = hidden_size, \n",
    "                                    nlayers=10, \n",
    "                                    reduce = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "--------------------\n",
    "\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "Decoder w/o Attention\n",
    "------------------------\n",
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_bow = nnet_models_new.DecoderRNN(target_vocab, hidden_size, rnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_bow = nnet_models_new.seq2seq(encoder_bow, decoder_bow,\n",
    "                              lr = lr, \n",
    "                              use_cuda = use_cuda, \n",
    "                              hiddensize = hidden_size, \n",
    "                              numlayers = hidden_size, \n",
    "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
    "                              longest_label = longest_label,\n",
    "                              clip = gradient_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_filepath(path, enc_type):\n",
    "    filename = 'nmt_enc_'+enc_type+'_dec_rnn.pth'\n",
    "    return os.path.join(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(nmt_model, path, enc_type):\n",
    "    if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    filename = 'nmt_enc_'+enc_type+'_dec_rnn.pth'\n",
    "    torch.save(nmt_model, os.path.join(path, filename))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, nmt, num_epochs=50, val_every=1, saved_model_path = '.', enc_type ='rnn'):\n",
    "\n",
    "    best_bleu = -1;\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        for data in dataloader['train']:\n",
    "    \n",
    "            _, curr_loss = nmt.train_step(data);\n",
    "            running_loss += curr_loss\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader['train']) \n",
    "        \n",
    "        print(\"epoch {} loss = {}, time = {}\".format(epoch, epoch_loss,\n",
    "                                                        time.time() - start))\n",
    "        sys.stdout.flush()\n",
    "   \n",
    "        if epoch%val_every == 0:\n",
    "            val_bleu_score = nmt.get_bleu_score(dataloader['val']);\n",
    "            print('validation bleu: ', val_bleu_score)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            nmt.scheduler_step(val_bleu_score);\n",
    "            \n",
    "            if val_bleu_score > best_bleu:\n",
    "                best_bleu = val_bleu_score\n",
    "                best_wts = nmt.state_dict()\n",
    "                save_models(nmt, saved_model_path, enc_type);\n",
    "\n",
    "        print('='*50)\n",
    "\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))\n",
    "\n",
    "    return nmt.load_state_dict(best_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Bow Encoder GRU Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_again = False\n",
    "modelname = 'bow'\n",
    "if os.path.exists(get_full_filepath(saved_models_dir, modelname)) and (not train_again):\n",
    "    nmt_bow = torch.load(get_full_filepath(saved_models_dir, modelname))\n",
    "else:\n",
    "    nmt_bow = train_model(dataloader_dict, nmt_bow, \n",
    "                          num_epochs = num_epochs, \n",
    "                          saved_model_path = saved_models_dir, \n",
    "                          enc_type = 'bow_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmt_bow.get_bleu_score(dataloader_dict['val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_rnn = nnet_models_new.EncoderRNN(source_vocab, hidden_size, rnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_rnn = nnet_models_new.DecoderRNN(target_vocab, hidden_size, rnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_rnn = nnet_models_new.seq2seq(encoder_rnn, decoder_rnn,\n",
    "                              lr = lr, \n",
    "                              use_cuda = use_cuda, \n",
    "                              hiddensize = hidden_size, \n",
    "                              numlayers = hidden_size, \n",
    "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
    "                              longest_label = longest_label,\n",
    "                              clip = gradient_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_again = True\n",
    "if os.path.exists(get_full_filepath(saved_models_dir, 'rnn')) and (not train_again):\n",
    "    nmt_rnn = torch.load(get_full_filepath(saved_models_dir, 'rnn'))\n",
    "else:\n",
    "    nmt_rnn = train_model(dataloader_dict, nmt_rnn, \n",
    "                      num_epochs = num_epochs, \n",
    "                      saved_model_path = saved_models_dir, \n",
    "                      enc_type = 'rnn_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmt_rnn.get_bleu_score(dataloader_dict['val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder + Source Side Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention = True\n",
    "self_attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_encoderattn = nnet_models_new.EncoderRNN(source_vocab, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_encoderattn = nnet_models_new.Decoder_SelfAttn(output_size=target_vocab,\n",
    "                                 hidden_size=hidden_size, \n",
    "                                 encoder_attention = encoder_attention,\n",
    "                                 self_attention = self_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_encoderattn = nnet_models_new.seq2seq(encoder_encoderattn, decoder_encoderattn,\n",
    "                              lr = lr, \n",
    "                              use_cuda = use_cuda, \n",
    "                              hiddensize = hidden_size, \n",
    "                              numlayers = hidden_size, \n",
    "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
    "                              longest_label = longest_label,\n",
    "                              clip = gradient_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'encoderattn'\n",
    "if os.path.exists(get_full_filepath(saved_models_dir, modelname)):\n",
    "    nmt_encoderattn = torch.load(get_full_filepath(saved_models_dir, modelname))\n",
    "else:\n",
    "    nmt_encoderattn = train_model(dataloader_dict, nmt_encoderattn, \n",
    "                      num_epochs = num_epochs, \n",
    "                      saved_model_path = saved_models_dir, \n",
    "                      enc_type = 'encoderattn_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Encoder, Self Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_selfattn = nnet_models_new.EncoderRNN(source_vocab, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_selfattn = nnet_models_new.Decoder_SelfAttn(output_size=target_vocab,\n",
    "                                 hidden_size=hidden_size, \n",
    "                                 self_attention = self_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_selfattn = nnet_models_new.seq2seq(encoder_selfattn, decoder_selfattn,\n",
    "                              lr = lr, \n",
    "                              use_cuda = use_cuda, \n",
    "                              hiddensize = hidden_size, \n",
    "                              numlayers = hidden_size, \n",
    "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
    "                              longest_label = longest_label,\n",
    "                              clip = gradient_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'selfattn'\n",
    "if os.path.exists(get_full_filepath(saved_models_dir, modelname)):\n",
    "    nmt_selfattn = torch.load(get_full_filepath(saved_models_dir, modelname))\n",
    "else:\n",
    "    nmt_selfattn = train_model(dataloader_dict, nmt_selfattn, \n",
    "                      num_epochs = num_epochs, \n",
    "                      saved_model_path = saved_models_dir, \n",
    "                      enc_type = 'selfattn_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
