{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Modeling\n",
    "\n",
    "In this lab, we will overview the **masked language modeling** objective, and how the **Transformer** architecture is used for large-scale masked language modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os, sys, glob, json, math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let set the random seeds for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Recently, Devlin et al. published [BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf).\n",
    "\n",
    "\n",
    "**B**idirectional\n",
    "\n",
    "**E**ncoder\n",
    "\n",
    "**R**epresentations from\n",
    "\n",
    "**T**ransformers\n",
    "\n",
    "\n",
    "#### Goal: \n",
    "1. **pre-train** a model that produces language representations. \n",
    "2. **fine-tune** the model on a task.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Language Model Objective\n",
    "\n",
    "Randomly mask some of the tokens from the input, predict original vocabulary id of each masked token.\n",
    "\n",
    "- Given sequence $x_1,\\ldots,x_N$.\n",
    "\n",
    "- Form **mask** $m_1,\\ldots,m_N$ where $m_i\\in \\{0,1\\}$.\n",
    "    - E.g. $m_i=1$ with probability 0.15\n",
    "    \n",
    "- Form **masked sequence** $\\tilde{x}_1,\\ldots,\\tilde{x}_N$.\n",
    "    - $\\tilde{x}_i=\\begin{cases} x_i & m_i=0\\\\ \\texttt{[MASK]} & m_i=1\\end{cases}$\n",
    "\n",
    "\n",
    "#### $$\\mathcal{L}_{\\text{MLM}}=-\\sum_{\\underbrace{i | m_i=1}_{\\text{MASKED POSITIONS}}}\\log p_{\\theta}(\\underbrace{x_i}_{\\text{TRUE TOKEN}}|\\underbrace{\\tilde{x}_1,\\ldots,\\tilde{x}_N}_{\\text{MASKED SEQUENCE}})$$\n",
    "\n",
    "\n",
    "<!-- Below, we will discuss the exact form of $\\tilde{x}_i$ that the BERT authors used. -->\n",
    "\n",
    "\n",
    "<!-- #### Diagram of BERT Implementation -->\n",
    "<!-- ![](bert_overview.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "So far we have modeled a sequence by factorizing the joint distribution into conditionals, and **parameterizing each conditional with a recurrent network**:\n",
    "\n",
    "\n",
    "#### $$p_{\\theta}(x_1,\\ldots,x_T)=\\prod_{t=1}^T p_{\\theta}(x_t | x_{<t})$$\n",
    "\\begin{align}\n",
    "h_t &= RNN(x_{t-1}, h_t)\\\\\n",
    "p_{\\theta}(x_t | x_{<t}) &=\\text{softmax}\\left(Wh_t+b\\right),\n",
    "\\end{align}\n",
    "\n",
    "where $\\theta$ are the model parameters (RNN parameters, $W, b$, embedding matrix).\n",
    "\n",
    "\n",
    "#### Alternative\n",
    "\n",
    "An alternative proposed in [[Vaswani et al 2017](https://arxiv.org/pdf/1706.03762.pdf)] is to parameterize each conditional with a **particular feed-forward architecture** called the **Transformer**. With this model, it is possible to compute all conditionals with a **single feed-forward pass**:\n",
    "\\begin{align}\n",
    "(h_1,\\ldots,h_T) &= Transformer(x)\\\\\n",
    "p_{\\theta}(x_t | x_{<t}) &= \\text{softmax}\\left(Wh_t + b\\right)\n",
    "\\end{align}\n",
    "\n",
    "We will discuss briefly the key ideas, the overall **Transformer architecture (encoder only)**, and how they are used in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Level View\n",
    "\n",
    "We can view the Transformer encoder as mapping a sequence to a sequence of vectors.\n",
    "\n",
    "<img src=\"img/high1.png\" alt=\"Drawing\" style=\"width: 35%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through the key ideas of how this mapping is designed, and discuss some of its resulting properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Idea 1: Position Embeddings\n",
    "\n",
    "**Unlike RNNs which can learn positional information via the hidden state over time, the Transformer has no notion of time**.\n",
    "\n",
    "Thus we encode inputs with **position** as well as **token** embeddings:\n",
    "\n",
    "<img src=\"img/high2.png\" alt=\"Drawing\" style=\"width: 35%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = ['<s>', 'my', 'pet', '[M]', '<s>']\n",
    "\n",
    "max_len = 10\n",
    "\n",
    "vocab = {'<s>': 0, 'my': 1, 'pet': 2, 'dog': 3, 'cat': 4, 'lion': 5, '[M]': 6}\n",
    "\n",
    "dim = 6\n",
    "\n",
    "token_embed = nn.Embedding(len(vocab), embedding_dim=dim)  # an embedding for each token\n",
    "position_embed = nn.Embedding(max_len, embedding_dim=dim)  # an empbedding to count for the position of the token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector = torch.tensor([vocab[x] for x in input_sequence]).unsqueeze(1)   # get the numerical representation of the token\n",
    "\n",
    "input_embeddings = token_embed(input_vector) + position_embed(torch.arange(len(input_vector))).unsqueeze(1)  # add the input embedding to the position embedding\n",
    "input_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning!!** The pytorch Transformer classes accept input as `Length x Batch x Dim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Idea 2: Modularity\n",
    "The Transformer (encoder) is composed of a stack of **N identical layers**.\n",
    "\n",
    "<img src=\"img/layers.png\" alt=\"Drawing\" style=\"width: 35%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "TransformerEncoder is a stack of N encoder layers\n",
       "\n",
       "Args:\n",
       "    encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
       "    num_layers: the number of sub-encoder-layers in the encoder (required).\n",
       "    norm: the layer normalization component (optional).\n",
       "\n",
       "Examples::\n",
       "    >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
       "    >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
       "    >>> src = torch.rand(10, 32, 512)\n",
       "    >>> out = transformer_encoder(src)\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/aims/lib/python3.7/site-packages/torch/nn/modules/transformer.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "nn.TransformerEncoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `forward` passes the input through the N layers, then normalizes it:\n",
    "\n",
    "**Warning!!** The forward function accepts input as `Length x Batch x Dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.TransformerEncoder.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=2, dim_feedforward=64, dropout=0.1) # attention is all you need config nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "\n",
    "encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: \t(5, 1, 6)\n",
      "output size:\t(5, 1, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5391, -1.9445,  1.0577, -0.4564,  0.0294,  0.7747]],\n",
       "\n",
       "        [[-0.7556,  0.5501, -1.3479,  1.5129,  0.7344, -0.6939]],\n",
       "\n",
       "        [[ 0.7792, -1.9722,  0.9250,  0.5844,  0.1997, -0.5160]],\n",
       "\n",
       "        [[-0.9059, -1.2809,  0.1254, -0.3488,  1.7034,  0.7068]],\n",
       "\n",
       "        [[ 1.1515, -1.2860,  0.2149,  1.3268, -0.3691, -1.0381]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = encoder(input_embeddings)\n",
    "\n",
    "print(\"input size: \\t%s\" % str(tuple(input_embeddings.shape)))\n",
    "print(\"output size:\\t%s\" % str(tuple(outputs.shape)))             # contextualize embedding of each token\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1117, -0.4966,  0.1631, -0.8817,  0.0539,  0.6684]],\n",
      "\n",
      "        [[-0.0597, -0.4675, -0.2153,  0.8840, -0.7584, -0.3689]],\n",
      "\n",
      "        [[-0.3424, -1.4020,  0.3206, -1.0219,  0.7988, -0.0923]],\n",
      "\n",
      "        [[-0.7690, -1.5606, -0.5309,  0.2178,  1.3232,  1.8169]],\n",
      "\n",
      "        [[-0.1117, -0.4966,  0.1631, -0.8817,  0.0539,  0.6684]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(token_embed(input_vector)) # the original token embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each layer has two parts, **self-attention** and a feed-forward transformation:\n",
    "\n",
    "<img src=\"img/layer.png\" alt=\"Drawing\" style=\"width: 65%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.TransformerEncoderLayer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.TransformerEncoderLayer.forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Idea 3: Self-Attention\n",
    "\n",
    "In the RNN, the hidden state contains information about previous tokens.\n",
    "The Transformer instead performs **attention** over all inputs at a given layer. 'Attention' computes an output vector by taking a weighted sum of input vectors. The weights are 'attention weights'. The Transformer uses **scaled dot-product attention**:\n",
    "#### $$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "and 'Multi-head Attention' refers to applying several of these operations in parallel.\n",
    "\n",
    "#### *Key Property*: Each output vector of a layer $n$ can using information from **all** inputs to the layer $n$.\n",
    "\n",
    "Thus each **final output vector** can incorporate information from **all input words**.\n",
    "\n",
    "(If we want to prevent information flow such as in left-to-right language modeling, we can use masking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madd_bias_kv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madd_zero_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Allows the model to jointly attend to information\n",
       "from different representation subspaces.\n",
       "See reference: Attention Is All You Need\n",
       "\n",
       ".. math::\n",
       "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
       "    \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
       "\n",
       "Args:\n",
       "    embed_dim: total dimension of the model.\n",
       "    num_heads: parallel attention heads.\n",
       "    dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
       "    bias: add bias as module parameter. Default: True.\n",
       "    add_bias_kv: add bias to the key and value sequences at dim=0.\n",
       "    add_zero_attn: add a new batch of zeros to the key and\n",
       "                   value sequences at dim=1.\n",
       "    kdim: total number of features in key. Default: None.\n",
       "    vdim: total number of features in key. Default: None.\n",
       "\n",
       "    Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
       "    query, key, and value have the same number of features.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
       "    >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/aims/lib/python3.7/site-packages/torch/nn/modules/activation.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.MultiheadAttention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (5, 1, 6)\n",
      "output shape: (5, 1, 6)\n",
      "tensor([[[ 0.5391, -1.9445,  1.0577, -0.4564,  0.0294,  0.7747]],\n",
      "\n",
      "        [[-0.7556,  0.5501, -1.3479,  1.5129,  0.7344, -0.6939]],\n",
      "\n",
      "        [[ 0.7792, -1.9722,  0.9250,  0.5844,  0.1997, -0.5160]],\n",
      "\n",
      "        [[-0.9059, -1.2809,  0.1254, -0.3488,  1.7034,  0.7068]],\n",
      "\n",
      "        [[ 1.1515, -1.2860,  0.2149,  1.3268, -0.3691, -1.0381]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "\n",
      "attn weights shape: (1, 5, 5)\n",
      "tensor([[[0.1703, 0.2043, 0.1716, 0.2744, 0.1794],\n",
      "         [0.2384, 0.1427, 0.2343, 0.1328, 0.2517],\n",
      "         [0.1686, 0.1739, 0.1843, 0.2152, 0.2581],\n",
      "         [0.1906, 0.1497, 0.2084, 0.2162, 0.2351],\n",
      "         [0.1768, 0.1457, 0.1723, 0.2346, 0.2706]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn = nn.MultiheadAttention(dim, 2, dropout=0.0)\n",
    "\n",
    "attn_outputs, attn_weights = attn.forward(query=outputs, key=outputs, value=outputs)\n",
    "\n",
    "print(\"input shape: %s\" % (str(tuple(outputs.size()))))\n",
    "print(\"output shape: %s\" % (str(tuple(attn_outputs.size()))))\n",
    "print(outputs)\n",
    "\n",
    "print(\"\\nattn weights shape: %s\" % (str(tuple(attn_weights.size()))))\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, dim=8, num_layers=4, nhead=2):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, dim)\n",
    "        self.position_embed = nn.Embedding(max_len, dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=nhead, dim_feedforward=64, dropout=0.0)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.projection = nn.Linear(dim, vocab_size)\n",
    "    \n",
    "    def features(self, token_indices):\n",
    "        pos = torch.arange(len(token_indices), device=token_indices.device).unsqueeze(1)\n",
    "        x = self.token_embed(token_indices) + self.position_embed(pos)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, token_indices):\n",
    "        x = self.features(token_indices)\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1841,  0.3811,  1.4767,  0.3744, -2.2709, -0.2429, -0.0631,\n",
       "           0.5290]],\n",
       "\n",
       "        [[ 0.6175,  0.9034,  1.0025, -0.8787,  1.1249, -1.4216, -0.0295,\n",
       "          -1.3185]],\n",
       "\n",
       "        [[-0.3699,  0.7491,  0.4773, -0.6109,  1.7609,  0.3687, -0.6204,\n",
       "          -1.7548]],\n",
       "\n",
       "        [[-1.1181,  0.9639, -0.8665,  0.5216, -1.5619,  0.5622,  0.0716,\n",
       "           1.4273]],\n",
       "\n",
       "        [[-0.1518, -0.3191,  1.9927,  0.0086, -1.1681, -1.3354,  0.1103,\n",
       "           0.8627]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(len(vocab), max_len=100)\n",
    "\n",
    "model.features(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 8])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features(input_vector).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embed): Embedding(7, 8)\n",
       "  (position_embed): Embedding(100, 8)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=8, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=8, bias=True)\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=8, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=8, bias=True)\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=8, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=8, bias=True)\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=8, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=8, bias=True)\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Linear(in_features=8, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1013, -1.4392, -0.7411, -1.5282,  0.5737, -1.4641, -0.3159,\n",
       "           0.9548]],\n",
       "\n",
       "        [[-0.0104,  0.0784,  0.5036, -0.7424,  1.9766, -0.9712,  0.1476,\n",
       "          -1.6098]],\n",
       "\n",
       "        [[-0.7888, -0.8349,  1.3856, -2.7129,  1.0593,  1.2605,  0.0641,\n",
       "          -0.4017]],\n",
       "\n",
       "        [[ 0.0079, -1.4143, -2.0439, -0.7530,  0.1951,  1.4279, -0.3208,\n",
       "           1.5795]],\n",
       "\n",
       "        [[-1.1013, -1.4392, -0.7411, -1.5282,  0.5737, -1.4641, -0.3159,\n",
       "           0.9548]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_embed(input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Masked Language Modeling\n",
    "\n",
    "Recall the **key property** of Transformers: due to self-attention, each output vector can incorporate information from *all* input tokens.\n",
    "\n",
    "<img src=\"img/mlm.png\" alt=\"Drawing\" style=\"width: 45%;\"/>\n",
    "\n",
    "This is useful for masked language modeling, where we want to use information from the entire context when predicting the masked token(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLM on Persona-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133176/133176 [00:31<00:00, 4251.03it/s]\n",
      "100%|██████████| 133176/133176 [00:00<00:00, 168611.57it/s]\n",
      "100%|██████████| 16181/16181 [00:00<00:00, 169260.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 19157\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "raw_datasets, datasets, vocab = utils.load_personachat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "trainloader = DataLoader(datasets['train'], batch_size=4, collate_fn=lambda x: utils.pad_collate_fn(vocab.get_id('<pad>'), x))\n",
    "validloader = DataLoader(datasets['valid'], batch_size=4, collate_fn=lambda x: utils.pad_collate_fn(vocab.get_id('<pad>'), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0],\n",
       "        [ 4,  4,  4, 22],\n",
       "        [ 5,  5, 18, 23],\n",
       "        [ 6, 13, 17, 24],\n",
       "        [ 7, 14, 19, 15],\n",
       "        [ 8, 15, 13, 25],\n",
       "        [ 9, 16, 20, 26],\n",
       "        [10, 17, 21, 27],\n",
       "        [11, 12, 12, 28],\n",
       "        [12,  0,  0, 29],\n",
       "        [ 0,  2,  2, 30],\n",
       "        [ 2,  2,  2, 24],\n",
       "        [ 2,  2,  2,  4],\n",
       "        [ 2,  2,  2, 31],\n",
       "        [ 2,  2,  2, 32],\n",
       "        [ 2,  2,  2, 27],\n",
       "        [ 2,  2,  2, 33],\n",
       "        [ 2,  2,  2, 34],\n",
       "        [ 2,  2,  2, 35],\n",
       "        [ 2,  2,  2, 36],\n",
       "        [ 2,  2,  2, 24],\n",
       "        [ 2,  2,  2,  0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(trainloader.__iter__())\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(inputs, mask_prob, pad_token_id, mask_token_id, vsize):\n",
    "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\"\"\"\n",
    "    inputs = inputs.clone()\n",
    "    labels = inputs.clone()\n",
    "    # Sample tokens in each sequence for masked-LM training\n",
    "    masked_indices = torch.bernoulli(torch.full(labels.shape, mask_prob)).bool()\n",
    "    masked_indices = masked_indices & (inputs != pad_token_id)\n",
    "    labels[~masked_indices] = -1  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = mask_token_id\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(vsize, labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask token id: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0],\n",
       "        [ 1,  4,  4, 22],\n",
       "        [ 5,  5, 18, 23],\n",
       "        [ 6, 13, 17,  1],\n",
       "        [ 7,  1, 19, 15],\n",
       "        [ 8, 15, 13,  1],\n",
       "        [ 9, 16, 20, 26],\n",
       "        [10, 17, 21, 27],\n",
       "        [ 1, 12, 12, 28],\n",
       "        [12,  0,  0, 29],\n",
       "        [ 0,  2,  2, 30],\n",
       "        [ 2,  2,  2, 24],\n",
       "        [ 2,  2,  2,  4],\n",
       "        [ 2,  2,  2, 31],\n",
       "        [ 2,  2,  2, 32],\n",
       "        [ 2,  2,  2, 27],\n",
       "        [ 2,  2,  2,  1],\n",
       "        [ 2,  2,  2, 34],\n",
       "        [ 2,  2,  2, 35],\n",
       "        [ 2,  2,  2, 36],\n",
       "        [ 2,  2,  2, 24],\n",
       "        [ 2,  2,  2,  0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = mask_tokens(batch, mask_prob=0.15, mask_token_id=vocab.get_id('[M]'), pad_token_id=vocab.get_id('<pad>'), vsize=len(vocab))\n",
    "print(\"Mask token id: %d\" % vocab.get_id('[M]'))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -1, -1, -1],\n",
       "        [ 4, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, 24],\n",
       "        [-1, 14, -1, -1],\n",
       "        [-1, -1, -1, 25],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [11, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, 33],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(vocab), max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4, 19157])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(inputs)\n",
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.2056, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_ = logits.view(-1, logits.size(2))\n",
    "labels_ = labels.view(-1)\n",
    "\n",
    "criterion(logits_, labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import torch.optim as optim\n",
    "    from tqdm import tqdm, trange\n",
    "    from collections import defaultdict\n",
    "    from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "    trainloader = DataLoader(datasets['train'], batch_size=64, collate_fn=lambda x: utils.pad_collate_fn(vocab.get_id('<pad>'), x))\n",
    "    validloader = DataLoader(datasets['valid'], batch_size=64, collate_fn=lambda x: utils.pad_collate_fn(vocab.get_id('<pad>'), x))\n",
    "\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = Transformer(len(vocab), max_len=65, dim=256, nhead=8).to(device)\n",
    "\n",
    "    model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(model_parameters, lr=0.001)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1).to(device)\n",
    "\n",
    "    stats = defaultdict(list)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for step, batch in enumerate(trainloader):\n",
    "            model.train()        \n",
    "            # Mask the batch\n",
    "            inputs, labels = mask_tokens(batch, mask_prob=0.15, \n",
    "                                         pad_token_id=vocab.get_id('<pad>'),\n",
    "                                         mask_token_id=vocab.get_id('[M]'), \n",
    "                                         vsize=len(vocab))\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits_ = logits.view(-1, logits.size(2))\n",
    "            labels_ = labels.view(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(logits_, labels_)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            stats['train_loss'].append(loss.item())\n",
    "            stats['train_loss_log'].append(loss.item())\n",
    "            if (step % 500) == 0:\n",
    "                avg_loss = sum(stats['train_loss_log']) / len(stats['train_loss_log'])\n",
    "                print(\"Epoch %d Step %d\\tTrain Loss %.3f\" % (epoch, step, avg_loss))\n",
    "                stats['train_loss_log'] = []\n",
    "\n",
    "        for batch in validloader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Mask the batch\n",
    "                inputs, labels = mask_tokens(batch, mask_prob=0.15, \n",
    "                                             pad_token_id=vocab.get_id('<pad>'),\n",
    "                                             mask_token_id=vocab.get_id('[M]'), \n",
    "                                             vsize=len(vocab))\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                logits = model(inputs)\n",
    "                logits_ = logits.view(-1, logits.size(2))\n",
    "                labels_ = labels.view(-1)\n",
    "\n",
    "                loss = criterion(logits_, labels_)\n",
    "                stats['valid_loss'].append(loss.item())\n",
    "        print(\"=== Epoch %d\\tValid Loss %.3f\" % (epoch, stats['valid_loss'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Conditionals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "checkpoint = utils.load('model', 'model', best=True)\n",
    "options = checkpoint['options']\n",
    "stats = checkpoint['stats']\n",
    "\n",
    "\n",
    "model = utils.Transformer(len(vocab), options['max_len'], \n",
    "                          dim=options['dim'], \n",
    "                          nhead=options['nhead'])\n",
    "model.load_state_dict(checkpoint['model_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['<s>', 'i', 'have', 'a', 'pet', '[M]', '.', '<s>'],\n",
    "             ['<s>', 'i', 'have', 'two', 'pet', '[M]', '.', '<s>'],\n",
    "             ['<s>', 'my', '[M]', 'is', 'a', 'lawyer', '.', '<s>'],\n",
    "             ['<s>', 'my', '[M]', 'is', 'a', '[M]', '.', '<s>'],\n",
    "             ['<s>', 'i', '[M]', '[M]', '[M]', 'sometimes', '.' , '<s>']]\n",
    "\n",
    "\n",
    "def get_top_masked_tokens(tokens, vocab, device, top=10):\n",
    "    ids = torch.tensor([vocab.get_id(x) for x in tokens], device=device).unsqueeze(1)\n",
    "    masked = ids == vocab.get_id('[M]')\n",
    "\n",
    "    logits = model(ids)[masked]\n",
    "    probs = torch.softmax(logits, -1)\n",
    "\n",
    "    print(' '.join(tokens))\n",
    "    for ps in probs:\n",
    "        probs, idxs = ps.sort(descending=True)\n",
    "\n",
    "        for i in range(top):\n",
    "            print(\"\\t%s (%.4f)\" % (vocab.get_token(idxs[i].item()),\n",
    "                                   probs[i].item()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> i have a pet [M] . <s>\n",
      "\tcat (0.0707)\n",
      "\tdog (0.0533)\n",
      "\tsibling (0.0342)\n",
      "\tpuppy (0.0340)\n",
      "\tsister (0.0302)\n",
      "\tretriever (0.0265)\n",
      "\tdaughter (0.0264)\n",
      "\tshepard (0.0232)\n",
      "\tnamed (0.0213)\n",
      "\tbrother (0.0208)\n",
      "\n",
      "<s> i have two pet [M] . <s>\n",
      "\tcats (0.1525)\n",
      "\tdogs (0.0874)\n",
      "\tgirls (0.0748)\n",
      "\tboys (0.0501)\n",
      "\tbrothers (0.0499)\n",
      "\twives (0.0420)\n",
      "\tchildren (0.0386)\n",
      "\tkids (0.0377)\n",
      "\tsisters (0.0333)\n",
      "\t, (0.0219)\n",
      "\n",
      "<s> my [M] is a lawyer . <s>\n",
      "\tmother (0.2872)\n",
      "\tdad (0.2481)\n",
      "\tmom (0.1561)\n",
      "\thusband (0.0864)\n",
      "\tfather (0.0363)\n",
      "\tbrother (0.0230)\n",
      "\tjob (0.0144)\n",
      "\tsister (0.0143)\n",
      "\tparents (0.0131)\n",
      "\twife (0.0104)\n",
      "\n",
      "<s> my [M] is a [M] . <s>\n",
      "\tmother (0.2330)\n",
      "\tdad (0.2212)\n",
      "\tmom (0.1373)\n",
      "\thusband (0.1110)\n",
      "\tbrother (0.0364)\n",
      "\tfather (0.0357)\n",
      "\tsister (0.0285)\n",
      "\tjob (0.0145)\n",
      "\twife (0.0141)\n",
      "\tparents (0.0127)\n",
      "\n",
      "\tteacher (0.0899)\n",
      "\tlawyer (0.0456)\n",
      "\tnurse (0.0426)\n",
      "\tcop (0.0414)\n",
      "\tmechanic (0.0386)\n",
      "\tdoctor (0.0259)\n",
      "\tpilot (0.0195)\n",
      "\tjournalist (0.0163)\n",
      "\tdancer (0.0148)\n",
      "\thairdresser (0.0123)\n",
      "\n",
      "<s> i [M] [M] [M] sometimes . <s>\n",
      "\tam (0.1669)\n",
      "\tlove (0.1151)\n",
      "\tlike (0.0823)\n",
      "\tdo (0.0610)\n",
      "\thave (0.0457)\n",
      "\tcan (0.0237)\n",
      "\thate (0.0237)\n",
      "\tjust (0.0199)\n",
      "\tget (0.0172)\n",
      "\tenjoy (0.0159)\n",
      "\n",
      "\ta (0.1115)\n",
      "\tin (0.0584)\n",
      "\tnot (0.0565)\n",
      "\tto (0.0477)\n",
      "\tmy (0.0177)\n",
      "\tthe (0.0163)\n",
      "\ti (0.0146)\n",
      "\tinto (0.0119)\n",
      "\tfor (0.0111)\n",
      "\tlike (0.0101)\n",
      "\n",
      "\t, (0.0760)\n",
      "\t. (0.0338)\n",
      "\tright (0.0203)\n",
      "\tbut (0.0201)\n",
      "\tup (0.0146)\n",
      "\tand (0.0135)\n",
      "\tnot (0.0133)\n",
      "\tmusic (0.0083)\n",
      "\tfood (0.0074)\n",
      "\tcoffee (0.0065)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    get_top_masked_tokens(s, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to *BERT*\n",
    "\n",
    "**B**idirectional\n",
    "\n",
    "**E**ncoder\n",
    "\n",
    "**R**epresentations from\n",
    "\n",
    "**T**ransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Masked Language Modeling at scale\n",
    "\n",
    "#### - Learned representations are useful downstream\n",
    "\n",
    "<img src=\"img/bert_citations.png\" alt=\"Drawing\" style=\"width: 45%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great implementation in [transformers](https://github.com/huggingface/transformers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details -- Model Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\text{BERT}_{\\text{BASE}}$: 12 layers, hidden dimension 768, 12 attention heads (**110 million parameters**)\n",
    "- $\\text{BERT}_{\\text{LARGE}}$: 24 layers, hidden dimension 1024, 16 attention heads (**340 million parameters**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536138705d04416b989d6c08eb6457ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14446fc3ab746c4976a9f9932911f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2664ab97dc634a00ab67bfb2ad101735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-cased', output_attentions=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details -- Input Implementation\n",
    "\n",
    "\n",
    "- `[CLS]` token: starts each sequence. Used as aggregate sequence representation.\n",
    "- `[SEP]` token: separates two segments (e.g. two sentences).\n",
    "- **Segment embedding**: learned embedding for every token indicating whether it belongs\n",
    "to sentence A or sentence B.\n",
    "- **Position embedding**: learned.\n",
    "\n",
    "\n",
    "<img src=\"img/bert_inputs.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "\n",
    "**Exercise:** Which downstream tasks would two sequences be useful for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "#### BERT represents text using **subword** tokens with a 30k token vocabulary.  \n",
    "\n",
    "\n",
    "\n",
    "(more info [here](https://github.com/google/sentencepiece) and in the papers mentioned there)\n",
    "\n",
    "<!-- - **Token embedding**: WordPiece embeddings with 30k token vocabulary. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pre', '##tra', '##ining', 'is', 'cool', '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Pretraining is cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', '##ER', '##T', 'represents', 'text', 'using', 'sub', '##words', '.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"BERT represents text using subwords.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Learned Conditionals (& Representations)\n",
    "\n",
    "**Probing tasks** can be used to examine aspects of what the model has learned. \n",
    "\n",
    "Following [Petroni et al 2019](https://arxiv.org/pdf/1909.01066.pdf) we probe for '**knowledge**' that the model has learned by querying for masked out objects, e.g.:\n",
    "\n",
    "<img src=\"img/bert_kb.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "\n",
    "The task also illustrates some aspects of the **conditional distributions** and **contextualized representations** that the model has learned.\n",
    "\n",
    "(image from [Petroni et al 2019])\n",
    "\n",
    "\n",
    "**Exercise:** The authors only consider *single-token* prediction. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probing Task\n",
    "\n",
    "We use a dataset from [Petroni et al 2019](https://github.com/facebookresearch/LAMA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'masked_sentences': ['To emphasize the 50th anniversary of the Super Bowl the [MASK] color was used.'],\n",
       " 'obj_label': 'gold',\n",
       " 'id': '56be4db0acb8001400a502f0_0',\n",
       " 'sub_label': 'Squad'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "data = utils.load_lama_squad(download=True)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:07<00:00, 38.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct@1: 0.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "model.eval()\n",
    "for example in tqdm(data, total=len(data)):\n",
    "    sentence, label = example['masked_sentences'][0], example['obj_label']\n",
    "    inp = torch.tensor([\n",
    "        [tokenizer.cls_token_id] + \n",
    "        tokenizer.encode(sentence) + \n",
    "        [tokenizer.sep_token_id]\n",
    "    ], device=device)\n",
    "    \n",
    "    mask = (inp == tokenizer.vocab[tokenizer.mask_token])\n",
    "    out, attn = model(inp)\n",
    "    \n",
    "    probs, token_ids = out[mask].softmax(1).topk(10)\n",
    "    probs = probs[0].tolist()\n",
    "    token_ids = token_ids[0].tolist()\n",
    "\n",
    "    tokens = [tokenizer.ids_to_tokens[i] for i in token_ids]\n",
    "\n",
    "    results.append({\n",
    "        'sentence': sentence,\n",
    "        'label': label,\n",
    "        'top_tokens': tokens,\n",
    "        'top_probs': probs,\n",
    "        'correct@1': tokens[0] == label,\n",
    "        'attn': attn\n",
    "    })\n",
    "\n",
    "print(\"correct@1: %.3f\" % (\n",
    "    len([r for r in results if r['correct@1']]) / len(results)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f982d30a1675478c84ba0cd7360c0032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='idx', max=36), Dropdown(description='attn_layer', option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "correct = [r for r in results if r['correct@1']]\n",
    "wrong = [r for r in results if not r['correct@1']]\n",
    "\n",
    "def show(idx=0, attn_layer=0, is_correct=True):\n",
    "    result = correct[idx] if is_correct else wrong[idx]\n",
    "\n",
    "    # --- format the result into a string\n",
    "    top_str = '\\n\\t'.join([\n",
    "        ('\\t%s\\t(%.4f)' % (tokens, probs)) \n",
    "        for tokens, probs in zip(result['top_tokens'], result['top_probs'])\n",
    "    ])\n",
    "    print(\"%s\\n\\tlabel:\\t%s\\n\\n\\ttop:%s\" % (\n",
    "        result['sentence'], \n",
    "        result['label'], \n",
    "        top_str\n",
    "    ))\n",
    "\n",
    "    # --- visualize attention\n",
    "    print(\"Attention weights (12 heads) from layer %d:\" % attn_layer)\n",
    "    fig, axs = plt.subplots(3, 4, figsize=(18, 12))\n",
    "\n",
    "    toks = ['[CLS]'] + tokenizer.tokenize(result['sentence']) + ['[SEP]']\n",
    "    for i, ax in enumerate(axs.reshape(-1)):\n",
    "        ax.matshow(result['attn'][attn_layer][0][i].data.cpu().numpy(), cmap='gray')\n",
    "\n",
    "        ax.set_xticks(range(len(toks)))\n",
    "        ax.set_xticklabels(toks, rotation=90, fontsize=15)\n",
    "        ax.set_yticks(range(len(toks)))\n",
    "        ax.set_yticklabels(toks, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "interactive(\n",
    "    show, \n",
    "    idx=(0, min(len(correct), len(wrong))-1), \n",
    "    attn_layer=range(12), \n",
    "    is_correct=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
