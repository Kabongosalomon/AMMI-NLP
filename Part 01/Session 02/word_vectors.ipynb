{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys\n",
    "import numpy as np\n",
    "from heapq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(filename):\n",
    "    fin = io.open(filename, 'r', encoding='utf-8', newline='\\n')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "#         data[tokens[0]] = np.asarray(map(float, tokens[1:]))\n",
    "        data[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ** Word vectors ** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading word vectors\n",
    "\n",
    "print('')\n",
    "print(' ** Word vectors ** ')\n",
    "print('')\n",
    "\n",
    "word_vectors = load_vectors('wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function computes the cosine similarity between vectors u and v\n",
    "\n",
    "def cosine(u, v):\n",
    "    ## FILL CODE\n",
    "    return u.dot(v)/(np.linalg.norm(u)* np.linalg.norm(v))\n",
    "\n",
    "## This function returns the word corresponding to \n",
    "## nearest neighbor vector of x\n",
    "## The list exclude_words can be used to exclude some\n",
    "## words from the nearest neighbors search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity(apple, apples) = 0.637\n",
      "similarity(apple, banana) = 0.431\n",
      "similarity(apple, tiger) = 0.212\n"
     ]
    }
   ],
   "source": [
    "# compute similarity between words\n",
    "\n",
    "print('similarity(apple, apples) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['apples']))\n",
    "print('similarity(apple, banana) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['banana']))\n",
    "print('similarity(apple, tiger) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['tiger']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for nearest neighbors\n",
    "\n",
    "def nearest_neighbor(x, word_vectors, exclude_words=[]):\n",
    "    best_score = -1.0\n",
    "    best_word = ''\n",
    "    \n",
    "    ## FILL CODE\n",
    "    for words in word_vectors.keys():\n",
    "#         ipdb.set_trace()\n",
    "        temp = cosine(word_vectors[words], x)\n",
    "        if temp > best_score and words not in exclude_words:\n",
    "            best_score = temp\n",
    "            best_word = words\n",
    "            \n",
    "    return best_word\n",
    "\n",
    "## This function return the words corresponding to the\n",
    "## K nearest neighbors of vector x.\n",
    "## You can use the functions heappush and heappop.\n",
    "\n",
    "def knn(x, vectors, k):\n",
    "    heap = []\n",
    "\n",
    "    ## FILL CODE\n",
    "#     for words in vectors.keys():\n",
    "#         score = cosine(x,  vectors[words])\n",
    "#         b_word = nearest_neighbor(x, vectors, exclude)\n",
    "#         heappush(heap, (b_word, score))\n",
    "#         if len(heap)>k:\n",
    "# #             ipdb.set_trace()\n",
    "#             heappop(heap)\n",
    "#         exclude.append(b_word)\n",
    "        \n",
    "        \n",
    "    exclude = []    \n",
    "    for ki in range(k+1):\n",
    "        \n",
    "        b_word = nearest_neighbor(x, vectors, exclude)\n",
    "        score = cosine(x,  vectors[b_word])\n",
    "#         if score == 1:\n",
    "#             exclude.append(b_word)\n",
    "#             pass\n",
    "        heap.append((score, b_word))\n",
    "        exclude.append(b_word)\n",
    "\n",
    "    return [heappop(heap) for i in range(len(heap))][::-1][:-1] # reverse and don't take the last element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine(word_vectorsnearest_neighbor(word_vectors['cat'],  word_vectors), nearest_neighbor(word_vectors['cat'],word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> h = []\n",
    "# >>> heappush(h, (5, 'write code'))\n",
    "# >>> heappush(h, (7, 'release product'))\n",
    "# >>> heappush(h, (1, 'write spec'))\n",
    "# >>> heappush(h, (3, 'create tests'))\n",
    "# >>> print(heappop(h))\n",
    "# >>> print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest neighbor of cat is: dog\n",
      "\n",
      "cat\n",
      "--------------\n",
      "cats\t0.732\n",
      "dog\t0.638\n",
      "pet\t0.573\n",
      "rabbit\t0.549\n",
      "dogs\t0.538\n"
     ]
    }
   ],
   "source": [
    "# looking at nearest neighbors of a word\n",
    "\n",
    "print('The nearest neighbor of cat is: ' +\n",
    "      nearest_neighbor(word_vectors['cat'], word_vectors, ['cat', 'cats']))\n",
    "\n",
    "knn_cat = knn(word_vectors['cat'], word_vectors, 5)\n",
    "print('')\n",
    "print('cat')\n",
    "print('--------------')\n",
    "for score, word in knn(word_vectors['cat'], word_vectors, 5):\n",
    "    print(word + '\\t%.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}_{d}=\\arg \\max _{i}\\left(\\mathbf{x}_{c}-\\mathbf{x}_{a}+\\mathbf{x}_{b}\\right)^{\\top} \\mathbf{x}_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function return the word d, such that a:b and c:d\n",
    "## verifies the same relation\n",
    "from nltk.stem import PorterStemmer\n",
    "ps =PorterStemmer()\n",
    "def analogy(a, b, c, word_vectors):\n",
    "    ## FILL CODE\n",
    "    a = a.lower()\n",
    "    b = b.lower()\n",
    "    c = c.lower()\n",
    "    \n",
    "    best_anal = - np.inf\n",
    "    best_anal_word = ''\n",
    "    \n",
    "    x_a = word_vectors[a]/np.linalg.norm(word_vectors[a])\n",
    "    x_b = word_vectors[b]/np.linalg.norm(word_vectors[b])\n",
    "    x_c = word_vectors[c]/np.linalg.norm(word_vectors[c])\n",
    "    \n",
    "    for word in word_vectors.keys():\n",
    "        if True in [i in word for i in [a, b, c]]: # make sure to not consider all word that are in connextion with a, b and c\n",
    "#             ipdb.set_trace()\n",
    "            continue\n",
    "            \n",
    "        word_vectors[word] = word_vectors[word]/np.linalg.norm(word_vectors[word])\n",
    "        \n",
    "        anal = (x_c + x_b - x_a).dot(word_vectors[word])\n",
    "        \n",
    "        if anal > best_anal:\n",
    "            best_anal = anal\n",
    "            best_anal_word = word\n",
    "        \n",
    "    return best_anal_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "france - paris + rome = italy\n",
      "\n",
      "uncle - father + mother = aunt\n",
      "\n",
      "king - man + woman = queen\n"
     ]
    }
   ],
   "source": [
    "# Word analogies\n",
    "\n",
    "print('')\n",
    "print('france - paris + rome = ' + analogy('paris', 'france', 'rome', word_vectors))\n",
    "\n",
    "print('')\n",
    "print('uncle - father + mother = ' + analogy('father', 'uncle', 'mother', word_vectors))\n",
    "\n",
    "print('')\n",
    "print('king - man + woman = ' + analogy('man', 'king', 'woman', word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "similarity(genius, man) = 0.445\n",
      "similarity(genius, woman) = 0.325\n"
     ]
    }
   ],
   "source": [
    "## A word about biases in word vectors:\n",
    "\n",
    "print('')\n",
    "print('similarity(genius, man) = %.3f' %\n",
    "      cosine(word_vectors['man'], word_vectors['genius']))\n",
    "print('similarity(genius, woman) = %.3f' %\n",
    "      cosine(word_vectors['woman'], word_vectors['genius']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the association strength between:\n",
    "##   - a word w\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "def association_strength(w, A, B, vectors):\n",
    "    strength = 0.0\n",
    "    ## FILL CODE\n",
    "    a_sum = 0.0\n",
    "    b_sum = 0.0\n",
    "    \n",
    "    for a in A : \n",
    "        a_sum += cosine(vectors[w], vectors[a])\n",
    "    \n",
    "    for b in B : \n",
    "        b_sum += cosine(vectors[w], vectors[b])\n",
    "    \n",
    "    \n",
    "    \n",
    "    strength = 1/len(A) * a_sum - 1/len(B) * b_sum\n",
    "    return strength\n",
    "\n",
    "## Perform the word embedding association test between:\n",
    "##   - two sets of words X and Y\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "def weat(X, Y, A, B, vectors):\n",
    "    score = 0.0\n",
    "    ## FILL CODE\n",
    "    score_1 = 0.0\n",
    "    score_2 = 0.0\n",
    "    for w in X:\n",
    "        score_1 += association_strength(w, A, B, vectors)\n",
    "    \n",
    "    for z in Y:\n",
    "        score_2 += association_strength(z, A, B, vectors)\n",
    "     \n",
    "    score = score_1 - score_2\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embedding association test: 0.847\n"
     ]
    }
   ],
   "source": [
    "## Replicate one of the experiments from:\n",
    "##\n",
    "## Semantics derived automatically from language corpora contain human-like biases\n",
    "## Caliskan, Bryson, Narayanan (2017)\n",
    "\n",
    "career = ['executive', 'management', 'professional', 'corporation', \n",
    "          'salary', 'office', 'business', 'career']\n",
    "family = ['home', 'parents', 'children', 'family',\n",
    "          'cousins', 'marriage', 'wedding', 'relatives']\n",
    "male = ['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n",
    "female = ['amy', 'joan', 'lisa', 'sarah', 'diana', 'kate', 'ann', 'donna']\n",
    "\n",
    "print('')\n",
    "print('Word embedding association test: %.3f' %\n",
    "      weat(career, family, male, female, word_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>About the Authors:</h1> \n",
    "\n",
    "<a href=\"https://skabongo.github.io/\">Salomon Kabongo</a>, Master degree student at <a href=\"https://aims.ac.za/\">the African Master in Machine Intelligence (AMMI, Ghana)</a> his research focused on the use machine learning technique in the field of Natural Language Processing, learn more about him [here](https://skabongo.github.io/) or [twitter](https://twitter.com/SalomonKabongo1).\n",
    "\n",
    "**References :** NLP Course at AMMI by [Edouard Grave](https://twitter.com/exgrv?lang=en)\n",
    "\n",
    "Copyright &copy; 2020. This notebook and its source code are released under the terms of the <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">Apache License 2.0</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
