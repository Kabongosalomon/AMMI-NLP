{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"font-family:verdana;font-size:300%;text-align:center;background-color:#f2f2f2;color:#0d0d0d\">AMMI NLP - Part 1</h1>\n",
    "\n",
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:Center;color:#993333\"> Lab 1: Introduction to text classification  </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:left;color:blue\">Section 1: Text Classification with Naive Bayes Classifier </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this part you'll implement Naive Bayes classifier to classify the text. You need to build a model that predicts the langauge of the text given the words of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys, math, re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    '''\n",
    "    Parameters:\n",
    "    filename (string): path to file to be read\n",
    "    \n",
    "    Return: \n",
    "    List of tuples (explained in first question)\n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        data.append((tokens[0], tokens[1:]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('__label__deu',\n",
       " ['Ich', 'würde', 'alles', 'tun,', 'um', 'dich', 'zu', 'beschützen.'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(\"train1.txt\")\n",
    "data[0]\n",
    "# Tuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tom', 'ist', 'an', 'Kunst', 'völlig', 'uninteressiert.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(data):\n",
    "    '''\n",
    "    Parameters:\n",
    "    \n",
    "    data is  list of [(label, words), (label, worlds), ......]\n",
    "    list of tuples in the shape (string, [list of strings]) )\n",
    "    \n",
    "    Returns: \n",
    "    \n",
    "    This function should return a dictionary containing the following:\n",
    "    { \n",
    "    # label_counts (python dictionary): \n",
    "         {label:  no. of times the label appeared },\n",
    "    # word_counts  (dictionary of dictionaries): \n",
    "         {label: {word: no. of times this word appeared with this label }},\n",
    "    # label_total (int): \n",
    "        total number of labels. (size of train data),\n",
    "    # word_total  (python dictionary) total number of words (from the entire corupus) of the particular label:\n",
    "          {label: no.of words}\n",
    "          \n",
    "          }\n",
    "    '''\n",
    "    \n",
    "    label_total = 0\n",
    "    word_total = defaultdict(lambda: 0)\n",
    "    label_counts = defaultdict(lambda: 0)\n",
    "    word_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "    for example in data:\n",
    "        label, sentence = example\n",
    "        \n",
    "        ## FILL CODE\n",
    "        label_counts[label] += 1\n",
    "        \n",
    "        for w in sentence:\n",
    "            word_total[w] += 1\n",
    "            word_counts[label][w] += 1\n",
    "            \n",
    "    label_total = len(label_counts.keys())\n",
    "    \n",
    "    return {'label_counts': label_counts, \n",
    "            'word_counts': word_counts, \n",
    "            'label_total': label_total, \n",
    "            'word_total': word_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, mu, label_counts, word_counts, label_total, word_total):\n",
    "    '''\n",
    "     Parameters: \n",
    "        sentence (string): sentence to be classified\n",
    "        mu (positive real number): Laplace Smoothing hyperparameter\n",
    "        ** The other parameters introduced in the count_words function\n",
    "    \n",
    "    Returns:\n",
    "    best_label (string): the label that has the highest score. \n",
    "    \n",
    "    Implement the function to predict the best label for the given sentence using Naive Bayes algorithm \n",
    "    '''\n",
    "    best_label = None\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    V = len(word_total.values())\n",
    "    \n",
    "\n",
    "    for label in word_counts.keys():\n",
    "#         score = 0.0\n",
    "        \n",
    "        ## FILL CODE\n",
    "        pwd = 1\n",
    "        \n",
    "        Den = sum(word_counts[label].values()) + mu*V   # Laplace smoothing\n",
    "#         Den = sum(word_counts[label].values())\n",
    "        for w in sentence:\n",
    "            pwd *= (mu+word_counts[label][w])/Den       # Laplace smoothing\n",
    "#             pwd *= (word_counts[label][w])/Den\n",
    "        \n",
    "        score = label_counts[label]*pwd  \n",
    "        \n",
    "        if score > best_score :\n",
    "            best_score = score\n",
    "            best_label = label\n",
    "            \n",
    "        \n",
    "    return best_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_2(sentence, mu, label_counts, word_counts, label_total, word_total):\n",
    "    '''\n",
    "     Parameters: \n",
    "        sentence (string): sentence to be classified\n",
    "        mu (positive real number): Laplace Smoothing hyperparameter\n",
    "        ** The other parameters introduced in the count_words function\n",
    "    \n",
    "    Returns:\n",
    "    best_label (string): the label that has the highest score. \n",
    "    \n",
    "    Implement the function to predict the best label for the given sentence using Naive Bayes algorithm \n",
    "    \n",
    "    '''\n",
    "    best_label = None\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    V = len(word_total.values())\n",
    "    \n",
    "    for label in word_counts.keys():\n",
    "        score = 0.0\n",
    "        \n",
    "        ## FILL CODE\n",
    "                \n",
    "        Den = sum(word_counts[label].values()) + mu*V   # Laplace smoothing\n",
    "#         Den = sum(word_counts[label].values())\n",
    "        for w in sentence:\n",
    "            score += np.log((mu+word_counts[label][w])/Den)       # # Laplace smoothing\n",
    "#                 pwd *= (word_counts[label][w])/Den\n",
    "        \n",
    "        score = np.log(label_counts[label])+score \n",
    "        \n",
    "        if score > best_score :\n",
    "            best_score = score\n",
    "            best_label = label\n",
    "        \n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('+', ['food', 'was', 'amazing']),\n",
       " ('+', ['great', 'food']),\n",
       " ('-', ['food', 'was', 'bland']),\n",
       " ('+', ['amazing', 'restaurant']),\n",
       " ('-', ['restaurant', 'is', 'not', 'great'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data(\"course_ex.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\")\n",
    "# print(\"** Naive Bayes **\")\n",
    "# print(\"\")\n",
    "\n",
    "# mu = 1.0\n",
    "# train_data = load_data(\"course_ex.txt\")\n",
    "# valid_data = load_data(\"course_val.txt\")\n",
    "# counts = count_words(train_data)\n",
    "# # ipdb.set_trace()\n",
    "# print(\"Validation accuracy: %.3f\" % compute_accuracy(valid_data, mu, counts))\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(counts['word_counts']['+'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts['word_total'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(valid_data, mu, counts):\n",
    "    '''\n",
    "    Parameters:\n",
    "    valid_data (list of tuples): returned value of load_data function \n",
    "    mu (positive real): Laplace smoothing hyper-parameter\n",
    "    counts (dictionary of dictionaries): return value of count_words_function\n",
    "    \n",
    "    Returns: \n",
    "    accuracy (float): the accuracy of the Naive Bayes classifier\n",
    "    '''\n",
    "    accuracy = 0.0\n",
    "    num = 0\n",
    "    for label, sentence in valid_data:\n",
    "         ## FILL CODE\n",
    "#         pred = predict(sentence, mu, counts['label_counts'],\\\n",
    "#                        counts['word_counts'], counts['label_total'], \\\n",
    "#                        counts['word_total'])\n",
    "        \n",
    "        pred = predict(sentence, mu, **counts)\n",
    "        if pred == label:\n",
    "            num += 1\n",
    "\n",
    "    return num/len(valid_data)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Naive Bayes **\n",
      "\n",
      "Validation accuracy: 94.700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Naive Bayes **\")\n",
    "print(\"\")\n",
    "\n",
    "mu = 1.0\n",
    "train_data = load_data(\"train1.txt\")\n",
    "valid_data = load_data(\"valid1.txt\")\n",
    "counts = count_words(train_data)\n",
    "\n",
    "print(\"Validation accuracy: %.3f\" % compute_accuracy(valid_data, mu, counts))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:left;color:black\">***************************************************************</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:left;color:blue\">Section 2: Softmax Classification of Text  </h1>\n",
    "\n",
    "##### In this part you'll implement a Softmax Classifier to classify the text (think of it as a 1 layer feedforward neural network). You need to build a model that predicts the language of the text given the words of the text\n",
    "\n",
    "$$\n",
    "f_{i}(\\mathbf{s})=\\frac{\\exp \\left(s_{i}\\right)}{\\sum_{i^{\\prime}} \\exp \\left(s_{i^{\\prime}}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(filename, threshold=1):\n",
    "    '''\n",
    "    Parameters:\n",
    "    filename (string): path to the data file\n",
    "    \n",
    "    Returns:\n",
    "    word_dic: dictionary maps words to number of times it appeard in the corpus\n",
    "            dic {word: no of times word appears }\n",
    "    label_dic: dictionary maps labels to integers\n",
    "        dic {label: label_id}\n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    word_dict, label_dict = {}, {}\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        if not label in label_dict:\n",
    "            label_dict[label] = len(label_dict)\n",
    "\n",
    "        for w in tokens[1:]:\n",
    "            counts[w] += 1\n",
    "            \n",
    "    for k, v in counts.items():\n",
    "        if v > threshold:\n",
    "            word_dict[k] = len(word_dict)\n",
    "    return word_dict, label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, word_dict, label_dict):\n",
    "    '''\n",
    "    ## This function converts the text to a list of tuples of \n",
    "    [(label_id, word_representation),...]\n",
    "    \n",
    "    Parameters:\n",
    "    filename (string): path to the file which contains the data\n",
    "    word_dict (python dictionary): returned by build_dict() function above.\n",
    "    label_dict (python dictionary): reutrned by build_dict() function above() \n",
    "    \n",
    "    Returns:\n",
    "    data (list of tuples): \n",
    "    The representation of the data in the form \n",
    "    [(y_0, x_0, .. (y_i, x_i), ... (y_n, x_n))]\n",
    "    where y is the value of the class \n",
    "    x is the representation of the sentence as a word count vector \n",
    "    \n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    dim = len(word_dict)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        yi = label_dict[label]\n",
    "        xi = np.zeros(dim)\n",
    "\n",
    "        for word in tokens[1:]:\n",
    "            if word in word_dict:\n",
    "                wid = word_dict[word]\n",
    "                xi[wid] += 1.0\n",
    "        data.append((yi, xi))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, label_dic = build_dict(\"train1.txt\", threshold=1)\n",
    "# label_dic\n",
    "d = load_data(\"train1.txt\", word_dict, label_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([1., 1., 1., ..., 0., 0., 0.])),\n",
       " (0, array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " (1, array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " (0, array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " (2, array([0., 0., 0., ..., 0., 0., 0.]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    This function should apply softmax to vector x\n",
    "    \n",
    "    Parameter:\n",
    "    x (numpy array)\n",
    "    Returns: \n",
    "    softmax(x) (numpy array)\n",
    "    \n",
    "    '''\n",
    "    ## FILL CODE\n",
    "    m = max(x)\n",
    "    x = x-m\n",
    "    \n",
    "\n",
    "    return np.exp(x)/ np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Hint) Derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:verdana;font-size:150%;text-align:center;background-color:#f2f2f2;color:#993333; border:2px; border-style:solid; border-color:gray; padding: 1em\"> \n",
    "   Let $x_i$ be the input vector $W$ is the weight vector $m=$no. of labels, $n=$vocab size\n",
    "    $$ {\\bf S} = W × x_i $$  $x_{i} \\in R^{nx1} W \\in R^{m×n}$\n",
    "    $$  $$\n",
    "    $${\\bf O} = softmax(s) $$\n",
    "    $${\\bf L} = -log(O[y_{i}]) $$\n",
    "    $$  $$\n",
    "    $$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial S} . \\frac{\\partial S}{\\partial W} $$\n",
    "    $$  $$\n",
    "    $ \\nabla L_{W} = (O-y_{true})$  x   $x_{i}^{T} $  \n",
    "    $$ (O-y_{true}) \\in R^{mx1}, x_{i}  \\in R^{nx1}$$\n",
    "\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, data, niter):\n",
    "    '''\n",
    "    This function should perform the Stochastic Gradient Descent algorithm \n",
    "    \n",
    "    Parameter:\n",
    "    w (numpy array): weight vector\n",
    "    data (list of tuples): [...(y_i, x_i)...] from above\n",
    "    niter (int): number of iterations\n",
    "    \n",
    "    Retunrs:\n",
    "    w (numpy array): weight vector after training\n",
    "    '''\n",
    "#     nlabels, dim = w.shape\n",
    "#     for iter in range(niter):\n",
    "#         ## FILL CODE\n",
    "        \n",
    "#         pass \n",
    "    \n",
    "    \n",
    "#     return w\n",
    "\n",
    "    nlabels, dim = w.shape\n",
    "    lr = 0.01\n",
    "    for iter in range(niter):\n",
    "        ## FILL CODE\n",
    "        for ex in data:\n",
    "            y,x = ex\n",
    "            onhot = np.zeros(10)\n",
    "            onhot[y] = 1\n",
    "            y = onhot\n",
    "            s = np.dot(w,x)\n",
    "            o = softmax(s)\n",
    "            y = y.reshape(nlabels,1)\n",
    "            o = o.reshape(nlabels,1)\n",
    "            x = x.reshape(1,dim)\n",
    "            d_lw = (o - y)*x\n",
    "            w = w - lr*d_lw\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    '''\n",
    "    This function should compute and return the prediction. \n",
    "    Parameters:\n",
    "    w (numpy array): trained weight vector\n",
    "    x (numpy array): word count vector\n",
    "    \n",
    "    Returns: \n",
    "    prediction (int): index of the correct prediction (y_i)\n",
    "    '''\n",
    "    ## FILL CODE\n",
    "    return np.argmax(softmax(w.dot(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(w, valid_data):\n",
    "    '''\n",
    "    This function should compute the accuracy of the classifier \n",
    "    Parameters:\n",
    "    w (numpy array): trained weight vector\n",
    "    valid_data (list of tuples): loaded validation data using load_data() function \n",
    "    \n",
    "    Returns: \n",
    "    accuracy (float): accuracy of the classifier \n",
    "    '''\n",
    "    ## FILL CODE\n",
    "\n",
    "    accuracy = 0\n",
    "    for ex in valid_data:\n",
    "        y,x = ex\n",
    "        pred = predict(w, x)\n",
    "        if pred == y:\n",
    "            accuracy += 1\n",
    "\n",
    "    return accuracy/len(valid_data)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Logistic Regression **\n",
      "\n",
      "\n",
      "Validation accuracy: 88.900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Logistic Regression **\")\n",
    "print(\"\")\n",
    "\n",
    "word_dict, label_dict = build_dict(\"train1.txt\")\n",
    "train_data = load_data(\"train1.txt\", word_dict, label_dict)\n",
    "valid_data = load_data(\"valid1.txt\", word_dict, label_dict)\n",
    "\n",
    "nlabels = len(label_dict)\n",
    "dim = len(word_dict)\n",
    "w = np.zeros([nlabels, dim])\n",
    "w = sgd(w, train_data, 5)\n",
    "print(\"\")\n",
    "print(\"Validation accuracy: %.3f\" % compute_accuracy(w, valid_data))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>About the Authors:</h1> \n",
    "\n",
    "<a href=\"https://skabongo.github.io/\">Salomon Kabongo</a>, Master degree student at <a href=\"https://aims.ac.za/\">the African Master in Machine Intelligence (AMMI, Ghana)</a> his research focused on the use machine learning technique in the field of Natural Language Processing, learn more about him [here](https://skabongo.github.io/) or [twitter](https://twitter.com/SalomonKabongo1).\n",
    "\n",
    "**References :** NLP Course at AMMI by [Edouard Grave](https://twitter.com/exgrv?lang=en)\n",
    "\n",
    "Copyright &copy; 2020. This notebook and its source code are released under the terms of the <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">Apache License 2.0</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
